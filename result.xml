<documents>
  <document>
    <name>nime2001_003.pdf</name>
    <abstract> 
This paper will present observations on the design, artistic, 
and human factors of creating digital music controllers.  
Specific projects will be presented, and a set of design 
principles will be supported from those examples. 
</abstract>
    <keywords>  Musical control, artistic interfaces.  </keywords>
  </document>
  <document>
    <name>nime2001_007.pdf</name>
    <abstract>
Over the last four years, we have developed a series of
lectures, labs and project assignments aimed at introducing
enough technology so that students from a mix of
disciplines can design and build innovative interface
devices.
</abstract>
    <keywords> Input devices, music controllers, CHI technology, courses. </keywords>
  </document>
  <document>
    <name>nime2001_011.pdf</name>
    <abstract>
In this paper we describe our efforts towards the
development of live performance computer-based musical
instrumentation.   Our design criteria include initial ease
of use coupled with a long term potential for virtuosity,
minimal and low variance latency, and clear and simple
strategies for programming the relationship between
gesture and musical result.   We present custom
controllers and unique adaptations of standard gestural
interfaces, a programmable connectivity processor, a
communications protocol called Open Sound Control
(OSC), and a variety of metaphors for musical control.  
We further describe applications of our technology to a
variety of real musical performances and directions for
future research.
</abstract>
  </document>
  <document>
    <name>nime2001_015.pdf</name>
    <abstract>
This paper reviews the existing literature on input device
evaluation and design in human-computer interaction (HCI)
and discusses possible applications of this knowledge to
the design and evaluation of new interfaces for musical
expression. Specifically, a set of musical tasks is suggested
to allow the evaluation of different existing controllers.

 </abstract>
    <keywords> Input device design, gestural control, interactive systems  </keywords>
  </document>
  <document>
    <name>nime2001_019.pdf</name>
    <abstract>?&lt;.0) L'L$&amp;) L&amp;$0$"#0) #&lt;$) ."#$&amp;T',$) ($Q$+-LN$"#0) '"()N40.,-T) #&lt;$) (4-) U."#$&amp;T',$1V) T-&amp;N$() J;) 64&amp;#.0) W'&lt;") '"() 5'"?&amp;4$N'"M) ) X$) ($0,&amp;.J$) %$0#4&amp;'+) ."0#&amp;4N$"#) ($0.%"1."#$&amp;',#.Q$) L$&amp;T-&amp;N'",$) ."#$&amp;T',$0) T-&amp;) .NL&amp;-Q.0'#.-"'+N40.,1)0L&lt;$&amp;.,'+)0L$'S$&amp;0)EN4+#.C,&lt;'""$+1)-4#R'&amp;(C&amp;'(.'#."%%$-($0.,) 0L$'S$&amp;) '&amp;&amp;';0F) '"() 3$"0-&amp;C3L$'S$&amp;C/&amp;&amp;';0E3$"3/0Y) ,-NJ."'#.-"0) -T) Q'&amp;.-40) 0$"0-&amp;) ($Q.,$0) R.#&lt;0L&lt;$&amp;.,'+) 0L$'S$&amp;) '&amp;&amp;';0FM)X$) (.0,400) #&lt;$) ,-",$L#1) ($0.%"'"(),-"0#&amp;4,#.-")-T)#&lt;$0$)0;0#$N01)'"(1)%.Q$)$Z'NL+$0)T&amp;-N0$Q$&amp;'+)"$R)L4J+.0&lt;$()650)-T)R-&amp;S)J;)W'&lt;")'"()?&amp;4$N'"M
</abstract>
    <keywords>!"#$&amp;',#.Q$) O40.,) $&amp;T-&amp;N'",$1) [$0#4&amp;'+ !"#$&amp;T',$1) 3-".,5.0L+';1)3$"0-&amp;\3L$'S$&amp;)/&amp;&amp;';1)3$"3/M  </keywords>
  </document>
  <document>
    <name>nime2001_024.pdf</name>
    <keywords> multidimensionality, control, resonance, pitch tracking </keywords>
  </document>
  <document>
    <name>nime2001_027.pdf</name>
    <abstract>
The Accordiatron is a new MIDI controller for real-time
performance based on the paradigm of a conventional
squeeze box or concertina. It translates the gestures of a
performer to the standard communication protocol of
MIDI, allowing for flexible mappings of performance data
to sonic parameters. When used in conjunction with a real-
time signal processing environment, the Accordiatron
becomes an expressive, versatile musical instrument.  A
combination of sensory outputs providing both discrete and
continuous data gives the subtle expressiveness and control
necessary for interactive music.
</abstract>
    <keywords> MIDI controllers, computer music, interactive music, electronic musical instruments, musical instrument design, human computer interface </keywords>
  </document>
  <document>
    <name>nime2001_030.pdf</name>
    <abstract>
The technologies behind passive resonant magnetically-
coupled tags are introduced and their application as a
musical controller is illustrated for solo or group
performances, interactive installations, and music toys.  

</abstract>
    <keywords> RFID, resonant tags, EAS tags, musical controller, tangible interface  </keywords>
  </document>
  <document>
    <name>nime2001_034.pdf</name>
    <abstract> 

In this paper, we introduce our research challenges for 
creating new musical instruments using everyday-life media 
with intimate interfaces, such as the self-body, clothes, wa-
ter and stuffed toys. Various sensor technologies including 
image processing and general touch sensitive devices are 
employed to exploit these interaction media. The focus of 
our effort is to provide user-friendly and enjoyable experi-
ences for new music and sound performances. Multi-
modality of musical instruments is explored in each attempt. 
The degree of controllability in the performance and the 
richness of expressions are also discussed for each installa-
tion.  
</abstract>
    <keywords>  New interface, music controller, dance, image processing,  water interface, stuffed toy  </keywords>
  </document>
  <document>
    <name>nime2001_038.pdf</name>
    <abstract>
The MATRIX (Multipurpose Array of Tactile Rods for
Interactive eXpression) is a new musical interface for
amateurs and professionals alike. It gives users a 3-
dimensional tangible interface to control music using their
hands, and can be used in conjunction with a traditional
musical instrument and a microphone, or as a stand-alone
gestural input device. The surface of the MATRIX acts as a
real-time interface that can manipulate the parameters of a
synthesis engine or effect algorithm in response to a
performer's expressive gestures. One example is to have the
rods of the MATRIX control the individual grains of a
granular synthesizer, thereby "sonically sculpting" the
microstructure of a sound. In this way, the MATRIX
provides an intuitive method of manipulating sound with a
very high level of real-time control.
</abstract>
    <keywords> Musical controller, tangible interface, real-time expression, audio synthesis, effects algorithms, signal processing, 3-D interface, sculptable surface </keywords>
  </document>
  <document>
    <name>nime2001_051.pdf</name>
    <abstract> 
KL0/! %,%()! )(M0(N/! ,! 2&amp;$H()! #4! %)#O(F'/! 'L,'! (P%+#)(!
H&amp;0+-021! (+(F')#20F! $&amp;/0F,+! !"#$%&amp;Q! 02'()4,F(/! ,2-! #HO(F'/!
-(/012(-! '#! H(! &amp;/(-! ,2-! (2O#3(-! H3! ,23H#-3! H&amp;'! 02!
%,)'0F&amp;+,)! 'L#/(! NL#! -#! 2#'! /((! 'L($/(+M(/! ,/! 2,'&amp;),++3!
$&amp;/0F,+J! R2! )(4+(F'021! #2! 'L(! /')(21'L/! #4! 'L(/(! %)#O(F'/Q!
02'()(/'021! -0)(F'0#2/! 4#)! /0$0+,)! N#)S! 02! 'L(! 4&amp;'&amp;)(! ,)(!
F#2/0-()(-J!

</abstract>
    <keywords>  T+,3Q!(P%+#),'0#2Q!/#&amp;2-!$,%%021Q!(21,1021!F#2'(2'Q!/#&amp;2-! -(/012J!  </keywords>
  </document>
  <document>
    <name>nime2002_001.pdf</name>
    <abstract>
In this paper we describe the digital emulation of a opti-
cal photosonic instrument. First we briefly describe the
optical instrument which is the basis of this emulation.
Then we give a musical description of the instrument
implementation and its musical use and we conclude
with the "duo" possibility of such an emulation.

</abstract>
    <keywords> Photosonic synthesis, digital emulation, Max-Msp, gestural devices.  </keywords>
  </document>
  <document>
    <name>nime2002_005.pdf</name>
    <abstract> 
In this paper we will have a short overview of some of the 
systems we have been developing as an independent com-
pany over the last years. We will focus especially on our 
latest experiments in developing wireless gestural systems 
using the camera as an interactive tool to generate 2D and 
3D visuals and music.        

</abstract>
  </document>
  <document>
    <name>nime2002_010.pdf</name>
    <abstract>
This paper describes the design and development of sev-
eral musical instruments and MIDI controllers built by
David Bernard (as part of The Sound Surgery project:
www.thesoundsurgery.co.uk) and used in club perform-
ances around Glasgow during 1995-2002.  It argues that
changing technologies and copyright are shifting our
understanding of music from "live art" to "recorded me-
dium" whilst blurring the boundaries between sound and
visual production.

</abstract>
    <keywords> Live electronic music, experimental instruments, MIDI controllers, audio-visual synchronisation, copyright, SKINS digital hand drum.  </keywords>
  </document>
  <document>
    <name>nime2002_012.pdf</name>
    <abstract>  
This paper discusses the Jam-O-Drum multi-player musical 
controller and its adaptation into a gaming controller inter-
face known as the Jam-O-Whirl. The Jam-O-World project 
positioned these two controller devices in a dedicated pro-
jection environment that enabled novice players to partici-
pate in immersive musical gaming experiences. Players' 
actions, detected via embedded sensors in an integrated 
tabletop surface, control game play, real-time computer 
graphics and musical interaction. Jam-O-World requires 
physical and social interaction as well as collaboration 
among players.  
 
</abstract>
    <keywords>  Collaboration, computer graphics, embedded sensors, gam- ing controller, immersive musical gaming experiences, mu- sical controller, multi-player, novice, social interaction.    </keywords>
  </document>
  <document>
    <name>nime2002_038.pdf</name>
    <abstract> 
Mapping, which describes the way a per-
former's controls are connected to sound vari-
ables, is a useful concept when applied to the 
structure of electronic instruments modelled 
after traditional acoustic instruments. But 
mapping is a less useful concept when applied 
to the structure of complex and interactive in-
struments in which algorithms generate control 
information. 
 
This paper relates the functioning and benefits 
of different types of electronic instruments to 
the structural principles on which they are 
based. Structural models of various instru-
ments will be discussed and musical examples 
played. 

</abstract>
    <keywords>  mapping fly-by-wire algorithmic network in- teractivity instrument deterministic indetermin- istic   </keywords>
  </document>
  <document>
    <name>nime2002_043.pdf</name>
    <abstract> 
This paper describes a virtual musical instrument based 
on the scanned synthesis technique and implemented in 
Max-Msp. The device is composed of a computer and 
three gesture sensors. The timbre of the produced sound 
is rich and changing. The instrument proposes an intui-
tive and expressive control of the sound thanks to a 
complex mapping between gesture and sound. 

</abstract>
  </document>
  <document>
    <name>nime2002_050.pdf</name>
    <abstract>
In this paper we describe three new music controllers, each
designed to be played by two players. As the intimacy be-
tween two people increases so does their ability to anticipate
and predict the other's actions. We hypothesize that this in-
timacy between two people can be used as a basis for new
controllers for musical expression. Looking at ways people
communicate non-verbally, we are developing three new in-
struments based on different communication channels. The
Tooka is a hollow tube with a pressure sensor and buttons
for each player. Players place opposite ends in their mouths
and modulate the pressure in the tube with their tongues and
lungs, controlling sound. Coordinated button presses con-
trol the music as well. The Pushka, yet to be built, is a semi-
rigid rod with strain gauges and position sensors to track the
rod's position. Each player holds opposite ends of the rod
and manipulates it together. Bend, end point position, ve-
locity and acceleration and torque are mapped to musical
parameters. The Pullka, yet to be built, is simply a string at-
tached at both ends with two bridges. Tension is measured
with strain gauges. Players manipulate the string tension
at each end together to modulate sound. We are looking at
different musical mappings appropriate for two players.

</abstract>
    <keywords> Two person musical instruments, intimacy, human-human communication, cooperative music, passive haptic interface  </keywords>
  </document>
  <document>
    <name>nime2002_056.pdf</name>
    <abstract> 
The Cardboard Box Garden (CBG) originated from a 
dissatisfaction with current computer technology as it is 
presented to children. This paper shall briefly review the 
process involved in the creation of this installation, from 
motivation through to design and subsequent implemen-
tation and user experience with the CBG. Through the 
augmentation of an everyday artefact, namely the stan-
dard cardboard box, a simple yet powerful interactive 
environment was created that has achieved its goal of 
stirring childrens imagination - judging from the experi-
ence of our users. 

</abstract>
    <keywords>  Education, play, augmented reality, pervasive comput- ing, disappearing computer, assembly, cardboard box   </keywords>
  </document>
  <document>
    <name>nime2002_059.pdf</name>
    <abstract>
Research and musical creation with gestural-oriented
interfaces have recently seen a renewal of interest and
activity at Ircam [1][2]. In the course of several musical
projects, undertaken by young composers attending the
one-year Course in Composition and Computer Music or by
guests artists, Ircam Education and Creation departments
have proposed various solutions for gesture-controlled
sound synthesis and processing. In this article, we describe
the technical aspects of AtoMIC Pro, an Analog to MIDI
converter proposed as a re-usable solution for digitizing
several sensors in different contexts such as interactive
sound installation or virtual instruments.
The main direction of our researches, and of this one in
particular, is to create tools that can be fully integrated into
an artistic project as a real part of the composition and
performance processes.

</abstract>
    <keywords> Gestural controller, Sensor, MIDI, Music.  SOLUTION FOR MULTI-SENSOR ACQUISITION </keywords>
  </document>
  <document>
    <name>nime2002_065.pdf</name>
    <abstract>
We explore the role that metaphor plays in developing ex-
pressive devices by examining the MetaMuse system. Meta-
Muse is a prop-based system that uses the metaphor of rain-
fall to make the process of granular synthesis understand-
able. We discuss MetaMuse within a framework we call
"transparency" that can be used as a predictor of the expres-
sivity of musical devices. Metaphor depends on a literature,
or cultural basis, which forms the basis for making transpar-
ent device mappings. In this context we evaluate the effect
of metaphor in the MetaMuse system.

</abstract>
    <keywords> Expressive interface, transparency, metaphor, prop-based con- troller, granular synthesis.  </keywords>
  </document>
  <document>
    <name>nime2002_071.pdf</name>
    <abstract> 
The use of free gesture in making music has usually 
been confined to instruments that use direct mappings 
between movement and sound space. Here we demon-
strate the use of categories of gesture as the basis of 
musical learning and performance collaboration. These 
are used in a system that reinterprets the approach to 
learning through performance that is found in many 
musical cultures and discussed here through the exam-
ple of Kpelle music. 

</abstract>
    <keywords>  Collaboration, Performance, Metaphor, Gesture   </keywords>
  </document>
  <document>
    <name>nime2002_073.pdf</name>
    <abstract> 
This paper presents a novel coupling of haptics technol-
ogy and music, introducing the notion of tactile compo-
sition or aesthetic composition for the sense of touch.  A 
system that facilitates the composition and perception of 
intricate, musically structured spatio-temporal patterns of 
vibration on the surface of the body is described.  An 
initial test of the system in a performance context is dis-
cussed.  The fundamental building blocks of a composi-
tional language for touch are considered. 
 

</abstract>
  </document>
  <document>
    <name>nime2002_080.pdf</name>
    <abstract> 
The Circular Optical Object Locator  is a collaborative 
and cooperative music-making device.  It uses an inex-
pensive digital video camera to observe a rotating plat-
ter.  Opaque objects placed on the platter are detected by 
the camera during rotation.  The locations of the objects 
passing under the camera are used to generate music. 

</abstract>
    <keywords>  Input devices, music controllers, collaborative, real-time  score manipulation.   </keywords>
  </document>
  <document>
    <name>nime2002_082.pdf</name>
    <abstract> 
We have created a new electronic musical instrument, 
referred to as the Termenova (Russian for "daughter of 
Theremin") that combines a free-gesture capacitive-
sensing device with an optical sensing system that de-
tects the reflection of a hand when it intersects a beam of 
an array of red lasers. The laser beams, which are made 
visible by a thin layer of theatrical mist, provide visual 
feedback and guidance to the performer to alleviate the 
difficulties of using a non-contact interface as well as 
adding an interesting component for the audience to ob-
serve. The system uses capacitive sensing to detect the 
proximity of the player's hands; this distance is mapped 
to pitch, volume, or other continuous effect. The laser 
guide positions are calibrated before play with position-
controlled servo motors interfaced to a main controller 
board; the location of each beam corresponds to the po-
sition where the performer should move his or her hand 
to achieve a pre-specified pitch and/or effect. The optical 
system senses the distance of the player's hands from the 
source of each laser beam, providing an additional di-
mension of musical control. 

</abstract>
    <keywords>  Theremin, gesture interface, capacitive sensing, laser  harp, optical proximity sensing, servo control, musical  controller   </keywords>
  </document>
  <document>
    <name>nime2002_094.pdf</name>
    <keywords>  musical controller, Tactex, tactile interface, tuning sys- tems    </keywords>
  </document>
  <document>
    <name>nime2002_101.pdf</name>
    <abstract> 
We are interested in exhibiting our programs at your 
demo section at the conference. We believe that the sub-
ject of your conference is precisely what we are experi-
menting with in our musical software. 

</abstract>
    <keywords>  Further info on our website  http//www.ixi-software.net.   </keywords>
  </document>
  <document>
    <name>nime2002_102.pdf</name>
    <abstract> 
In this paper we present Afasia, an interactive multime-
dia performance based in Homer's Odyssey [2]. Afasia is 
a one-man digital theater play in which a lone performer 
fitted with a sensor-suit conducts, like Homer, the whole 
show by himself, controlling 2D animations, DVD video 
and conducting the music mechanically performed by a 
robot quartet. After contextualizing the piece, all of its 
technical elements, starting with the hardware input and 
output components, are described. A special emphasis is 
given to the interactivity strategies and the subsequent 
software design. Since its first version premiered in Bar-
celona in 1998, Afasia has been performed in many 
European and American countries and has received sev-
eral international awards. 

</abstract>
    <keywords>  Multimedia interaction, musical robots, real-time musi- cal systems.   </keywords>
  </document>
  <document>
    <name>nime2002_108.pdf</name>
    <abstract> 
This paper describes the design of an electronic Tabla 
controller. The E-Tabla controls both sound and graph-
ics simultaneously. It allows for a variety of traditional 
Tabla strokes and new performance techniques. Graphi-
cal feedback allows for artistical display and pedagogi-
cal feedback.  

</abstract>
    <keywords>  Electronic Tabla, Indian Drum Controller, Physical  Models, Graphical Feedback      </keywords>
  </document>
  <document>
    <name>nime2002_113.pdf</name>
    <abstract> 
In this paper, we describe a computer-based solo musical 
instrument for live performance. We have adapted a 
Wacom graphic tablet equipped with a stylus transducer 
and a game joystick to use them as a solo expressive 
instrument. We have used a formant-synthesis model that 
can produce a vowel-like singing voice. This instrument 
allows multidimensional expressive fundamental fre-
quency control and vowel articulation. The fundamental 
frequency angular control used here allows different 
mapping adjustments that correspond to different me-
lodic styles. 
</abstract>
    <keywords>  Bi-manual, off-the-shelf input devices, fundamental fre- quency control, sound color navigation, mapping.   </keywords>
  </document>
  <document>
    <name>nime2002_118.pdf</name>
    <abstract> 
This paper introduces a subtle interface, which evolved 
from the design of an alternative gestural controller in 
the development of a performance interface.  The con-
ceptual idea used is based on that of the traditional Bo-
dhran instrument, an Irish frame drum.  The design 
process was user-centered and involved professional 
Bodhran players and through prototyping and user-
testing the resulting Vodhran emerged. 

</abstract>
    <keywords>  Virtual instrument, sound modeling, gesture, user- centered design   </keywords>
  </document>
  <document>
    <name>nime2002_120.pdf</name>
    <abstract>
Here we present 2Hearts, a music system controlled by
the heartbeats of two people.  As the players speak and
touch, 2Hearts extracts meaningful variables from their
heartbeat signals.  These variables are mapped to musi-
cal parameters, conveying the changing patterns of ten-
sion and relaxation in the players' relationship.  We de-
scribe the motivation for creating 2Hearts, observations
from the prototypes that have been built, and principles
learnt in the ongoing development process.

</abstract>
    <keywords> Heart Rate, Biosensor, Interactive Music, Non-Verbal Communication, Affective Computing, Ambient Display  </keywords>
  </document>
  <document>
    <name>nime2002_126.pdf</name>
    <keywords>  Gesture, weight distribution, effort, expression, intent,  movement, 3D sensing pressure, force, sensor, resolu- tion, control device, sound, music, input.    </keywords>
  </document>
  <document>
    <name>nime2002_131.pdf</name>
    <abstract> 
This paper briefly describes a number of performance 
interfaces under the broad theme of Interactive Gesture 
Music (IGM).  With a short introduction, this paper dis-
cusses the main components of a Trans-Domain Map-
ping (TDM) framework, and presents various prototypes 
developed under this framework, to translate meaningful 
activities from one creative domain onto another, to pro-
vide real-time control of musical events with physical 
movements. 

</abstract>
    <keywords>  Gesture, Motion, Interactive, Performance, Music.   </keywords>
  </document>
  <document>
    <name>nime2002_137.pdf</name>
    <abstract> 
The design of a virtual keyboard, capable of reproducing 
the tactile feedback of several musical instruments is 
reported. The key is driven by a direct drive motor, 
which allows friction free operations. The force to be 
generated by the motor is calculated in real time by a 
dynamic simulator, which contains the model of 
mechanisms' components and constraints. Each model is 
tuned on the basis of measurements performed on the 
real system. So far, grand piano action, harpsichord and 
Hammond organ have been implemented successfully on 
the system presented here. 

</abstract>
    <keywords>  Virtual mechanisms, dynamic simulation   </keywords>
  </document>
  <document>
    <name>nime2002_143.pdf</name>
    <abstract> 
Interactivity has become a major consideration in the 
development of a contemporary art practice that engages 
with the proliferation of computer based technologies.   

Keywords 
</abstract>
    <keywords> are your choice.   </keywords>
  </document>
  <document>
    <name>nime2002_145.pdf</name>
    <abstract> 
Passive RF Tagging can provide an attractive medium 
for development of free-gesture musical interfaces.  This 
was initially explored in our Musical Trinkets installa-
tion, which used magnetically-coupled resonant LC cir-
cuits to identify and track the position of multiple objects 
in real-time.  Manipulation of these objects in free space 
over a read coil triggered simple musical interactions.  
Musical Navigatrics builds upon this success with new 
more sensitive and stable sensing, multi-dimensional 
response, and vastly more intricate musical mappings 
that enable full musical exploration of free space through 
the dynamic use and control of arpeggiatiation and ef-
fects.  The addition of basic sequencing abilities also 
allows for the building of complex, layered musical in-
teractions in a uniquely easy and intuitive manner. 

</abstract>
    <keywords>  passive tag, position tracking, music sequencer interface   </keywords>
  </document>
  <document>
    <name>nime2002_148.pdf</name>
    <abstract>
We present Audiopad, an interface for musical performance  
that aims to combine the modularity of knob based control-
lers with the expressive character of multidimensional track-
ing interfaces. The performer's manipulations of physical 
pucks on a tabletop control a real-time synthesis process.  
The pucks are embedded with LC tags that the system tracks 
in two dimensions with a series of specially shaped antennae.  
The system projects graphical information on and around the 
pucks to give the performer sophisticated control over the 
synthesis process.

</abstract>
    <keywords> RF tagging, MIDI, tangible interfaces, musical controllers,  object tracking  </keywords>
  </document>
  <document>
    <name>nime2002_156.pdf</name>
    <abstract> 
In this paper, we develop the concept of "composed 
instruments". We will look at this idea from two 
perspectives: the design of computer systems in the 
context of live performed music and musicological 
considerations. A historical context is developed. 
Examples will be drawn from recent compositions. 
Finally basic concepts from computer science will be 
examined for their relation ship to this concept. 

</abstract>
    <keywords>  Instruments, musicology, composed instrument,  Theremin, Martenot, interaction, streams, MAX.    </keywords>
  </document>
  <document>
    <name>nime2002_161.pdf</name>
    <abstract>
The cicada uses a rapid sequence of buckling ribs to initi-
ate and sustain vibrations in its tymbal plate (the primary
mechanical resonator in the cicada's sound production sys-
tem). The tymbalimba, a music controller based on this
same mechanism, has a row of 4 convex aluminum ribs (as
on the cicada's tymbal) arranged much like the keys on a
calimba. Each rib is spring loaded and capable of snapping
down into a V-shape (a motion referred to as buckling), un-
der the downward force of the user's finger. This energy
generated by the buckling motion is measured by an ac-
celerometer located under each rib and used as the input
to a physical model.

</abstract>
    <keywords> Bioacoustics, Physical Modeling, Controllers, Cicada, Buck- ling mechanism.  </keywords>
  </document>
  <document>
    <name>nime2002_167.pdf</name>
    <abstract> 
This paper describes the hardware and the software of a 
computer-based doppler-sonar system for movement 
detection. The design is focused on simplicity and low-
cost do-it-yourself construction.  

</abstract>
    <keywords>  sonar   </keywords>
  </document>
  <document>
    <name>nime2002_171.pdf</name>
    <abstract> 
This paper describes a technique of multimodal, 
multichannel control of electronic musical devices using 
two control methodologies, the Electromyogram (EMG) 
and relative position sensing. Requirements for the 
application of multimodal interaction theory in the musical 
domain are discussed. We introduce the concept of 
bidirectional complementarity to characterize the 
relationship between the component sensing technologies. 
Each control can be used independently, but together they 
are mutually complementary. This reveals a fundamental 
difference from orthogonal systems. The creation of a 
concert piece based on this system is given as example. 

</abstract>
    <keywords>  Human Computer Interaction, Musical Controllers,  Electromyogram, Position Sensing, Sensor Instruments   </keywords>
  </document>
  <document>
    <name>nime2002_177.pdf</name>
    <abstract>
Active force-feedback holds the potential for precise and
rapid controls.  A high performance device can be built
from a surplus disk drive and controlled from an inex-
pensive microcontroller.  Our new design,The Plank has
only one axis of force-feedback with limited range of
motion. It is being used to explore methods of feeling
and directly manipulating sound waves and spectra suit-
able for live performance of computer music.
</abstract>
    <keywords> Haptics, music controllers, scanned synthesis.  </keywords>
  </document>
  <document>
    <name>nime2002_181.pdf</name>
    <abstract>
Here we propose a novel musical controller which acquires
imaging data of the tongue with a two-dimensional medical
ultrasound scanner. A computer vision algorithm extracts
from the image a discrete tongue shape to control, in real-
time, a musical synthesizer and musical effects. We evalu-
ate the mapping space between tongue shape and controller
parameters and its expressive characteristics.

</abstract>
    <keywords> Tongue model, ultrasound, real-time, music synthesis, speech interface  </keywords>
  </document>
  <document>
    <name>nime2002_186.pdf</name>
    <abstract> 
The Beatbugs are hand-held percussive instruments that  
allow the creation, manipulation, and sharing of rhyth-
mic motifs through a simple interface. When multiple 
Beatbugs are connected in a network, players can form 
large-scale collaborative compositions by interdepen-
dently sharing and developing each other's motifs. Each 
Beatbug player can enter a motif that is then sent through 
a stochastic computerized "Nerve Center" to other play-
ers in the network. Receiving players can decide whether 
to develop the motif further (by continuously manipulat-
ing pitch, timbre, and rhythmic elements using two bend 
sensor antennae) or to keep it in their personal instru-
ment (by entering and sending their own new motifs to 
the group.) The tension between the system's stochastic 
routing scheme and the players' improvised real-time 
decisions leads to an interdependent, dynamic, and con-
stantly evolving musical experience. A musical composi-
tion entitled "Nerve" was written for the system by au-
thor Gil Weinberg. It was premiered on February 2002 
as part of Tod Machover's Toy Symphony [1] in a con-
cert with the Deutsches Symphonie Orchester Berlin, 
conducted by Kent Nagano. The paper concludes with a 
short evaluative discussion of the concert and the week-
long workshops that led to it. 

</abstract>
    <keywords>  Interdependent Musical Networks, group playing, per- cussive controllers.   </keywords>
  </document>
  <document>
    <name>nime2002_192.pdf</name>
    <abstract> 
In this demonstration we will show a variety of com-
puter-based musical instruments designed for live per-
formance. Our design criteria include initial ease of use 
coupled with a long term potential for virtuosity, mini-
mal and low variance latency, and clear and simple 
strategies for programming the relationship between 
gesture and musical result. We present custom control-
lers and unique adaptations of standard gestural inter-
faces, a programmable connectivity processor, a com-
munications protocol called Open Sound Control (OSC), 
and a variety of metaphors for musical control. 

</abstract>
    <keywords>  Expressive control, mapping gestures to acoustic results,  metaphors for musical control, Tactex, Buchla Thunder,  digitizing tablets.   </keywords>
  </document>
  <document>
    <name>nime2002_195.pdf</name>
    <abstract>
The Mutha Rubboard is a musical controller based on the
rubboard, washboard or frottoir metaphor commonly used
in the Zydeco music genre of South Louisiana. It is not only
a metamorphosis of a traditional instrument, but a modern
bridge of exploration into a rich musical heritage. It uses
capacitive and piezo sensing technology to output MIDI and
raw audio data.

This new controller reads the key placement in two parallel
planes by using radio capacitive sensing circuitry expand-
ing greatly on the standard corrugated metal playing sur-
face. The percussive output normally associated with the
rubboard is captured through piezo contact sensors mounted
directly on the keys (the playing implements). Additionally,
mode functionality is controlled by discrete switching on
the keys.

This new instrument is meant to be easily played by both
experienced players and those new to the rubboard. It lends
itself to an expressive freedom by placing the control sur-
face on the chest and allowing the hands to move uninhib-
ited about it or by playing it in the usual way, preserving its
musical heritage.

</abstract>
    <keywords> MIDI controllers, computer music, Zydeco music, interac- tive music, electronic musical instrument, human computer interface, Louisiana heritage, physical modeling, bowl res- onators.  </keywords>
  </document>
  <document>
    <name>nime2002_199.pdf</name>
    <abstract>
Falling Up is an evening-length performance incorporating
dance and theatre with movement-controlled audio/video
playback and processing. The solo show is a collaboration be-
tween Cindy Cummings (performance) and Todd Winkler
(sound, video), first performed at the Dublin Fringe Festival,
2001.  Each thematic section of the work shows a different type
of interactive relationship between movement, video and
sound. This demonstration explains the various technical con-
figurations and aesthetic thinking behind aspects of the work.

</abstract>
    <keywords> Dance, Video processing, Movement sensor, VNS, Very Nervous System  </keywords>
  </document>
  <document>
    <name>nime2002_201.pdf</name>
    <keywords> Hyperbow, Hyperviolin, Hyperinstrument, violin, bow, po- sition sensor, accelerometer, strain sensor  </keywords>
  </document>
  <document>
    <name>nime2003_003.pdf</name>
    <abstract>
In this paper we present a design for the EpipE, a new
expressive electronic music controller based on the Irish
Uilleann Pipes, a 7-note polyphonic reeded woodwind. The
core of this proposed controller design is a continuous
electronic tonehole-sensing arrangement, equally applicable
to other woodwind interfaces like those of the flute, recorder or
Japanese shakuhachi. The controller will initially be used to
drive a physically-based synthesis model, with the eventual
goal being the development of a mapping layer allowing the
EpipE interface to operate as a MIDI-like controller of arbitrary
synthesis models.

</abstract>
    <keywords> Controllers, continuous woodwind tonehole sensor, uilleann pipes, Irish bagpipe, physical modelling, double reed, conical bore, tonehole.  </keywords>
  </document>
  <document>
    <name>nime2003_015.pdf</name>
    <keywords> MIDI Controller, Wind Controller, Breath Control, Human Computer Interaction.  </keywords>
  </document>
  <document>
    <name>nime2003_019.pdf</name>
    <abstract>
The STRIMIDILATOR is an instrument that uses the devia-
tion and the vibration of strings as MIDI-controllers. This
method of control gives the user direct tactile force feedback
and allows for subtle control. The development of the in-
strument and its different functions are described.

</abstract>
    <keywords> MIDI controllers, tactile force feedback, strings.  Figure The STRIMIDILATOR  </keywords>
  </document>
  <document>
    <name>nime2003_024.pdf</name>
    <abstract>
Over the past year the instructors of the Human Computer
Interaction courses at CCRMA have undertaken a technol-
ogy shift to a much more powerful teaching platform. We
describe the technical features of the new Atmel AVR based
platform, contrasting it with the Parallax BASIC Stamp
platform used in the past. The successes and failures of
the new platform are considered, and some student project
success stories described.

</abstract>
  </document>
  <document>
    <name>nime2003_030.pdf</name>
    <abstract>
The Disc Jockey (DJ) software system Mixxx is presented.
Mixxx makes it possible to conduct studies of new interac-
tion techniques in connection with the DJ situation, by its
open design and easy integration of new software modules
and MIDI connection to external controllers. To gain a bet-
ter understanding of working practices, and to aid the design
process of new interfaces, interviews with two contemporary
musicians and DJ's are presented. In contact with these
musicians development of several novel prototypes for DJ
interaction have been made. Finally implementation details
of Mixxx are described.

</abstract>
  </document>
  <document>
    <name>nime2003_036.pdf</name>
  </document>
  <document>
    <name>nime2003_054.pdf</name>
    <abstract>
In this paper, we examine the use of spatial layouts of musical
material for live performance control. Emphasis is given to
software tools that provide for the simple and intuitive
geometric organization of sound material, sound processing
parameters, and higher-level musical structures.

</abstract>
  </document>
  <document>
    <name>nime2003_070.pdf</name>
    <abstract>
This paper first introduces two previous software-based music
instruments designed by the author, and analyses the crucial
importance of the visual feedback introduced by their
interfaces. A quick taxonomy and analysis of the visual
components in current trends of interactive music software is
then proposed, before introducing the reacTable*, a new
project that is currently under development. The reacTable* is
a collaborative music instrument, aimed both at novices and
advanced musicians, which employs computer vision and
tangible interfaces technologies, and pushes further the visual
feedback interface ideas and techniques aforementioned.

</abstract>
  </document>
  <document>
    <name>nime2003_077.pdf</name>
    <abstract>
A handheld electronic musical instrument, named the Bento-
Box, was developed. The motivation was to develop an
instrument which one can easily carry around and play in
moments of free time, for example when riding public
transportation or during short breaks at work. The device was
designed to enable quick learning by having various scales
programmed for different styles of music, and also be
expressive by having hand controlled timbral effects which
can be manipulated while playing. Design analysis and
iteration lead to a compact and ergonomic device. This paper
focuses on the ergonomic design process of the hardware.

</abstract>
    <keywords> MIDI controller, electronic musical instrument, musical instrument design, ergonomics, playability, human computer interface.  </keywords>
  </document>
  <document>
    <name>nime2003_083.pdf</name>
    <keywords> Chemical music, Applied chemistry, Battery Controller.  </keywords>
  </document>
  <document>
    <name>nime2003_087.pdf</name>
    <abstract>
This report details work on the interdisciplinary media
project TGarden. The authors discuss the challenges
encountered while developing a responsive musical
environment for the general public involving wearable,
sensor-integrated clothing as the central interface and input
d e v i c e . T h e p r o j e c t ' s d r a m a t u r g i c a l and
technical/implementation background are detailed to
provide a framework for the creation of a responsive hardware
and software system that reinforces a tangible relationship
between the participant's improvised movement and musical
response. Finally, the authors take into consideration testing
scenarios gathered from public prototypes in two European
locales in 2001 to evaluate user experience of the system.

</abstract>
    <keywords> Gesture, interaction, embodied action, enaction, physical model, responsive environment, interactive musical systems, affordance, interface, phenomenology, energy, kinetics, time constant, induced ballistics, wearable computing, accelerometer, audience participation, dynamical system, dynamic compliance, effort, wearable instrument, augmented physicality.  </keywords>
  </document>
  <document>
    <name>nime2003_091.pdf</name>
    <abstract>
We present a sensor-doll interface as a musical outlet for
personal expression. A doll serves the dual role of being both
an expressive agent and a playmate by allowing solo and
accompanied performance. An internal computer and sensor
system allow the doll to receive input from the user and its
surroundings, and then respond accordingly with musical
feedback. Sets of musical timbres and melodies may be
changed by presenting the doll with a series of themed cloth
hats, each suggesting a different style of play. The doll may
perform by itself and play a number of melodies, or it may
collaborate with the user when its limbs are squeezed or bent.
Shared play is further encouraged by a basic set of aural tones
mimicking conversation.

</abstract>
    <keywords> Musical improvisation, toy interface agent, sensor doll, context awareness.  </keywords>
  </document>
  <document>
    <name>nime2003_095.pdf</name>
  </document>
  <document>
    <name>nime2003_109.pdf</name>
    <abstract>
In the project Sonic City, we have developed a system that
enables users to create electronic music in real time by walking
through and interacting with the urban environment. We
explore the use of public space and everyday behaviours for
creative purposes, in particular the city as an interface and
mobility as an interaction model for electronic music making.
A multi-disciplinary design process resulted in the
implementation of a wearable, context-aware prototype. The
system produces music by retrieving information about
context and user action and mapping it to real-time processing
of urban sounds. Potentials, constraints, and implications of
this type of music creation are discussed.

</abstract>
  </document>
  <document>
    <name>nime2003_116.pdf</name>
    <abstract>
The role of the face and mouth in speech production as well as
non-verbal communication suggests the use of facial action to
control musical sound. Here we document work on the
Mouthesizer, a system which uses a headworn miniature
camera and computer vision algorithm to extract shape
parameters from the mouth opening and output these as MIDI
control changes. We report our experience with various
gesture-to-sound mappings and musical applications, and
describe a live performance which used the Mouthesizer
interface.

</abstract>
    <keywords> Video-based interface; mouth controller; alternative input devices.  </keywords>
  </document>
  <document>
    <name>nime2003_122.pdf</name>
    <keywords> Alternate controller, gesture, microphone technique, vocal performance, performance interface, electronic music.  </keywords>
  </document>
  <document>
    <name>nime2003_129.pdf</name>
    <abstract>
We explore a variety of design criteria applicable to the
creation of collaborative interfaces for musical experience. The
main factor common to the design of most collaborative
interfaces for novices is that musical control is highly
restricted, which makes it possible to easily learn and
participate in the collective experience. Balancing this trade-
off is a key concern for designers, as this happens at the
expense of providing an upward path to virtuosity with the
interface. We attempt to identify design considerations
exemplified by a sampling of recent collaborative devices
primarily oriented toward novice interplay. It is our intention
to provide a non-technical overview of design issues inherent
in configuring multiplayer experiences, particularly for entry-
level players.

</abstract>
    <keywords> Design, collaborative interface, musical experience, multiplayer, novice, musical control.  </keywords>
  </document>
  <document>
    <name>nime2003_135.pdf</name>
    <abstract>
MidiGrid is a computer-based musical instrument, primarily
controlled with the computer mouse, which allows live
performance of MIDI-based musical material by mapping 2-
dimensional position onto musical events. Since its
invention in 1987, it has gained a small, but enthusiastic,
band of users, and has become the primary instrument for
several people with physical disabilities. This paper reviews
its development, uses and user interface issues, and highlights
the work currently in progress for its transformation into
MediaGrid.

</abstract>
  </document>
  <document>
    <name>nime2003_140.pdf</name>
    <abstract>
This paper presents a study of bimanual control applied to
sound synthesis. This study deals with coordination,
cooperation, and abilities of our hands in musical context. We
describe examples of instruments made using subtractive
synthesis, scanned synthesis in Max/MSP and commercial
stand-alone software synthesizers via MIDI communication
protocol. These instruments have been designed according to a
multi-layer-mapping model, which provides modular design.
They have been used in concerts and performance
considerations are discussed too.

</abstract>
    <keywords> Gesture control, mapping, alternate controllers, musical instruments.  </keywords>
  </document>
  <document>
    <name>nime2003_146.pdf</name>
    <abstract>
This paper describes the implementation of Time Delay Neural
Networks (TDNN) to recognize gestures from video images.
Video sources are used because they are non-invasive and do not
inhibit performer's physical movement or require specialist
devices to be attached to the performer which experience has
shown to be a significant problem that impacts musicians
performance and can focus musical rehearsals and performances
upon technical rather than musical concerns (Myatt 2003).
We describe a set of hand gestures learned by an artificial neural
network to control musical parameters expressively in real time.
The set is made up of different types of gestures in order to
investigate:

-aspects of the recognition process
-expressive musical control
-schemes of  parameter mapping
-generalization issues for an extended set for musical

control
The learning procedure of the Neural Network is described
which is based on variations by affine transformations of image
sequences of the hand gestures.
The whole application including the gesture capturing is
implemented in jMax to achieve real time conditions and easy
integration into a musical environment to realize different
mappings and routings of the control stream.
The system represents a practice-based research using actual
music models like compositions and processes of composition
which will follow the work described in the paper.

</abstract>
    <keywords> Gesture Recognition, Artificial Neural Network, Expressive Control, Real-time Interaction  </keywords>
  </document>
  <document>
    <name>nime2003_151.pdf</name>
    <abstract>
This paper describes the artistic projects undertaken at Immersion
Music, Inc. (www.immersionmusic.org) during its three-year
existence. We detail work in interactive performance systems,
computer-based training systems, and concert production.

</abstract>
    <keywords> Interactive computer music systems, gestural interaction, Conductor's Jacket, Digital Baton   </keywords>
  </document>
  <document>
    <name>nime2003_161.pdf</name>
    <keywords> Motion capture, gestural control, mapping.  </keywords>
  </document>
  <document>
    <name>nime2003_164.pdf</name>
    <abstract>
In this paper, we discuss a design principle for the musical
instruments that are useful for both novices and professional
musicians and that facilitate musically rich expression. We
believe that the versatility of conventional musical
instruments causes difficulty in performance. By dynamically
specializing a musical instrument for performing a specific
(genre of) piece, the musical instrument could become more
useful for performing the piece and facilitates expressive
performance. Based on this idea, we developed two new types
of musical instruments, i.e., a "given-melody-based musical
instrument" and a "harmonic-function-based musical
instrument." From the experimental results using two
prototypes, we demonstrate the efficiency of the design
principle.

</abstract>
  </document>
  <document>
    <name>nime2003_170.pdf</name>
    <abstract>
In this paper, we introduce Block Jam, a Tangible User
Interface that controls a dynamic polyrhythmic sequencer
using 26 physical artifacts. These physical artifacts, that we
call blocks, are a new type of input device for manipulating
an interactive music system. The blocks' functional and
topological statuses are tightly coupled to an ad hoc
sequencer, interpreting the user's arrangement of the blocks
as meaningful musical phrases and structures.

We demonstrate that we have created both a tangible and
visual language that enables both the novice and musically
trained users by taking advantage of both their explorative
and intuitive abilities. The tangible nature of the blocks and
the intuitive interface promotes face-to-face collaboration
and social interaction within a single system. The principle
of collaboration is further extended by linking two Block
Jam systems together to create a network.

We discuss our project vision, design rational, related
works, and the implementation of Block Jam prototypes.

Figure 1. A cluster of blocks, note the mother block on the
bottom right

</abstract>
    <keywords> Tangible interface, modular system, polyrhythmic sequencer.  VISION We believe in a future where music will no longer be  considered a linear composition, but a dynamic structure, and musical composition will extend to interaction. We also believe that through the </keywords>
  </document>
  <document>
    <name>nime2003_180.pdf</name>
    <abstract>
In this paper I present the gluiph, a single-board computer that
was conceived as a platform for integrated electronic musical
instruments. It aims to provide new instruments as well as
existing ones with a stronger identity by untethering them
from the often lab-like stage setups built around general pur-
pose computers. The key additions to its core are a flexible
sensor subsystem and multi-channel audio I/O. In contrast to
other stand-alone approaches it retains a higher degree of
flexibility by supporting popular music programming lan-
guages, with Miller Puckette's pd [1] being the current focus.

</abstract>
  </document>
  <document>
    <name>nime2003_184.pdf</name>
    <abstract>
In this paper, we describe a new interface for musical
performance, using the interaction with a graphical user
interface in a powerful manner: the user directly touches a
screen where graphical objects are displayed and can use
several fingers simultaneously to interact with the objects. The
concept of this interface is based on the superposition of the
gesture spatial place and the visual feedback spatial place; i t
gives the impression that the graphical objects are real. This
concept enables a huge freedom in designing interfaces. The
gesture device we have created gives the position of four
fingertips using 3D sensors and the data is performed in the
Max/MSP environment. We have realized two practical
examples of musical use of such a device, using Photosonic
Synthesis and Scanned Synthesis.

</abstract>
    <keywords> HCI, touch screen, multimodality, mapping, direct interaction, gesture devices, bimanual interaction, two-handed, Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2003_201.pdf</name>
    <abstract>
This paper suggests that there is a need for formalizing a
component model of gestural primitive throughput in music
instrument design. The purpose of this model is to construct a
coherent and meaningful interaction between performer and
instrument. Such a model has been implicit in previous research
for interactive performance systems. The model presented here
distinguishes gestural primitives from units of measure of
gestures. The throughput model identifies symmetry between
performance gestures and musical gestures, and indicates a role
for gestural primitives when a performer navigates regions of
stable oscillations in a musical instrument. The use of a high-
dimensional interface tool is proposed for instrument design, for
fine-tuning the mapping between movement sensor data and
sound synthesis control data.

</abstract>
    <keywords> Performance gestures, musical gestures, instrument design, mapping, tuning, affordances, stability.  </keywords>
  </document>
  <document>
    <name>nime2003_208.pdf</name>
    <abstract>
SensorBox is a low cost, low latency, high-resolution
interface for obtaining gestural data from sensors for use in
realtime with a computer-based interactive system. We
discuss its implementation, benefits, current limitations, and
compare it with several popular interfaces for gestural data
acquisition.

</abstract>
    <keywords> Sensors, gestural acquisition, audio interface, interactive music, SensorBox.  </keywords>
  </document>
  <document>
    <name>nime2003_211.pdf</name>
    <abstract>
This software tool, developed in Max/MSP, presents
performers with image files consisting of traditional notation
as well as conducting in the form of video playback. The
impetus for this work was the desire to allow the musical
material for each performer of a given piece to differ with
regard to content and tempo.

</abstract>
    <keywords> Open form, notation, polymeter, polytempi, Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2003_213.pdf</name>
    <abstract>
This document describes modular software supporting live
signal processing and sound file playback within the
Max/MSP environment. Dsp.rack integrates signal
processing, memory buffer recording, and pre-recorded
multi-channel file playback using an interconnected,
programmable signal flow matrix, and an eight-channel i/o
format.

</abstract>
    <keywords> Digital signal processing, Max/MSP, computer music performance, matrix routing, live performance processing.  </keywords>
  </document>
  <document>
    <name>nime2003_216.pdf</name>
    <abstract>
This paper is a demo proposal for a new musical interface
based on a DNA-like double-helix and concepts in character
generation. It contains a description of the interface,
motivations behind developing such an interface, various
mappings of the interface to musical applications, and the
requirements to demo the interface.

</abstract>
    <keywords> Performance, Design, Experimentation, DNA, Big Five.  </keywords>
  </document>
  <document>
    <name>nime2003_218.pdf</name>
    <abstract>
This paper describes a system which uses the output from
head-tracking and gesture recognition software to drive a
parameterized guitar effects synthesizer in real-time.

</abstract>
    <keywords> Head-tracking, gestural control, continuous control, parameterized effects processor.  </keywords>
  </document>
  <document>
    <name>nime2003_222.pdf</name>
    <abstract>
Sodaconductor is a musical interface for generating OSC
control data based on the dynamic physical simulation tool
Sodaconstructor as it can be seen and heard on
http://www.sodaplay.com.

</abstract>
  </document>
  <document>
    <name>nime2003_225.pdf</name>
    <abstract>
Ircam has been deeply involved into gesture analysis and sensing
for about four years now, as several artistic projects demonstrate.
Ircam has often been solicited for sharing software and hardware
tools for gesture sensing, especially devices for the acquisition and
conversion of sensor data, such as the AtoMIC Pro [1][2]. This
demo-paper describes the recent design of a new sensor to MIDI
interface called EoBody1

</abstract>
    <keywords> Gestural controller, Sensor, MIDI, Computer Music.  </keywords>
  </document>
  <document>
    <name>nime2004_001.pdf</name>
    <abstract>
The Tooka was created as an exploration of two person
instruments. We have worked with two Tooka performers to
enhance the original experimental device to make a musical
instrument played and enjoyed by them. The main additions to
the device include: an additional button that behaves as a
music capture button, a bend sensor, an additional thumb-
actuated pressure sensor for vibrato, additional musical
mapping strategies, and new interfacing hardware. These
developments  a rose  through exper iences  and
recommendations from the musicians playing it. In addition to
the changes to the Tooka, this paper describes the learning
process and experiences of the musicians performing with the
Tooka.

</abstract>
    <keywords> Musician-centred design, two-person musical instrument.  </keywords>
  </document>
  <document>
    <name>nime2004_007.pdf</name>
    <abstract>
This paper describes the design of an Electronic Sitar controller, a
digitally modified version of Saraswati's (the Hindu Goddess of
Music) 19-stringed, pumpkin shelled, traditional North Indian
instrument. The ESitar uses sensor technology to extract gestural
information from a performer, deducing music information such
as pitch, pluck timing, thumb pressure, and 3-axes of head tilt to
trigger real-time sounds and graphics. It allows for a variety of
traditional sitar technique as well as new performance methods.
Graphical feedback allows for artistic display and pedagogical
feedback. The ESitar uses a programmable Atmel microprocessor
which outputs control messages via a standard MIDI jack.

</abstract>
  </document>
  <document>
    <name>nime2004_013.pdf</name>
    <keywords> Sound feedback, Karate, Learning environment, Wearable device  </keywords>
  </document>
  <document>
    <name>nime2004_019.pdf</name>
    <abstract>
This article reflects the current state of the reacTable* project,
an electronic music instrument with a tangible table-based
interface, which is currently under development at the
Audiovisual Institute at the Universitat Pompeu Fabra. In this
paper we are focussing on the issue of Dynamic Patching,
which is a particular and unique aspect of the sound synthesis
and control paradigms of the reacTable*. Unlike common
visual programming languages for sound synthesis, which
conceptually separate the patch building process from the
actual musical performance, the reacTable* combines the
construction and playing of the instrument in a unique way.
The tangible interface allows direct manipulation control over
any of the used building blocks, which physically represent
the whole synthesizer function.

</abstract>
  </document>
  <document>
    <name>nime2004_023.pdf</name>
  </document>
  <document>
    <name>nime2004_027.pdf</name>
  </document>
  <document>
    <name>nime2004_031.pdf</name>
    <abstract>
This paper presents a project involving a percussionist play-
ing on a virtual percussion. Both artistic and technical as-
pects of the project are developed. Especially, a method for
strike recognition using the Flock of Birds is presented, as
well as its use for artistic purpose.

</abstract>
    <keywords> Gesture analysis, virtual percussion, strike recognition.  </keywords>
  </document>
  <document>
    <name>nime2004_039.pdf</name>
    <abstract> 
In this paper, we describe an adaptive approach to gesture 
mapping for musical applications which serves as a mapping 
system for music instrument design. A neural network approach is 
chosen for this goal and all the required interfaces and 
abstractions are developed and demonstrated in the Pure Data 
environment. In this paper, we will focus on neural network 
representation and implementation in a real-time musical 
environment. This adaptive mapping is evaluated in different 
static and dynamic situations by a network of sensors sampled at a 
rate of 200Hz in real-time. Finally, some remarks are given on the 
network design and future works. 

</abstract>
    <keywords>  Real-time gesture control, adaptive interfaces, Sensor and actuator  technologies for musical applications, Musical mapping  algorithms and intelligent controllers, Pure Data.   </keywords>
  </document>
  <document>
    <name>nime2004_047.pdf</name>
    <abstract> 
This paper describes the use of evolutionary and artificial life 
techniques in sound design and the development of performance 
mapping to facilitate the real-time manipulation of such sounds 
through some input device controlled by the performer. A 
concrete example of such a system is described which allows 
musicians without detailed knowledge and experience of sound 
synthesis techniques to interactively develop new sounds and 
performance manipulation mappings according to their own 
aesthetic judgements. Experiences with the system are discussed.   

</abstract>
  </document>
  <document>
    <name>nime2004_051.pdf</name>
    <abstract>
In this report, we  discuss Tree Music, an interactive computer
music installation created using GAIA (Graphical Audio Interface
Application), a new open-source interface for controlling the
RTcmix synthesis and effects processing engine. Tree Music,
commissioned by the University of Virginia Art Museum,  used a
wireless camera with a wide-angle lens to capture motion and
occlusion data from exhibit visitors. We show how GAIA was
used to structure and navigate the compositional space, and how
this program supports both graphical and text-based programming
in the same application. GAIA provides a GUI which combines
two open-source applications: RTcmix and Perl.

</abstract>
    <keywords> Composition, new interfaces, interactive systems, open source, Real time audio, GUI controllers, video tracking  </keywords>
  </document>
  <document>
    <name>nime2004_055.pdf</name>
    <abstract>
This essay outlines a framework for understanding new
musical compositions and performances that utilize
pre-existing sound recordings.  In attempting to
articulate why musicians are increasingly using sound
recordings in their creative work,  the author calls for
new performance tools that enable the dynamic use of
pre-recorded music.  
</abstract>
    <keywords> Call and response, turntablism, DJ tools, oral culture </keywords>
  </document>
  <document>
    <name>nime2004_059.pdf</name>
    <abstract> 
When envisaging new digital instruments, designers do not 
have to limit themselves to their sonic capabilities (which can 
be absolutely any), not even to their algorithmic power; they 
must be also especially careful about the instruments' 
conceptual capabilities, to the ways instruments impose or 
suggest to their players new ways of thinking, new ways of 
establishing relations, new ways of interacting, new ways of 
organizing time and textures; new ways, in short, of playing 
new musics. This article explores the dynamic relation that 
builds between the player and the instrument, introducing 
concepts such as efficiency, apprenticeship and learning curve 
It aims at constructing a framework in which the possibilities 
and the diversity of music instruments as well as the 
possibilities and the expressive freedom of human music 
performers could start being evaluated. 

</abstract>
    <keywords>  Musical instruments design, learning curve, apprenticeship,  musical efficiency.   </keywords>
  </document>
  <document>
    <name>nime2004_064.pdf</name>
    <abstract>
This report presents a novel interface for musical performance 
which utilizes a record-player turntable augmented with a 
computation engine and a high-density optical sensing array.  The 
turntable functions as a standalone step sequencer for MIDI events 
transmitted to a computer or another device and it is programmed 
in real-time using visual disks.  The program instructions are 
represented on printed paper disks directly as characters of English 
alphabet that could be read by human as effectively as they are 
picked up by the machine's optical cartridge.  The result is a 
tangible interface that allows the user to manipulate pre-arranged 
musical material by hand, by adding together instrumental tracks to 
form a dynamic mix.  A functional implementation of this interface 
is discussed in view of historical background and other examples 
of electronic instruments for music creation and performance 
incorporating optical turntable as a central element.

</abstract>
    <keywords> Interaction, visualization, tangible interface, controllers, optical  turntable, performance.  </keywords>
  </document>
  <document>
    <name>nime2004_068.pdf</name>
    <abstract>
This paper describes the first system designed to allow chil-
dren to conduct an audio and video recording of an orches-
tra. No prior music experience is required to control the
orchestra, and the system uses an advanced algorithm to
time stretch the audio in real-time at high quality and with-
out altering the pitch. We will discuss the requirements and
challenges of designing an interface to target our particular
user group (children), followed by some system implemen-
tation details. An overview of the algorithm used for audio
time stretching will also be presented. We are currently us-
ing this technology to study and compare professional and
non-professional conducting behavior, and its implications
when designing new interfaces for multimedia. You're the
Conductor is currently a successful exhibit at the Children's
Museum in Boston, USA.

</abstract>
  </document>
  <document>
    <name>nime2004_074.pdf</name>
    <abstract>
The PebbleBox and the CrumbleBag are examples of a gran-
ular interaction paradigm, in which the manipulation of
physical grains of arbitrary material becomes the basis for
interacting with granular sound synthesis models. The sounds
made by the grains as they are manipulated are analysed,
and parameters such as grain rate, grain amplitude and
grain density are extracted. These parameters are then used
to control the granulation of arbitrary sound samples in real
time. In this way, a direct link is made between the hap-
tic sensation of interacting with grains and the control of
granular sounds.

</abstract>
    <keywords> Musical instrument, granular synthesis, haptic  </keywords>
  </document>
  <document>
    <name>nime2004_080.pdf</name>
  </document>
  <document>
    <name>nime2004_087.pdf</name>
    <abstract>
The choice of mapping strategies to effectively map con-
troller variables to sound synthesis algorithms is examined.
Specifically, we look at continuous mappings that have a
geometric representation. Drawing from underlying mathe-
matical theory, this paper presents a way to compare map-
ping strategies, with the goal of achieving an appropriate
match between mapping and musical performance context.
This method of comparison is applied to existing techniques,
while a suggestion is offered on how to integrate and extend
this work through a new implementation.

</abstract>
    <keywords> Mapping, Interface Design, Interpolation, Computational Geometry  </keywords>
  </document>
  <document>
    <name>nime2004_092.pdf</name>
    <abstract>
This paper discusses some of the issues pertaining to the
design of digital musical instruments that are to effectively fill
the role of traditional instruments (i.e. those based on physical
sound production mechanisms). The design and
implementation of a musical instrument that addresses some of
these issues, using scanned synthesis coupled to a "smart"
physical system, is described.

</abstract>
    <keywords> Digital musical instruments, real-time performance, scanned synthesis, pd, tactile interfaces, sensors, Shapetape, mapping.  </keywords>
  </document>
  <document>
    <name>nime2004_096.pdf</name>
    <abstract>
This paper describes an approach to match visual and acous-
tic parameters to produce an animated musical expression.
Music may be generated to correspond to animation, as
described here; imagery may be created to correspond to
music; or both may be developed simultaneously. This ap-
proach is intended to provide new tools to facilitate both
collaboration between visual artists and musicians and ex-
amination of perceptual issues between visual and acoustic
media. As a proof-of-concept, a complete example is devel-
oped with linear fractals as a basis for the animation, and
arranged rhythmic loops for the music. Since both visual
and acoustic elements in the example are generated from
concise specifications, the potential of this approach to cre-
ate new works through parameter space exploration is ac-
centuated, however, there are opportunities for application
to a wide variety of source material. These additional ap-
plications are also discussed, along with issues encountered
in development of the example.

</abstract>
    <keywords> Multimedia creation and interaction, parameter space, visu- alization, sonification.  </keywords>
  </document>
  <document>
    <name>nime2004_100.pdf</name>
    <keywords> Interactive Music Systems, Networking and Control, Voice and Speech Analysis, Auracle, JSyn, TransJam, Linear Pre- diction, Neural Networks, Voice Interface, Open Sound Con- trol  </keywords>
  </document>
  <document>
    <name>nime2004_104.pdf</name>
    <abstract>
In this paper, we propose Thermoscore, a musical score form

-that dynamically alters the temperature of the instrument/play
er interface. We developed the first version of the
Thermoscore display by lining Peltier devices on piano keys.
The system is controlled by MIDI notes-on messages from an
MIDI sequencer, so that a composer can design songs that are
sequences of temperature for each piano key. We also discuss
methodologies for composing with this system, and suggest
two approaches. The first is to make desirable keys (or other
keys) hot. The second one uses chroma-profile, that is, a radar
chart representation of the frequency of pitch notations in the

-piece. By making keys of the same chroma hot in reverse pro
portion to the value of the chroma-profile, it is possible to

-constrain the performer's improvisation and to bring the tonal
ity space close to a certain piece.

</abstract>
    <keywords> musical score, improvisation, peltier device, chroma profile  </keywords>
  </document>
  <document>
    <name>nime2004_112.pdf</name>
    <abstract>
This paper describes ThumbTEC, a novel general purpose
input device for the thumb or finger that is useful in a wide
variety of applications from music to text entry. The device i s
made up of three switches in a row and one miniature joystick
on top of the middle switch. The combination of joystick
direction and switch(es) controls what note or alphanumeric
character is selected by the finger. Several applications are
detailed.

</abstract>
    <keywords> One-Thumb Input Device, HCI, Isometric Joystick, Mobile Computing, Handheld Devices, Musical Instrument.  </keywords>
  </document>
  <document>
    <name>nime2004_120.pdf</name>
    <keywords> Rencon, Turing Test, Musical Expression, Performance Ren- dering  </keywords>
  </document>
  <document>
    <name>nime2004_124.pdf</name>
    <abstract>

This paper describes an approach for playing expressive

music, as it refers to a pianist's expressiveness, with a

tapping-style interface. MIDI-formatted expressive

performances played by pianists were first analyzed and

transformed into performance templates, in which the

deviations from a canonical description was separately

described for each event. Using one of the templates as a

skill complement, a player can play music expressively

over and under the beat level. This paper presents a

scheduler that allows a player to mix her/his own intension

and the expressiveness in the performance template. The

results of a forty-subject user study suggest that using the

expression template contributes the subject's joy of playing

music with the tapping-style performance interface. This

result is also supported by a brain activation study that was

done using a near-infrared spectroscopy (NIRS).

Categories and Subject Descriptors

H.5.5 [Information Interfaces and Presentation]: Sound and

Music Computing methodologies and techniques.

</abstract>
    <keywords>  Rencon, interfaces for musical expression, visualization  </keywords>
  </document>
  <document>
    <name>nime2004_130.pdf</name>
    <abstract>
A series of demonstrations of synthesized acappella songs
based on an auditory morphing using STRAIGHT [5] will
be presented. Singing voice data for morphing were ex-
tracted from the RWCmusic database of musical instru-
ment sound. Discussions on a new extension of the morph-
ing procedure to deal with vibrato will be introduced based
on the statistical analysis of the database and its effect on
synthesized acappella will also be demonstrated.

</abstract>
    <keywords> Rencon, Acappella, RWCdatabase, STRAIGHT, morph- ing  </keywords>
  </document>
  <document>
    <name>nime2004_138.pdf</name>
    <abstract> 
On-the-fly programming is a style of programming in which the 
programmer/performer/composer augments and modifies the 
program while it is running, without stopping or restarting, in 
order to assert expressive, programmable control at runtime.   
Because of the fundamental powers of programming languages, 
we believe the technical and aesthetic aspects of on-the-fly 
programming are worth exploring. 

In this paper, we present a formalized framework for on-the-fly 
programming, based on the ChucK synthesis language, which 
supports a truly concurrent audio programming model with 
sample-synchronous timing, and a highly on-the-fly style of 
programming.  We first provide a well-defined notion of on-the-
fly programming.  We then address four fundamental issues that 
confront the on-the-fly programmer: timing, modularity, 
conciseness, and flexibility.  Using the features and properties of 
ChucK, we show how it solves many of these issues.  In this new 
model, we show that (1) concurrency provides natural modularity 
for on-the-fly programming, (2) the timing mechanism in ChucK 
guarantees on-the-fly precision and consistency, (3) the Chuck 
syntax improves conciseness, and (4) the overall system is a 
useful framework for exploring on-the-fly programming.  Finally, 
we discuss the aesthetics of on-the-fly performance. 
</abstract>
  </document>
  <document>
    <name>nime2004_144.pdf</name>
    <abstract> 
This paper describes the design of an expressive tangible 

interface for cinema editing as a live performance. A short survey 
of live video practices is provided. The Live Cinema instrument is 
a cross between a musical instrument and a film editing tool, 
tailored for improvisational control as well as performance 
presence. Design specifications for the instrument evolved based 
on several types of observations including: our own performances 
in which we used a prototype based on available tools; an analysis 
of performative aspects of contemporary DJ equipment; and an 
evaluation of organizational aspects of several generations of film 
editing tools. Our instrument presents the performer with a large 
canvas where projected images can be grabbed and moved around 
with both hands simultaneously; the performer also has access to 
two video drums featuring haptic display to manipulate the shots 
and cut between streams. The paper ends with a discussion of 
issues related to the tensions between narrative structure and 
hands-on control, live and recorded arts and the scoring of 
improvised films. 

</abstract>
    <keywords>  live cinema, video controller, visual music, DJ, VJ, film editing,  tactile interface, two-hand interaction, improvisation,  performance, narrative structure.   </keywords>
  </document>
  <document>
    <name>nime2004_150.pdf</name>
    <abstract>
A system is introduced that allows a string player to control a
synthesis engine with the gestural skills he is used to. The
implemented system is based on an electric viola and a
synthesis engine that is directly controlled by the unanalysed
audio signal of the instrument and indirectly by control
parameters mapped to the synthesis engine. This method offers
a highly string-specific playability, as it is sensitive to the
kinds of musical articulation produced by traditional playing
techniques. Nuances of sound variation applied by the player
will be present in the output signal even if those nuances are
beyond traditionally measurable parameters like pitch,
amplitude or brightness. The relatively minimal hardware
requirements make the instrument accessible with little
expenditure.

</abstract>
    <keywords> Electronic bowed string instrument, playability, musical instrument design, human computer interface, oscillation controlled sound synthesis  </keywords>
  </document>
  <document>
    <name>nime2004_154.pdf</name>
    <abstract>
We present a system for collaborative musical creation on
mobile wireless networks. The work extends on simple peer-
to-peer file sharing systems towards ad-hoc mobility and
streaming. It extends upon music listening from a passive
act to a proactive, participative activity. The system consists
of a network based interactive music engine and a portable
rendering player. It serves as a platform for experiments on
studying the sense of agency in collaborative creative
process, and requirements for fostering musical satisfaction
in remote collaboration.   

</abstract>
  </document>
  <document>
    <name>nime2004_157.pdf</name>
    <abstract> 
This paper reports our recent developments on sensor 
acquisition systems, taking advantage of computer network 
technology. We present a versatile hardware system which can 
be connected to wireless modules, Analog to Digital Converters, 
and enables Ethernet communication. We are planning to make 
freely available the design of this architecture. We describe also 
several approaches we tested for wireless communication. Such 
technology developments are currently used in our newly 
formed Performance Arts Technology Group.  
 
</abstract>
    <keywords>  Gesture, Sensors, Ethernet, 802.11, Computer Music.      </keywords>
  </document>
  <document>
    <name>nime2004_161.pdf</name>
    <abstract> 
Sonic City is a wearable system enabling the use of the urban 
environment as an interface for real-time electronic music making, 
when walking through and interacting with a city. The device 
senses everyday interactions and surrounding contexts, and maps 
this information in real time to the sound processing of urban 
sounds. We conducted a short-term study with various 
participants using our prototype in everyday settings. This paper 
describes the course of the study and preliminary results in terms 
of how the participants used and experienced the system. These 
results showed that the city was perceived as the main performer 
but that the user improvised different tactics and ad hoc 
interventions to actively influence and participate in how the 
music was created. 

</abstract>
    <keywords>  User study, new interface for musical expression, interactive  music, wearable computing, mobility, context-awareness.   </keywords>
  </document>
  <document>
    <name>nime2004_165.pdf</name>
    <abstract> 
 This paper begins by evaluating various systems in terms of 
factors for building interactive audiovisual environments. The 
main issues for flexibility and expressiveness in the generation of 
dynamic sounds and images are then isolated. The design and 
development of an audiovisual system prototype is described at 
the end.  

</abstract>
    <keywords>  Audiovisual, composition, performance, gesture, image,  representation, mapping, expressiveness.   </keywords>
  </document>
  <document>
    <name>nime2004_169.pdf</name>
    <abstract>
We describe a simple, computationally light, real-time sys-
tem for tracking the lower face and extracting information
about the shape of the open mouth from a video sequence.
The system allows unencumbered control of audio synthesis
modules by action of the mouth. We report work in progress
to use the mouth controller to interact with a physical model
of sound production by the avian syrinx.

</abstract>
    <keywords> Mouth Controller, Face Tracking, Bioacoustics  </keywords>
  </document>
  <document>
    <name>nime2004_177.pdf</name>
    <keywords> Improvisation support, jam session, melody correction, N-gram model, melody modeling, musical instrument  </keywords>
  </document>
  <document>
    <name>nime2004_181.pdf</name>
    <abstract>
This paper describes new work and creations of LEMUR, a
group of artists and technologists creating robotic musical
instruments.

</abstract>
  </document>
  <document>
    <name>nime2004_185.pdf</name>
    <abstract>
Though musical performers routinely use eye 
movements to communicate with each other during 
musical performances, very few performers or composers 
have used eye tracking devices to direct musical 
compositions and performances.  EyeMusic is a system 
that uses eye movements as an input to electronic music 
compositions.  The eye movements can directly control 
the music, or the music can respond to the eyes moving 
around a visual scene.  EyeMusic is implemented so that 
any composer using established composition software 
can incorporate prerecorded eye movement data into their 
musical compositions.

</abstract>
    <keywords> Electronic music composition, eye movements, eye  tracking, human-computer interaction, Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2004_189.pdf</name>
    <abstract> 
When working with sample-based media, a performer is 
managing timelines, loop points, sample parameters and effects 
parameters. The Slidepipe is a performance controller that gives 
the artist a visually simple way to work with their material. Its 
design is modular and lightweight, so it can be easily transported 
and quickly assembled. Also, its large stature magnifies the 
gestures associated with its play, providing a more convincing 
performance. In this paper, I will describe what the controller is, 
how this new controller interface has affected my live 
performance, and how it can be used in different performance 
scenarios. 

</abstract>
    <keywords>  Controller, Sample Manipulation, Live Performance, Open Sound  Control, Human Computer Interaction   </keywords>
  </document>
  <document>
    <name>nime2004_193.pdf</name>
    <abstract>
This paper describes a theory for modulated objects based on
observations of recent musical interface design trends. The
theory implies extensions to an object-based approach to
controller design. Combining NIME research with
ethnographic study of shamanic traditions. The author
discusses the creation of new controllers based on the
shamanic use of ritual objects.

</abstract>
    <keywords> Music and Video Controllers, New Interface Design, Music Composition, Multimedia,  Mythology, Shamanism, Ecoacoustics  </keywords>
  </document>
  <document>
    <name>nime2004_199.pdf</name>
    <abstract> 
The Epipe is a novel electronic woodwind controller with 
continuous tonehole coverage sensing, an initial design for which 
was introduced at NIME '03. Since then, we have successfully 
completed two fully operational prototypes. This short paper 
describes some of the issues encountered during the design and 
construction of this controller. It also details our own early 
experiences and impressions of the interface as well as its 
technical specifications. 

</abstract>
    <keywords>    woodwind controller, variable tonehole control, MIDI, capacitive  sensing    </keywords>
  </document>
  <document>
    <name>nime2004_201.pdf</name>
    <abstract> 
This paper describes the SillyTone Squish Factory, a haptically 
engaging musical interface.  It contains the motivation behind 
the device's development, a description of the interface, various 
mappings of the interface to musical applications, details of its 
construction, and the requirements to demo the interface. 

</abstract>
  </document>
  <document>
    <name>nime2004_203.pdf</name>
    <abstract> 
StickMusic is an instrument comprised of two haptic devices, a 
joystick and a mouse, which control a phase vocoder in real time. 
The purpose is to experiment with ideas of how to apply haptic 
feedback when controlling synthesis algorithms that have no 
direct analogy to methods of generating sound in the physical 
world. 

</abstract>
    <keywords>  haptic feedback, gestural control, performance, joystick, mouse   </keywords>
  </document>
  <document>
    <name>nime2004_205.pdf</name>
    <abstract> 
High capacity of transmission lines (Ethernet in particular) is 
much higher than what imposed by MIDI today. So it is possible 
to use capturing interfaces with high-speed and high-resolution, 
thanks to the OSC protocol, for musical synthesis (either in real-
time or non real-time). These new interfaces offer many 
advantages, not only in the area of musical composition with use 
of sensors but also in live and interactive performances. In this 
manner, the processes of calibration and signal processing are 
delocalized on a personal computer and augments possibilities of 
processing. In this demo, we present two hardware interfaces 
developed in La kitchen with corresponding processing to achieve 
a high-resolution, high-speed sensor processing for musical 
applications.    

</abstract>
    <keywords>  Interface, Sensors, Calibration, Precision, OSC, Pure Data,  Max/MSP.   </keywords>
  </document>
  <document>
    <name>nime2004_207.pdf</name>
    <abstract>
We will discuss the case study of application of the Virtual
Musical Instrument and Sound Synthesis. Doing this
application, the main subject is advanced Mapping Interface in
order to connect these. For this experiment, our discussion
also refers to Neural Network, as well as a brief introduction of
the Virtual Musical Instrument "Le SuperPolm" and Gesture
Controller "BodySuit".

</abstract>
    <keywords> Virtual Musical Instrument, Gesture Controller, Mapping Interface  </keywords>
  </document>
  <document>
    <name>nime2004_209.pdf</name>
    <abstract>
In this paper, we describe a new MIDI controller, the Light
Pipes.  The Light Pipes are a series of pipes that respond to
incident light.  The paper will discuss the design of the
instrument, and the prototype we built.  A piece was composed
for the instrument using algorithms designed in Pure Data.

</abstract>
    <keywords> Controllers, MIDI, light sensors, Pure Data.  </keywords>
  </document>
  <document>
    <name>nime2004_211.pdf</name>
    <abstract>
In this paper, I describe a realtime sampling system for the
turntablist, and the hardware and software design of the second
prototype, 16padjoystickcontroller.

</abstract>
    <keywords> DJ, Turntablism, Realtime Sampling, MAX/MSP, Microchip PIC microcontroller, MIDI  </keywords>
  </document>
  <document>
    <name>nime2004_213.pdf</name>
    <abstract>
This paper describes the design and on-going development of
an expressive gestural MIDI interface and how this could
enhance live performance of electronic music.

</abstract>
    <keywords> gestural control, mapping, Pure Data (pd), accelerometers, MIDI, microcontrollers, synthesis, musical instruments  </keywords>
  </document>
  <document>
    <name>nime2004_215.pdf</name>
    <abstract> 
This paper proposes an interface for improvisational ensemble 
plays which synthesizes musical sounds and graphical images on 
the floor from people's act of "walking." The aim of this paper is 
to develop such a system that enables nonprofessional people in 
our public spaces to play good contrapuntal music without any 
knowledge of music theory. The people are just walking. This 
system is based on the i-trace system [1] which can capture the 
people's behavior and give some visual feedback.  

</abstract>
    <keywords>  Improvisational Ensemble Play, Contrapuntal Music, Human  Tracking, Traces, Spatially Augmented Reality   </keywords>
  </document>
  <document>
    <name>nime2005_002.pdf</name>
  </document>
  <document>
    <name>nime2005_005.pdf</name>
    <keywords> Infra-instruments, hyperinstruments, meta-instruments, virtual instruments, design concepts and principles.  </keywords>
  </document>
  <document>
    <name>nime2005_011.pdf</name>
    <abstract> 
In this paper, we introduce and analyze four gesture-controlled 
musical instruments. We briefly discuss the test platform designed 
to allow for rapid experimentation of new interfaces and control 
mappings. We describe our design experiences and discuss the 
effects of system features such as latency, resolution and lack of 
tactile feedback. The instruments use virtual reality hardware and 
computer vision for user input, and three-dimensional stereo 
vision as well as simple desktop displays for providing visual 
feedback. The instrument sounds are synthesized in real-time 
using physical sound modeling. 

</abstract>
  </document>
  <document>
    <name>nime2005_023.pdf</name>
    <abstract> 
In this paper we study the potential and the challenges posed by 
multi-user instruments, as tools that can facilitate interaction and 
responsiveness not only between performers and their instrument 
but also between performers as well. Several previous studies and 
taxonomies are mentioned, after what different paradigms 
exposed with examples based on traditional mechanical acoustic 
instruments. In the final part, several existing systems and 
implementations, now in the digital domain, are described and 
identified according to the models and paradigms previously 
introduced. 

</abstract>
    <keywords>  Multi-user instruments, collaborative music, new instruments  design guidelines.   </keywords>
  </document>
  <document>
    <name>nime2005_027.pdf</name>
    <abstract> 
This paper will investigate a variety of alternate controllers that 
are making an impact in interactive entertainment, particularly in 
the video game industry.  Since the late 1990's, the surging 
popularity of rhythmic and musical performance games in 
Japanese arcades has led to the development of new interfaces and 
alternate controllers for the consumer market worldwide.  
Rhythm action games such as Dance Dance Revolution, Taiko No 
Tatsujin (Taiko: Drum Master), and Donkey Konga are 
stimulating collaborative gameplay and exposing consumers to 
custom controllers designed specifically for musical and physical 
interaction. We are witnessing the emergence and acceptance of 
these breakthrough controllers and models for gameplay as an 
international cultural phenomenon penetrating the video game and 
toy markets in record numbers.   
Therefore, it is worth considering the potential benefits to 
developers of musical interfaces, electronic devices and alternate 
controllers in light of these new and emerging opportunities, 
particularly in the realm of video gaming, toy development, 
arcades, and other interactive entertainment experiences.  

</abstract>
  </document>
  <document>
    <name>nime2005_038.pdf</name>
    <abstract> 
The Self-Contained Unified Bass Augmenter (SCUBA) is a new 
augmentative OSC (Open Sound Control) [5] controller for the 
tuba. SCUBA adds new expressive possibilities to the existing 
tuba interface through onboard sensors.  These sensors provide 
continuous and discrete user-controlled parametric data to be 
mapped at will to signal processing parameters, virtual instrument 
control parameters, sound playback, and various other functions.  
In its current manifestation, control data is mapped to change the 
processing of the instrument's natural sound in Pd (Pure Data) [3]. 
SCUBA preserves the unity of the solo instrument interface by 
acoustically mixing direct and processed sound in the instrument's 
bell via mounted satellite speakers, which are driven by a 
subwoofer below the performer's chair.  The end result augments 
the existing interface while preserving its original unity and 
functionality. 

</abstract>
    <keywords>  Interactive music, electro-acoustic musical instruments, musical  instrument design, human computer interface, signal processing,  Open Sound Control (OSC)   </keywords>
  </document>
  <document>
    <name>nime2005_042.pdf</name>
    <abstract>
This paper presents a novel controller built to exploit the
physical behaviour of a simple dynamical system, namely a
spinning wheel. The phenomenon of gyroscopic precession
causes the instrument to slowly oscillate when it is spun
quickly, providing the performer with proprioceptive feed-
back. Also, due to the mass of the wheel and tire and the
resulting rotational inertia, it maintains a relatively con-
stant angular velocity once it is set in motion. Various sen-
sors were used to measure continuous and discrete quantities
such as the the angular frequency of the wheel, its spatial
orientation, and the performer's finger pressure. In addi-
tion, optical and hall-effect sensors detect the passing of a
spoke-mounted photodiode and two magnets. A base soft-
ware layer was developed in Max/MSP and various patches
were written with the goal of mapping the dynamic behavior
of the wheel to varied musical processes.

</abstract>
    <keywords> HCI, Digital Musical Instruments, Gyroscopic Precession, Rotational Inertia, Open Sound Control  </keywords>
  </document>
  <document>
    <name>nime2005_046.pdf</name>
    <abstract> 
The Smart Controller is a portable hardware device that responds 
to input control voltage, OSC, and MIDI messages; producing 
output control voltage, OSC, and MIDI messages (depending 
upon the loaded custom patch). The Smart Controller is a stand 
alone device; a powerful, reliable, and compact instrument 
capable of reducing the number of electronic modules required in 
a live performance or installation, particularly the requirement of 
a laptop computer.  More powerful, however, is the Smart 
Controller Workbench, a complete interactive development 
environment. In addition to enabling the composer to create and 
debug their patches, the Smart Controller Workbench accurately 
simulates the behaviour of the hardware, and functions as an in-
circuit debugger that enables the performer to remotely monitor, 
modify, and tune patches running in an installation without the 
requirement of stopping or interrupting the live performance. 

</abstract>
    <keywords>  Control Voltage, Open Sound Control, Algorithmic Composition,  MIDI, Sound Installations, programmable logic control,  synthesizers, electronic music, Sensors, Actuators, Interaction.   </keywords>
  </document>
  <document>
    <name>nime2005_050.pdf</name>
    <abstract>
This paper describes an installation created by LEMUR
(League of Electronic Musical Urban Robots) in January, 2005.
The installation included over 30 robotic musical instruments
and a multi-projector real-time video projection and was
controllable and programmable over a MIDI network. The
installation was also controllable remotely via the Internet and
could be heard and viewed via room mics and a robotic web
cam connected to a streaming server.

</abstract>
  </document>
  <document>
    <name>nime2005_060.pdf</name>
    <abstract>
When learning a classical instrument, people often either
take lessons in which an existing body of "technique" is de-
livered, evolved over generations of performers, or in some
cases people will "teach themselves" by watching people play
and listening to existing recordings. What does one do with
a complex new digital instrument?

In this paper I address this question drawing on my expe-
rience in learning several very different types of sophisticated
instruments: the Glove Talk II real-time gesture-to-speech
interface, the Digital Marionette controller for virtual 3D
puppets, and pianos and keyboards. As the primary user
of the first two systems, I have spent hundreds of hours
with Digital Marionette and Glove-Talk II, and thousands
of hours with pianos and keyboards (I continue to work as
a professional musician). I will identify some of the under-
lying principles and approaches that I have observed during
my learning and playing experience common to these instru-
ments. While typical accounts of users learning new inter-
faces generally focus on reporting beginner's experiences, for
various practical reasons, this is fundamentally different by
focusing on the expert's learning experience.

</abstract>
    <keywords> performance, learning new instruments  </keywords>
  </document>
  <document>
    <name>nime2005_065.pdf</name>
    <keywords>   Adaptive System, Sound Installation, Smart Interfaces, Music  Robots, Spatial Music, Conscious Subconscious Interaction.   </keywords>
  </document>
  <document>
    <name>nime2005_080.pdf</name>
    <abstract> 
McBlare is a robotic bagpipe player developed by the 
Robotics Institute at Carnegie Mellon University. McBlare 
plays a standard set of bagpipes, using a custom air 
compressor to supply air and electromechanical "fingers" to 
control the chanter. McBlare is MIDI controlled, allowing 
for simple interfacing to a keyboard, computer, or hardware 
sequencer. The control mechanism exceeds the measured 
speed of expert human performers. On the other hand, 
human performers surpass McBlare in their ability to 
compensate for limitations and imperfections in reeds, and 
we discuss future enhancements to address these problems. 
McBlare has been used to perform traditional bagpipe 
music as well as experimental computer generated music. 

</abstract>
    <keywords> bagpipes, robot, music, instrument, MIDI   </keywords>
  </document>
  <document>
    <name>nime2005_085.pdf</name>
    <abstract>
In this report, we describe our development on the Max/MSP
toolbox MnM dedicated to mapping between gesture and
sound, and more generally to statistical and machine learning
methods. This library is built on top of the FTM library, which
enables the efficient use of matrices and other data structures
in Max/MSP. Mapping examples are described based on
various matrix manipulations such as Single Value
Decomposition. The FTM and MnM libraries are freely
available.

</abstract>
    <keywords> Mapping, interface design, matrix, Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2005_089.pdf</name>
    <abstract>
This  paper  describes DspMap,  a graphical  user  interface (GUI)
designed to assist  the dynamic routing of signal generators and
modifiers currently being developed at the International Academy
of Media Arts &amp; Sciences. Instead of relying on traditional box-
and-line approaches, DspMap proposes a design paradigm where
connections are determined by the relative positions of the various
elements in a single virtual space.

</abstract>
    <keywords> Graphical  user  interface,  real-time  performance,  map,  dynamic routing  </keywords>
  </document>
  <document>
    <name>nime2005_093.pdf</name>
    <abstract>
The breath pressure signal applied to wind music instru-
ments is generally considered to be a slowly varying func-
tion of time. In a context of music control, this assumption
implies that a relatively low digital sample rate (100-200
Hz) is sufficient to capture and/or reproduce this signal.
We tested this assumption by evaluating the frequency con-
tent in breath pressure, particularly during the use of ex-
tended performance techniques such as growling, humming,
and flutter tonguing. Our results indicate frequency content
in a breath pressure signal up to about 10 kHz, with espe-
cially significant energy within the first 1000 Hz. We further
investigated the frequency response of several commercially
available pressure sensors to assess their responsiveness to
higher frequency breath signals. Though results were mixed,
some devices were found capable of sensing frequencies up
to at least 1.5 kHz. Finally, similar measurements were con-
ducted with Yamaha WX11 and WX5 wind controllers and
results suggest that their breath pressure outputs are sam-
pled at about 320 Hz and 280 Hz, respectively.

</abstract>
    <keywords> Breath Control, Wind Controller, Breath Sensors  </keywords>
  </document>
  <document>
    <name>nime2005_097.pdf</name>
    <abstract> 
Tangible Acoustic Interfaces (TAI) rely on various acoustic-
sensing technologies, such as sound source location and acoustic 
imaging, to detect the position of contact of users interacting with 
the surface of solid materials. With their ability to transform 
almost any physical objects, flat or curved surfaces and walls into 
interactive interfaces, acoustic sensing technologies show a 
promising way to bring the sense of touch into the realm of 
computer interaction. Because music making has been closely 
related to this sense during centuries, an application of particular 
interest is the use of TAI's for the design of new musical 
instruments that matches the physicality and expressiveness of 
classical instruments. This paper gives an overview of the various 
acoustic-sensing technologies involved in the realisation of TAI's 
and develops on the motivation underlying their use for the design 
of new musical instruments.  

</abstract>
    <keywords>  Tangible interfaces, new musical instruments design.   </keywords>
  </document>
  <document>
    <name>nime2005_101.pdf</name>
  </document>
  <document>
    <name>nime2005_105.pdf</name>
    <abstract>
This paper aims to present some perspectives on mapping
embouchure gestures of flute players and their use as control
variables. For this purpose, we have analyzed several types
of sensors, in terms of sensitivity, dimension, accuracy and
price, which can be used to implement a system capable of
mapping embouchure parameters such as air jet velocity and
air jet direction. Finally, we describe the implementation
of a sensor system used to map embouchure gestures of a
classical Boehm flute.

</abstract>
    <keywords> Embouchure, air pressure sensors, hot wires, mapping, aug- mented flute.  </keywords>
  </document>
  <document>
    <name>nime2005_109.pdf</name>
    <keywords>  Haptic, interaction, sound, music, control, installation.   </keywords>
  </document>
  <document>
    <name>nime2005_115.pdf</name>
    <abstract> 
We report on The Manual Input Sessions, a series of audiovisual 
vignettes which probe the expressive possibilities of free-form 
hand gestures. Performed on a hybrid projection system which 
combines a traditional analog overhead projector and a digital PC 
video projector, our vision-based software instruments generate 
dynamic sounds and graphics solely in response to the forms and 
movements of the silhouette contours of the user's hands. 
Interactions and audiovisual mappings which make use of both 
positive (exterior) and negative (interior) contours are discussed. 

</abstract>
    <keywords>  Audiovisual performance, hand silhouettes, computer vision,  contour analysis, sound-image relationships, augmented reality.   </keywords>
  </document>
  <document>
    <name>nime2005_121.pdf</name>
    <abstract>
The HandySinger system is a personified tool developed
to naturally express a singing voice controlled by the ges-
tures of a hand puppet. Assuming that a singing voice is a
kind of musical expression, natural expressions of the singing
voice are important for personification. We adopt a singing
voice morphing algorithm that effectively smoothes out the
strength of expressions delivered with a singing voice. The
system's hand puppet consists of a glove with seven bend
sensors and two pressure sensors. It sensitively captures
the user's motion as a personified puppet's gesture. To
synthesize the different expressional strengths of a singing
voice, the "normal" (without expression) voice of a particu-
lar singer is used as the base of morphing, and three differ-
ent expressions, "dark," "whisper" and "wet," are used as
the target. This configuration provides musically expressed
controls that are intuitive to users. In the experiment, we
evaluate whether 1) the morphing algorithm interpolates
expressional strength in a perceptual sense, 2) the hand-
puppet interface provides gesture data at sufficient resolu-
tion, and 3) the gestural mapping of the current system
works as planned.

</abstract>
    <keywords> Personified Expression, Singing Voice Morphing, Voice Ex- pressivity, Hand-puppet Interface  </keywords>
  </document>
  <document>
    <name>nime2005_127.pdf</name>
    <abstract> 
The central role of the face in social interaction and non-verbal 
communication suggest we explore facial action as a means of 
musical expression. This paper presents the design, 
implementation, and preliminary studies of a novel system 
utilizing face detection and optic flow algorithms to associate 
facial movements with sound synthesis in a topographically 
specific fashion. We report on our experience with various 
gesture-to-sound mappings and applications, and describe our 
preliminary experiments at musical performance using the 
system. 

</abstract>
    <keywords>  Video-based musical interface; gesture-based interaction; facial  expression; facial therapy interface.   </keywords>
  </document>
  <document>
    <name>nime2005_132.pdf</name>
    <abstract>
In this paper we present an example of the use of the singing
voice as a controller for digital music synthesis. The analy-
sis of the voice with spectral processing techniques, derived
from the Short-Time Fourier Transform, provides ways of
determining a performer's vocal intentions. We demonstrate
a prototype, in which the extracted vocal features drive the
synthesis of a plucked bass guitar. The sound synthesis stage
includes two different synthesis techniques, Physical Models
and Spectral Morph.

</abstract>
    <keywords> Singing voice, musical controller, sound synthesis, spectral processing.  </keywords>
  </document>
  <document>
    <name>nime2005_136.pdf</name>
    <abstract>
Electronic Musical Instrument Design is an excellent vehicle
for bringing students from multiple disciplines together to
work on projects, and help bridge the perennial gap between
the arts and the sciences. This paper describes how at Tufts
University, a school with no music technology program,
students from the engineering (electrical, mechanical, and
computer), music, performing arts, and visual arts areas use
their complementary skills, and teach each other, to develop
new devices and systems for music performance and control.

</abstract>
    <keywords> Science education, music education, engineering, electronic music, gesture controllers, MIDI.  </keywords>
  </document>
  <document>
    <name>nime2005_140.pdf</name>
    <abstract>
The [hid] toolkit is a set of software objects for designing
computer-based gestural instruments. All too frequently,
computer-based performers are tied to the keyboard-mouse-
monitor model, narrowly constraining the range of possible
gestures. A multitude of gestural input devices are readily
available, making it easy to utilize a broader range of ges-
tures. Human Interface Devices (HIDs) such as joysticks,
tablets, and gamepads are cheap and can be good musical
controllers. Some even provide haptic feedback. The [hid]
toolkit provides a unified, consistent framework for getting
gestural data from these devices, controlling the feedback,
and mapping this data to the desired output. The [hid]
toolkit is built in Pd, which provides an ideal platform for
this work, combining the ability to synthesize and control
audio and video. The addition of easy access to gestural
data allows for rapid prototypes. A usable environment
also makes computer music instrument design accessible to
novices.

</abstract>
    <keywords> Instrument design, haptic feedback, gestural control, HID  </keywords>
  </document>
  <document>
    <name>nime2005_144.pdf</name>
    <abstract> 
An experimental study comparing different user interfaces for a 
virtual drum is reported. Virtual here means that the drum is not a 
physical object. 16 subjects played the drum on five different 
interfaces and two metronome patterns trying to match their hits 
to the metronome clicks. Temporal accuracy of the playing was 
evaluated. The subjects also rated the interfaces subjectively. The 
results show that hitting the drum alternately from both sides with 
motion going through the drum plate was less accurate than the 
traditional one sided hitting. A physical stick was more accurate 
than a virtual computer graphic stick. Visual feedback of the drum 
slightly increased accuracy compared to receiving only auditory 
feedback. Most subjects evaluated the physical stick to offer a 
better feeling and to be more pleasant than the virtual stick. 

</abstract>
    <keywords>  Virtual drum, user interface, feedback, musical instrument design,  virtual reality, sound control, percussion instrument.   </keywords>
  </document>
  <document>
    <name>nime2005_148.pdf</name>
    <abstract> 
This paper takes the reader through various elements of the Go-
ingPublik sound artwork for distributive ensemble and introduces 
the Realtime Score Synthesis tool (RSS) used as a controller in 
the work. The collaboration between artists and scientists, details 
concerning the experimental hardware and software, and new 
theories of sound art are briefly explained and illustrated. The 
scope of this project is too broad to be fully covered in this paper, 
therefore the selection of topics made attempts to draw attention 
to the work itself and balance theory with practice. 
 

</abstract>
    <keywords>  Mobile Multimedia, Wearable Computers, Score Synthesis,  Sound Art, System Research, HCIs      </keywords>
  </document>
  <document>
    <name>nime2005_152.pdf</name>
    <keywords>  Motion tracking, mapping strategies, public installation, multiple  participants music interfaces.      </keywords>
  </document>
  <document>
    <name>nime2005_156.pdf</name>
    <abstract> 
This paper describes software tools used to create java 
applications for performing music using mobile phones.  The 
tools provide a means for composers working in the Pure Data 
composition environment to design and audition performances 
using ensembles of mobile phones. These tools were developed as 
part of a larger project motivated by the desire to allow large 
groups of non-expert players to perform music based on just 
intonation using ubiquitous technology. The paper discusses the 
process that replicates a Pure Data patch so that it will operate 
within the hardware and software constraints of the Java 2 Micro 
Edition. It also describes development of objects that will enable 
mobile phone performances to be simulated accurately in PD and 
to audition microtonal tuning implemented using MIDI in the 
j2me environment. These tools eliminate the need for composers 
to compose for mobile phones by writing java code. In a single 
desktop application, they offer the composer the flexibility to 
write music for multiple phones. 

</abstract>
    <keywords>    Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media  Performance; Just Intonation.   </keywords>
  </document>
  <document>
    <name>nime2005_160.pdf</name>
    <abstract> 
This paper details the motivations, design, and realization of 
Sustainable, a dynamic, robotic sound installation that employs a 
generative algorithm for music and sound creation.  The piece is 
comprised of seven autonomous water gong nodes that are 
networked together by water tubes to distribute water throughout 
the system.  A water resource allocation algorithm guides this 
distribution process and produces an ever-evolving sonic and 
visual texture.  A simple set of behaviors govern the individual 
gongs, and the system as a whole exhibits emergent properties 
that yield local and large scale forms in sound and light. 

</abstract>
  </document>
  <document>
    <name>nime2005_164.pdf</name>
    <abstract> 
We present our work in the development of an interface for an 
actor/singer and its use in performing. Our work combines aspects 
of theatrical music with technology. Our interface has allowed the 
development of a new vocabulary for musical and theatrical 
expression and the possibility for merging classical and 
experimental music. It gave rise to a strong, strange, 
unpredictable, yet coherent, "character" and opens up the 
possibility for a full performance that will explore aspects of 
voice, theatrical music and, in the future, image projection.  
</abstract>
    <keywords>  Theatrical music, computer interaction, voice, gestural control.   </keywords>
  </document>
  <document>
    <name>nime2005_168.pdf</name>
    <abstract>
This paper describes the development, function and
performance contexts of a digital musical instrument called
"boomBox."  The instrument is a wireless, orientation-aware
low-frequency, high-amplitude human motion controller for
live and sampled sound.  The instrument has been used in
performance and sound installation contexts.  I describe some
of what I have learned from the project herein.

</abstract>
  </document>
  <document>
    <name>nime2005_176.pdf</name>
    <abstract>
In this paper, we describe a course of research investigating the
potential for new types of music made possible by location
tracking and wireless technologies.  Listeners walk around
downtown Culver City, California and explore a new type of
musical album by mixing together songs and stories based on
their movement.  By using mobile devices as an interface, we
can create new types of musical experiences that allow
listeners to take a more interactive approach to an album.

</abstract>
    <keywords> Mobile Music, Digital Soundscape, Location-Based Entertainment, Mobility, Interactive Music, Augmented Reality  </keywords>
  </document>
  <document>
    <name>nime2005_180.pdf</name>
    <abstract>
Bangarama is a music controller using headbanging as the primary
interaction metaphor. It consists of a head-mounted tilt sensor and a
guitar-shaped controller that does not require complex finger posi-
tions. We discuss the specific challenges of designing and building
this controller to create a simple, yet responsive and playable in-
strument, and show how ordinary materials such as plywood, tin
foil, and copper wire can be turned into a device that enables a fun,
collaborative music-making experience.

</abstract>
    <keywords> head movements, music controllers, interface design, input devices  </keywords>
  </document>
  <document>
    <name>nime2005_184.pdf</name>
    <abstract> 
In recent years Computer Network-Music has increasingly 
captured the attention of the Computer Music Community. With 
the advent of Internet communication, geographical displacement 
amongst the participants of a computer mediated music 
performance achieved world wide extension. However, when 
established over long distance networks, this form of musical 
communication has a fundamental problem: network latency (or 
net-delay) is an impediment for real-time collaboration. From a 
recent study, carried out by the authors, a relation between 
network latency tolerance and Music Tempo was established. 
This result emerged from an experiment, in which simulated 
network latency conditions were applied to the performance of 
different musicians playing jazz standard tunes. 
The Public Sound Objects (PSOs) project is web-based shared 
musical space, which has been an experimental framework to 
implement and test different approaches for on-line music 
communication. This paper describe features implemented in the 
latest version of the PSOs system, including the notion of a 
network-music instrument incorporating latency as a software 
function, by dynamically adapting its tempo to the 
communication delay measured in real-time. 

</abstract>
    <keywords>  Network Music Instruments; Latency in Real-Time Performance;  Interface-Decoupled Electronic Musical Instruments; Behavioral  Driven Interfaces; Collaborative Remote Music Performance;    </keywords>
  </document>
  <document>
    <name>nime2005_188.pdf</name>
    <abstract>
We present the Pin&amp;Play&amp;Perform system: an interface in
the form of a tablet on which a number of physical controls
can be added, removed and arranged on the fly. These con-
trols can easily be mapped to existing music sofware using
the MIDI protocol. The interface provides a mechanism for
direct manipulation of application parameters and events
through a set of familiar controls, while also encouraging a
high degree of customisation through the ability to arrange,
rearrange and annotate the spatial layout of the interface
components on the surface of the tablet.

The paper describes how we have realized this concept us-
ing the Pin&amp;Play technology. As an application example, we
describe our experiences in using our interface in conjunc-
tion with Propellerheads' Reason, a popular piece of music
synthesis software.

</abstract>
    <keywords> tangible interface, rearrangeable interface, midi controllers  </keywords>
  </document>
  <document>
    <name>nime2005_196.pdf</name>
    <abstract> 
ChucK is a programming language for real-time sound synthesis.  
It provides generalized audio abstractions and precise control over 
timing and concurrency - combining the rapid-prototyping 
advantages of high-level programming tools, such as Pure Data, 
with the flexibility and controllability of lower-level, text-based 
languages like C/C++.  In this paper, we present a new time-based 
paradigm for programming controllers with ChucK.  In addition to 
real-time control over sound synthesis, we show how features 
such as dynamic patching, on-the-fly controller mapping, multiple 
control rates, and precisely-timed recording and playback of 
sensors can be employed under the ChucK programming model.  
Using this framework, composers, programmers, and performers 
can quickly write (and read/debug) complex controller/synthesis 
programs, and experiment with controller mapping on-the-fly. 

</abstract>
    <keywords>  Controller mapping, programming language, on-the-fly  programming, real-time interaction, concurrency.   </keywords>
  </document>
  <document>
    <name>nime2005_200.pdf</name>
    <abstract>
Drum controllers designed by researchers and commercial
companies use a variety of techniques for capturing percus-
sive gestures. It is challenging to obtain both quick response
times and low-level data (such as position) that contain ex-
pressive information. This research is a comprehensive study
of current methods to evaluate the available strategies and
technologies. This study aims to demonstrate the benefits
and detriments of the current state of percussion controllers
as well as yield tools for those who would wish to conduct
this type of study in the future.

</abstract>
    <keywords> Percussion Controllers, Timbre-recognition based instruments, Electronic Percussion, Sensors for Interface Design  </keywords>
  </document>
  <document>
    <name>nime2005_204.pdf</name>
    <abstract>
Discussion of time in interactive computer music systems engineer-
ing has been largely limited to data acquisition rates and latency.
Since music is an inherently time-based medium, we believe that
time plays a more important role in both the usability and imple-
mentation of these systems. In this paper, we present a time design
space, which we use to expose some of the challenges of devel-
oping computer music systems with time-based interaction. We
describe and analyze the time-related issues we encountered whilst
designing and building a series of interactive music exhibits that
fall into this design space. These issues often occur because of
the varying and sometimes conflicting conceptual models of time
in the three domains of user, application (music), and engineering.
We present some of our latest work in conducting gesture interpre-
tation and frameworks for digital audio, which attempt to analyze
and address these conflicts in temporal conceptual models.

</abstract>
  </document>
  <document>
    <name>nime2005_208.pdf</name>
    <keywords> Reconfigurable, Sensors, Computer Music  </keywords>
  </document>
  <document>
    <name>nime2005_212.pdf</name>
    <abstract> 
This paper describes the audio human computer interface 
experiments of ixi in the past and outlines the current platform for 
future research. ixi software [5] was founded by Thor Magnusson 
and Enrike Hurtado Mendieta in year 2000 and since then we've 
been working on building prototypes in the form of screen-based 
graphical user interfaces for musical performance, researching 
human computer interaction in the field of music and creating 
environments which other people can use to do similar work and 
for us to use in our workshops. Our initial starting point was that 
computer music software and the way their interfaces are built 
need not necessarily be limited to copying the acoustic musical 
instruments and studio technology that we already have, but 
additionally we can create unique languages and work processes 
for the virtual world. The computer is a vast creative space with 
specific qualities that can and should be explored. 

</abstract>
    <keywords>  Graphical user interfaces, abstract graphical interfaces, hyper- control, intelligent instruments, live performance, machine  learning, catalyst software, OSC, interfacing code, open source,  Pure Data, SuperCollider.   </keywords>
  </document>
  <document>
    <name>nime2005_216.pdf</name>
    <abstract> 
 

Musicians and composers have been using brainwaves as 
generative sources in music for at least 40 years and the 
possibility of a brain-computer interface for direct communication 
and control was first seriously investigated in the early 1970s.  
Work has been done by many artists and technologists in the 
intervening years to attempt to control music systems with 
brainwaves and - indeed - many other biological signals. Despite 
the richness of EEG, fMRI and other data which can be read from 
the human brain, there has up to now been only limited success in 
translating the complex encephalographic data into satisfactory 
musical results. We are currently pursuing research which we 
believe will lead to the possibility of direct brain-computer 
interfaces for rich and expressive musical control. This report will 
outline the directions of our current research and results.   

</abstract>
  </document>
  <document>
    <name>nime2005_220.pdf</name>
    <abstract>
We present a real-time system which allows musicians to
interact with synthetic virtual characters as they perform.
Using Max/MSP to parameterize keyboard and vocal in-
put, meaningful features (pitch, amplitude, chord informa-
tion, and vocal timbre) are extracted from live performance
in real-time. These extracted musical features are then
mapped to character behaviour in such a way that the mu-
sician's performance elicits a response from the virtual char-
acter. The system uses the ANIMUS framework to generate
believable character expressions. Experimental results are
presented for simple characters.

</abstract>
    <keywords> Music, synthetic characters, advanced man-machine inter- faces, virtual reality, behavioural systems, interaction tech- niques, visualization, immersive entertainment, artistic in- stallations  </keywords>
  </document>
  <document>
    <name>nime2005_224.pdf</name>
    <abstract>
In the Expression Synthesis Project (ESP), we propose a
driving interface for expression synthesis. ESP aims to
provide a compelling metaphor for expressive performance so
as to make high-level expressive decisions accessible to non-
experts. In ESP, the user drives a car on a virtual road that
represents the music with its twists and turns; and makes
decisions on how to traverse each part of the road. The driver's
decisions affect in real-time the rendering of the piece. The
pedals and wheel provide a tactile interface for controlling the
car dynamics and musical expression, while the display
portrays a first person view of the road and dashboard from the
driver's seat. This game-like interface allows non-experts to
create expressive renderings of existing music without having
to master an instrument, and allows expert musicians to
experiment with expressive choice without having to first
master the notes of the piece. The prototype system has been
tested and refined in numerous demonstrations. This paper
presents the concepts underlying the ESP system and the
architectural design and implementation of a prototype.

</abstract>
    <keywords> Music expression synthesis system, driving interface.  </keywords>
  </document>
  <document>
    <name>nime2005_228.pdf</name>
    <abstract>
While many new interfaces for musical expression have been pre-
sented in the past, methods to evaluate these interfaces are rare.
This paper presents a method and a study comparing the potential
for musical expression of different string-instrument based musical
interfaces. Cues for musical expression are defined based on re-
sults of research in musical expression and on methods for musical
education in instrumental pedagogy. Interfaces are evaluated ac-
cording to how well they are estimated to allow players making use
of their existing technique for the creation of expressive music.

</abstract>
    <keywords> Musical Expression, electronic bowed string instrument, evaluation of musical input devices, audio signal driven sound synthesis  </keywords>
  </document>
  <document>
    <name>nime2005_232.pdf</name>
  </document>
  <document>
    <name>nime2005_236.pdf</name>
    <abstract>
A wide variety of singing synthesis models and methods exist,
but there are remarkably few real-time controllers for these
models.  This paper describes a variety of devices developed
over the last few years for controlling singing synthesis
models implemented in the Synthesis Toolkit in C++ (STK),
Max/MSP, and ChucK.  All of the controllers share some
common features, such as air-pressure sensing for breathing
and/or loudness control, means to control pitch, and methods
for selecting and blending phonemes, diphones, and words.
However, the form factors, sensors, mappings, and algorithms
vary greatly between the different controllers.

</abstract>
    <keywords> Singing synthesis, real-time singing synthesis control.  </keywords>
  </document>
  <document>
    <name>nime2005_238.pdf</name>
    <abstract>
The author describes a recent composition for piano and
computer in which the score performed by the pianist, read
from a computer monitor, is generated in real-time from a
vocabulary of predetermined scanned score excerpts. The
author outlines the algorithm used to choose and display a
particular excerpt and describes some of the musical
difficulties faced by the pianist in a performance of the work.  

</abstract>
    <keywords> Score generation, Jitter.  </keywords>
  </document>
  <document>
    <name>nime2005_240.pdf</name>
    <abstract>
No Clergy is an interactive music performance/installation in
which the audience is able to shape the ongoing music. In it,
members of a small acoustic ensemble read music notation from
computer screens. As each page refreshes, the notation is altered
and shaped by both stochastic transformations of earlier music
with the same performance and audience feedback, collected via
standard CGI forms. 

</abstract>
    <keywords> notation, stochastic, interactive, audience, Python, Lilypond  </keywords>
  </document>
  <document>
    <name>nime2005_242.pdf</name>
    <abstract>
This paper describes the design of SoniMime, a system for
the sonification of hand movement for real-time timbre shap-
ing. We explore the application of the tristimulus timbre
model for the sonification of gestural data, working toward
the goals of musical expressivity and physical responsive-
ness. SoniMime uses two 3-D accelerometers connected to
an Atmel microprocessor which outputs OSC control mes-
sages. Data filtering, parameter mapping, and sound syn-
thesis take place in Pd running on a Linux computer.

</abstract>
    <keywords> Sonification, Musical Controller, Human Computer Interac- tion  </keywords>
  </document>
  <document>
    <name>nime2005_244.pdf</name>
    <keywords> Musical controller, sensate surface, mapping system  </keywords>
  </document>
  <document>
    <name>nime2005_246.pdf</name>
    <abstract>
This paper describes the design and implementation of Beat
Boxing, a percussive gestural interface for the live
performance of electronic music and control of computer-
based games and musical activities.

</abstract>
    <keywords> Performance, Gestural Mapping, Music Controller, Human- Computer Interaction, PureData (Pd), OSC  </keywords>
  </document>
  <document>
    <name>nime2005_248.pdf</name>
    <abstract> 
This paper describes the development of AirStick, an interface for 
musical expression. AirStick is played "in the air", in a Theremin 
style. It is composed of an array of infrared proximity sensors, 
which allow the mapping of the position of any interfering 
obstacle inside a bi-dimensional zone. This controller sends both x 
and y control data to various real-time synthesis algorithms. 

</abstract>
    <keywords>  Music Controller, Infrared Sensing, Computer Music.   </keywords>
  </document>
  <document>
    <name>nime2005_250.pdf</name>
    <keywords> Musical Controller, Collaborative Control, Haptic Interfaces  </keywords>
  </document>
  <document>
    <name>nime2005_252.pdf</name>
    <abstract> 
We present a Virtual Interface to Feel Emotions called VIFE 
_alpha v.01 (Virtual Interface to Feel Emotions). The work 
investigates the idea of Synaesthesia and her enormous 
possibilities creating new realities, sensations and zones where the 
user can find new points of interaction. This interface allows the 
user to create sonorous and visual compositions in real time. 6 
three-dimensional sonorous forms are modified according to the 
movements of the user. These forms represent sonorous objects 
that respond to this by means of sensorial stimuli. Multiple 
combinations of colors and sound effects superpose to an a the 
others to give rise to a unique experience. 

</abstract>
    <keywords>  Synaesthesia, 3D render, new reality, virtual interface, creative  interaction, sensors.   </keywords>
  </document>
  <document>
    <name>nime2005_254.pdf</name>
    <abstract>
The Sonictroller was originally conceived as a means of
introducing competition into an improvisatory musical
performance. By reverse-engineering a popular video game
console, we were able to map sound information (volume,
pitch, and pitch sequences) to any continuous or momentary
action of a video game sprite.

</abstract>
    <keywords> video game, Nintendo, music, sound, controller, Mortal Kombat, trumpet, guitar, voice  </keywords>
  </document>
  <document>
    <name>nime2005_258.pdf</name>
    <abstract> 
In this presentation, we discuss and demonstrate a multiple 
touch sensitive (MTS) keyboard developed by Robert Moog for 
John Eaton. Each key of the keyboard is equipped with sensors 
that detect the three-dimensional position of the performer's 
finger. The presentation includes some of Eaton's performances 
for certain earlier prototypes as well as this keyboard. 

</abstract>
    <keywords>  Multiple touch sensitive, MTS, keyboard, key sensor design,  upgrading to present-day computers   </keywords>
  </document>
  <document>
    <name>nime2005_260.pdf</name>
    <abstract> 
This paper will demonstrate the use of the Smart Controller 
workbench in the Interactive Bell Garden. 

</abstract>
    <keywords>  Control Voltage, Open Sound Control, Algorithmic Composition,  MIDI, Sound Installations, Programmable Logic Control,  Synthesizers.   </keywords>
  </document>
  <document>
    <name>nime2005_262.pdf</name>
    <abstract>
The Swayway is an audio/MIDI device inspired by the simple
concept of the wind chime.

This interactive sculpture translates its swaying motion,
triggered by the user, into sound and light. Additionally, the
motion of the reeds contributes to the visual aspect of the
piece, converting the whole into a sensory and engaging
experience.

</abstract>
    <keywords> Interactive sound sculpture, flex sensors, midi chimes, LEDs, sound installation.  </keywords>
  </document>
  <document>
    <name>nime2005_264.pdf</name>
    <abstract> 
This paper describes the transformation of an everyday object into 
a digital musical instrument.   By tracking hand movements and 
tilt on one of two axes, the Bubbaboard, a transformed handheld 
washboard, allows a user to play scales at different octaves while 
simultaneously offering the ability to use its inherent acoustic 
percussive qualities.  Processed sound is fed to the 
Mommaspeaker, which creates physically generated vibrato at a 
speed determined by tilting the Bubbaboard on its second axis. 

</abstract>
    <keywords>  Gesture based controllers, Musical Performance, MIDI,  Accelerometer, Microcontroller, Contact Microphone   </keywords>
  </document>
  <document>
    <name>nime2005_266.pdf</name>
    <abstract> 
The Wise Box is a new wireless digitizing interface for sensors 
and controllers. An increasing demand for this kind of 
hardware, especially in the field of dance and computer 
performance lead us to design a wireless digitizer that allows for 
multiple users, with high bandwidth and accuracy. The interface 
design was initiated in early 2004 and shortly described in 
reference [1]. Our recent effort was directed to make this device 
available for the community on the form of a manufactured 
product, similarly to our previous interfaces such as AtoMIC 
Pro, Eobody or Ethersense [1][2][3]. We describe here the 
principles we used for the design of the device as well as its 
technical specifications. The demo will show several devices 
running at once and used in real-time with a various set of 
sensors. 
 
</abstract>
    <keywords>  Gesture, Sensors, WiFi, 802.11, OpenSoundControl.      </keywords>
  </document>
  <document>
    <name>nime2005_268.pdf</name>
    <abstract> 
Soundstone is a small wireless music controller that tracks 

movement and gestures, and maps these signals to characteristics 

of various synthesized and sampled sounds. It is intended to 

become a general-purpose platform for exploring the sonification 
of movement, with an emphasis on tactile (haptic) feedback. 

</abstract>
    <keywords>  Gesture recognition, haptics, human factors, force, acceleration,   tactile feedback, general purpose controller, wireless.   </keywords>
  </document>
  <document>
    <name>nime2005_271.pdf</name>
    <abstract>
Contemplace is a spatial personality that redesigns itself
dynamically according to its conversations with its visitors.
Sometimes welcoming, sometimes shy, and sometimes
hostile, Contemplace's mood is apparent through a display of
projected graphics, spatial sound, and physical motion.
Contemplace is an environment in which inhabitation
becomes a two-way dialogue.

</abstract>
    <keywords> Interactive space, spatial installation, graphic and aural display, motion tracking, Processing, Flosc  </keywords>
  </document>
  <document>
    <name>nime2005_272.pdf</name>
    <abstract>
Mocean is an immersive environment that creates sensory
relationships between natural media, particularly exploring
the potential of water as an emotive interface.

</abstract>
    <keywords> New interface, water, pipe organ, natural media, PIC microcontroller, wind instrument, human computer interface.  </keywords>
  </document>
  <document>
    <name>nime2006_026.pdf</name>
    <abstract>
This paper presents the concepts and techniques used in a
family of location based multimedia works. The paper has
three main sections: 1.) to describe the architecture of an
audio-visual hardware/software framework we have
developed for the realization of a series of locative media
artworks, 2.) to discuss the theoretical and conceptual
underpinnings motivating the design of the technical
framework, and 3.) to elicit from this, fundamental issues
and questions that can be generalized and applicable to the
growing practice of locative media.

</abstract>
    <keywords> Mobile music, urban fiction, locative media.  </keywords>
  </document>
  <document>
    <name>nime2006_037.pdf</name>
    <abstract> 
This paper describes two new live performance scenarios for 
performing music using bluetooth-enabled mobile phones. 
Interaction between mobile phones via wireless link is a key 
feature of the performance interface for each scenario. Both 
scenarios are discussed in the context of two publicly performed 
works for an ensemble of players in which mobile phone 
handsets are used both as sound sources and as hand-held 
controllers. In both works mobile phones are mounted in a 
specially devised pouch attached to a cord and physically 
swung to produce audio chorusing. During performance some 
players swing phones while others operate phones as hand-held 
controllers. Wireless connectivity enables interaction between 
flying and hand-held phones. Each work features different 
bluetooth implementations. In one a dedicated mobile phone 
acts as a server that interconnects multiple clients, while in the 
other point to point communication takes place between clients 
on an ad hoc basis. The paper summarises bluetooth tools 
designed for live performance realisation and concludes with a 
comparative evaluation of both scenarios for future 
implementation of performance by large ensembles of non-
expert players performing microtonal music using ubiquitous 
technology. 

</abstract>
    <keywords>  Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media  Performance; Just Intonation.   </keywords>
  </document>
  <document>
    <name>nime2006_043.pdf</name>
    <abstract> 
Physically situated public art poses significant challenges for 

the design and realization of interactive, electronic sound 

works.  Consideration of diverse audiences, environmental 

sensitivity, exhibition conditions, and logistics must guide the 

artwork.  We describe our work in this area, using a recently 

installed public piece, Transition Soundings, as a case study that 

reveals a specialized interface and open-ended approach to 

interactive music making.  This case study serves as a vehicle 

for examination of the real world challenges posed by public art 

and its outcomes. 

</abstract>
    <keywords>  Music, Sound, Interactivity, Arts, Public Art, Network Systems,   Sculpture, Installation Art, Embedded Electronics.   </keywords>
  </document>
  <document>
    <name>nime2006_049.pdf</name>
    <keywords>  Graphical interfaces, collaborative performance, networking,   computer music ensemble, emergence, visualization,   education.   </keywords>
  </document>
  <document>
    <name>nime2006_053.pdf</name>
    <abstract> 
The culture of laptop improvisation has grown tremendously in 

recent years. The development of personalized software 

instruments presents interesting issues in the context of 

improvised group performances. This paper examines an 

approach that is aimed at increasing the modes of interactivity 

between laptop performers and at the same time suggests ways 

in which audiences can better discern and identify the sonic 

characteristics of each laptop performer. We refer to software 

implementation that was developed for the BLISS networked 

laptop ensemble with view to designing a shared format for the 

exchange of messages within local and internet based networks. 

</abstract>
    <keywords>  Networked audio technologies, laptop ensemble, centralized   audio server, improvisation   </keywords>
  </document>
  <document>
    <name>nime2006_061.pdf</name>
    <keywords> touch screen, PDA, Pure Data, controller, mobile musical instrument, human computer interaction  </keywords>
  </document>
  <document>
    <name>nime2006_065.pdf</name>
    <abstract> 
This paper discusses the concept of using background music to 

control video game parameters and thus actions on the screen. 

Each song selected by the player makes the game look different 

and behave variedly. The concept is explored by modifying an 

existing video game and playtesting it with different kinds of 

MIDI music. Several examples of mapping MIDI parameters to 

game events are presented. As mobile phones' MIDI players do 

not usually have a dedicated callback API, a real-time MIDI 

analysis software for Symbian OS was implemented. Future 

developments including real-time group performance as a way 

to control game content are also considered. 

</abstract>
    <keywords>  Games, MIDI, music, rhythm games, background music   reactive games, musically controlled games, MIDI-controlled   games, Virtual Sequencer.   </keywords>
  </document>
  <document>
    <name>nime2006_071.pdf</name>
    <abstract> 

Turntable musicians have yet to explore new expressions with 

digital technology. New higher-level development tools open 

possibilities for these artists to build their own instruments that 

can achieve artistic goals commercial products cannot. This 

paper will present a rough overview on the practice and recent 

development of turntable music, followed by descriptions of 

two projects by the author. 

</abstract>
    <keywords>  Turntable music, DJ, turntablist, improvisation, Max/MSP, PIC   Microcontroller, Physical Computing   </keywords>
  </document>
  <document>
    <name>nime2006_075.pdf</name>
    <abstract>
This report presents an interface for musical performance called 
the spinCycle.  spinCycle enables performers to make visual 
patterns with brightly colored objects on a spinning turntable 
platter that get translated into  musical arrangements in real-
time. I will briefly describe the hardware implementation and 
the sound generation logic used, as well as provide a historical 
background for the project.

</abstract>
    <keywords> Color-tracking, turntable, visualization, interactivity,  synesthesia  </keywords>
  </document>
  <document>
    <name>nime2006_081.pdf</name>
    <abstract> 
The PETECUBE project consists of a series of musical 

interfaces designed to explore multi-modal feedback. This 

paper will briefly describe the definition of multimodal 

feedback, the aim of the project, the development of the first 

PETECUBE and proposed further work.   

</abstract>
    <keywords>  Multi-modal Feedback. Haptics. Musical Instrument.   </keywords>
  </document>
  <document>
    <name>nime2006_085.pdf</name>
    <keywords> Digital musical instrument, kinesthetic feedback  </keywords>
  </document>
  <document>
    <name>nime2006_089.pdf</name>
    <abstract>
The Orbophone is a new interface that radiates rather than
projects sound and image. It provides a cohesive platform
for audio and visual presentation in situations where both
media are transmitted from the same location and
localization in both media is perceptually correlated. This
paper discusses the advantages of radiation over
conventional sound and image projection for certain kinds
of interactive public multimedia exhibits and describes the
artistic motivation for its development against a historical
backdrop of sound systems used in public spaces. One
exhibit using the Orbophone is described in detail together
with description and critique of the prototype, discussing
aspects of its design and construction. The paper concludes
with an outline of the Orbophone version 2.

</abstract>
    <keywords> Immersive Sound; Multi-channel Sound; Loud-speaker Array; Multimedia; Streaming Media; Real-Time Media Performance; Sound Installation.  </keywords>
  </document>
  <document>
    <name>nime2006_093.pdf</name>
    <abstract>
The gluion is a sensor interface that was designed to overcome

some of the limitations of more traditional designs based on

microcontrollers, which only provide a small, fixed number of

digital modules such as counters and serial interfaces. These are

often required to handle sensors where the physical parameter

cannot easily be converted into a voltage. Other sensors are

packed into modules that include converters and communicate

via SPI or I2C. Finallly, many designs require output

capabilities beyond simple on/off.

The gluion approaches these challenges thru its FPGA-based

design which allows for a large number of digital I/O modules.

It also provides superior flexibility regarding their

configuration, resolution, and functionality. In addition, the

FPGA enables a software implementation of the host link - in

the case of the gluion the OSC protocol as well as the

underlying Ethernet layers.

</abstract>
  </document>
  <document>
    <name>nime2006_097.pdf</name>
    <abstract>
A new sensor integration system and its first incarnation i s
described. As well as supporting existing analog sensor
arrays a new architecture allows for easy integration of the
new generation of low-cost digital sensors used in computer
music performance instruments and installation art.

</abstract>
    <keywords> Gesture, sensor, MEMS, FPGA, network, OSC, configurability  </keywords>
  </document>
  <document>
    <name>nime2006_101.pdf</name>
    <abstract>
How can we provide interfaces to synthesis algorithms that
will allow us to manipulate timbre directly, using the same
timbre-words that are used by human musicians to com-
municate about timbre? This paper describes ongoing
work that uses machine learning methods (principally ge-
netic algorithms and neural networks) to learn (1) to recog-
nise timbral characteristics of sound and (2) to adjust tim-
bral characteristics of existing synthesized sounds.

</abstract>
    <keywords> timbre; natural language; neural networks  </keywords>
  </document>
  <document>
    <name>nime2006_103.pdf</name>
    <keywords> composition, process, materials, gesture, controller, cross- modal interaction   </keywords>
  </document>
  <document>
    <name>nime2006_114.pdf</name>
    <abstract>
This paper reports on ongoing studies of the design and use of
support for remote group music making. In this paper we
outline the initial findings of a recent study focusing on the
function of decay  of contributions in collaborative music
making. Findings indicate that persistent contributions lend
themselves to individual musical composition and learning
novel interfaces, whilst contributions that quickly decay
engender a more focused musical interaction in experienced
participants.

</abstract>
  </document>
  <document>
    <name>nime2006_118.pdf</name>
    <keywords> Collaborative interface, remote jamming, network music, interaction design, novice, media space  INTRODUCTION Most would agree that music is an inherently social ac-  tivity [30], but since the </keywords>
  </document>
  <document>
    <name>nime2006_124.pdf</name>
    <abstract> 
In this paper, we describe the networking of multiple Integral 

Music Controllers (IMCs) to enable an entirely new method for 

creating music by tapping into the composite gestures and 

emotions of not just one, but many performers. The concept 

and operation of an IMC is reviewed as well as its use in a 

network of IMC controllers. We then introduce a new technique 

of Integral Music Control by assessing the composite gesture(s) 

and emotion(s) of a group of performers through the use of a 

wireless mesh network. The Telemuse, an IMC designed 

precisely for this kind of performance, is described and its use 

in a new musical performance project under development by 

the authors is discussed.   

</abstract>
  </document>
  <document>
    <name>nime2006_129.pdf</name>
    <abstract> 
This paper explores the use of perturbation in designing multi-

performer or multi-agent interactive musical interfaces. A 

problem with the multi-performer approach is how to 

cohesively organize the independent data inputs into useable 

control information for synthesis engines. Perturbation has 

proven useful for navigating multi-agent NIMEs. The author's 

Windtree is discussed as an example multi-performer 

instrument in which perturbation is used for multichannel 

ecological modeling. The Windtree uses a physical system 

turbulence model controlled in real time by four performers.  

</abstract>
  </document>
  <document>
    <name>nime2006_134.pdf</name>
    <abstract>
We describe the design of a system of compact, wireless
sensor modules meant to capture expressive motion when
worn at the wrists and ankles of a dancer. The sensors form a
high-speed RF network geared toward real-time data
acquisition from multiple devices simultaneously, enabling a
small dance ensemble to become a collective interface for
music control. Each sensor node includes a 6-axis inertial
measurement unit (IMU) comprised of three orthogonal
gyroscopes and accelerometers in order to capture local
dynamics, as well as a capacitive sensor to measure close
range node-to-node proximity. The nodes may also be
augmented with other digital or analog sensors. This paper
describes application goals, presents the prototype hardware
design, introduces concepts for feature extraction and
interpretation, and discusses early test results.

</abstract>
    <keywords> Interactive dance, wearable sensor networks, inertial gesture tracking, collective motion analysis, multi-user interface  </keywords>
  </document>
  <document>
    <name>nime2006_140.pdf</name>
    <keywords> Sound Spatialization, Ambisonics, Vector Based Additive Panning (VBAP), Wave Field Synthesis, Acousmatic Music  </keywords>
  </document>
  <document>
    <name>nime2006_144.pdf</name>
    <abstract>
Traditional uses of virtual audio environments tend to focus on
perceptually accurate acoustic representations. Though spatial-
ization of sound sources is important, it is necessary to leverage
control of the sonic representation when considering musical ap-
plications. The proposed framework allows for the creation of
perceptually immersive scenes that function as musical instru-
ments. Loudspeakers and microphones are modeled within the
scene along with the listener/performer, creating a navigable 3D
sonic space where sound sources and sinks process audio accord-
ing to user-defined spatial mappings.

</abstract>
    <keywords> Control paradigms, 3D audio, spatialization, immersive audio environments, auditory display, acoustic modeling, spatial inter- faces, virtual instrument design  </keywords>
  </document>
  <document>
    <name>nime2006_150.pdf</name>
    <keywords> Software Architecture, Interactive Systems, Music soft- ware  </keywords>
  </document>
  <document>
    <name>nime2006_156.pdf</name>
  </document>
  <document>
    <name>nime2006_162.pdf</name>
    <abstract> 
The ixi software project started in 2000 with the intention to 

explore new interactive patterns and virtual interfaces in 

computer music software. The aim of this paper is not to 

describe these programs, as they have been described elsewhere 

[14][15], but rather explicate the theoretical background that 

underlies the design of these screen-based instruments. After an 

analysis of the similarities and differences in the design of 

acoustic and screen-based instruments, the paper describes how 

the creation of an interface is essentially the creation of a 

semiotic system that affects and influences the musician and the 

composer. Finally the terminology of this semiotics is explained 

as an interaction model. 

</abstract>
    <keywords>  Interfaces, interaction design, HCI, semiotics, actors, OSC,   mapping, interaction models, creative tools.   </keywords>
  </document>
  <document>
    <name>nime2006_168.pdf</name>
    <keywords> Software control of computer music, laptop performance, graphical interfaces, freehand input, dynamic simulation  </keywords>
  </document>
  <document>
    <name>nime2006_172.pdf</name>
    <abstract> 
Development of a musical interface which allows people to play 

music intuitively and create music visibly. 

</abstract>
  </document>
  <document>
    <name>nime2006_176.pdf</name>
    <keywords> Gesture description, gesture analysis, standards  </keywords>
  </document>
  <document>
    <name>nime2006_180.pdf</name>
    <keywords> sensors, Wiki, collaborative website, open content  </keywords>
  </document>
  <document>
    <name>nime2006_192.pdf</name>
    <abstract> 
In this paper we describe a new guitar-like musical controller. The 

'GXtar' is an instrument which takes as a starting point a guitar but 

his role is to bring different and new musical possibilities while 

preserving the spirit and techniques of guitar. Therefore, it was 

conceived and carried out starting from the body of an electric 

guitar. The fingerboard of this guitar was equipped with two lines 

of sensors: linear position sensors, and tactile pressure sensors. 

These two lines of sensors are used as two virtual strings. Their 

two ends are the bridge and the nut of the guitar. The design of the 

instrument is made in a way that the position of a finger, on one of 

these virtual strings, corresponds to the note, which would have 

been played on a real and vibrating string. On the soundboard of 

the guitar, a controller, with 3 degrees of freedom, allows to drive 

other synthesis parameters. We then describe how this interface is 

integrated in a musical audio system and serves as a musical 

instrument. 

</abstract>
    <keywords>  Guitar, alternate controller, sensors, synthesizer, multidimensional   control.   </keywords>
  </document>
  <document>
    <name>nime2006_196.pdf</name>
  </document>
  <document>
    <name>nime2006_200.pdf</name>
    <abstract> 
A cost-effective method was developed for the estimation of the 

bow velocity in violin playing, using an accelerometer on the 

bow in combination with point tracking using a standard video 

camera. The video data are used to detect the moments of bow 

direction changes. This information is used for piece-wise 

integration of the accelerometer signal, resulting in a drift-free 

reconstructed velocity signal with a high temporal resolution. 

The method was evaluated using a 3D motion capturing system, 

providing a reliable reference of the actual bow velocity. The 

method showed good results when the accelerometer and video 

stream are synchronized. Additional latency and jitter of the 

camera stream can importantly decrease the performance of the 

method, depending on the bow stroke type.  

</abstract>
    <keywords>  Bowing gestures, bowed string, violin, bow velocity,   accelerometer, video tracking.   </keywords>
  </document>
  <document>
    <name>nime2006_208.pdf</name>
    <abstract>
This paper introduces the scoreTable*, a tangible interactive 

music score editor which started as a simple application for 

demoing "traditional" approaches to music creation, using the 

reacTable* technology, and which has evolved into an 

independent research project on its own. After a brief 

discussion on the role of pitch in music, we present a brief 

overview of related tangible music editors, and discuss several 

paradigms in computer music creation, contrasting synchronous 

with asynchronous approaches. The final part of the paper 

describes the current state of the scoreTable* as well as its 

future lines of research.

</abstract>
    <keywords>  Musical instrument, Collaborative Music, Computer Supported   Collaborative Work, Tangible User Interface, Music Theory.  </keywords>
  </document>
  <document>
    <name>nime2006_216.pdf</name>
    <abstract>
In this paper, we describe our experience in musical interface 

design for a large scale, high-resolution, multi-touch display 

surface. We provide an overview of historical and present-

day context in multi-touch audio interaction, and describe our 

approach to analysis of tracked multi-finger, multi-hand data for 

controlling live audio synthesis.

</abstract>
    <keywords> multi-touch, touch, tactile, bi-manual, multi-user, synthesis,   dynamic patching  </keywords>
  </document>
  <document>
    <name>nime2006_220.pdf</name>
    <keywords> Musical instrument design, mapping, gestures, organology.  </keywords>
  </document>
  <document>
    <name>nime2006_226.pdf</name>
  </document>
  <document>
    <name>nime2006_230.pdf</name>
    <abstract> 
This paper presents the development of novel "home-made" 

touch sensors using conductive pigments and various substrate 

materials. We show that it is possible to build one's own 

position, pressure and bend sensors with various electrical 

characteristics, sizes and shapes, and this for a very competitive 

price. We give examples and provide results from experimental 

tests of such developments. 

</abstract>
    <keywords>   Touch sensors, piezoresistive technology, conductive pigments,   sensitive materials, interface design   </keywords>
  </document>
  <document>
    <name>nime2006_234.pdf</name>
    <keywords> Ad hoc instruments, Pin&amp;Play, physical interfaces, music performance, new interfaces for musical expression.  </keywords>
  </document>
  <document>
    <name>nime2006_240.pdf</name>
    <abstract>
In this paper we introduce the Croaker, a novel input device
inspired by Russolo's Intonarumori. We describe the compo-
nents of the controller and the sound synthesis engine which
allows to reproduce several everyday sounds.

</abstract>
    <keywords> Noise machines, everyday sounds, physical models.  </keywords>
  </document>
  <document>
    <name>nime2006_250.pdf</name>
  </document>
  <document>
    <name>nime2006_254.pdf</name>
    <abstract> 
The MICON is an electronic music stand extending Maestro!, 

the latest in a series of interactive conducting exhibits that use 

real orchestral audio and video recordings. The MICON uses 

OpenGL-based rendering to display and animate score pages 

with a high degree of realism. It offers three different score 

display formats to match the user's level of expertise. A real-

time animated visual cueing system helps users with their 

conducting. The MICON has been evaluated with music 

students. 

</abstract>
    <keywords>  Music stand, score display, exhibit, conducting.   </keywords>
  </document>
  <document>
    <name>nime2006_260.pdf</name>
    <abstract>
Designing a conducting gesture analysis system for public spaces
poses unique challenges. We present conga, a software frame-
work that enables automatic recognition and interpretation of
conducting gestures. conga is able to recognize multiple types of
gestures with varying levels of difficulty for the user to perform,
from a standard four-beat pattern, to simplified up-down conduct-
ing movements, to no pattern at all. conga provides an extendable
library of feature detectors linked together into a directed acyclic
graph; these graphs represent the various conducting patterns as
gesture profiles. At run-time, conga searches for the best profile
to match a user's gestures in real-time, and uses a beat predic-
tion algorithm to provide results at the sub-beat level, in addition
to output values such as tempo, gesture size, and the gesture's
geometric center. Unlike some previous approaches, conga does
not need to be trained with sample data before use. Our prelim-
inary user tests show that conga has a beat recognition rate of
over 90%. conga is deployed as the gesture recognition system
for Maestro!, an interactive conducting exhibit that opened in the
Betty Brinn Children's Museum in Milwaukee, USA in March
2006.

</abstract>
    <keywords> gesture recognition, conducting, software gesture frameworks  </keywords>
  </document>
  <document>
    <name>nime2006_266.pdf</name>
    <keywords> Singing synthesis, voice source, voice quality, spectral  model, formant synthesis, instrument, gestural control.  </keywords>
  </document>
  <document>
    <name>nime2006_272.pdf</name>
    <abstract> 
We describe the implementation of an environment for 

Gesturally-Realized Audio, Speech and Song Performance 

(GRASSP), which includes a glove-based interface, a 

mapping/training interface, and a collection of Max/MSP/Jitter 

bpatchers that allow the user to improvise speech, song, sound 

synthesis, sound processing, sound localization, and video 

processing.  The mapping/training interface provides a 

framework for performers to specify by example the mapping 

between gesture and sound or video controls.  We demonstrate 

the effectiveness of the GRASSP environment for gestural 

control of musical expression by creating a gesture-to-voice 

system that is currently being used by performers.  

</abstract>
    <keywords>  Speech synthesis, parallel formant speech synthesizer, gesture   control, Max/MSP, Jitter, Cyberglove, Polhemus, sound   diffusion, UBC Toolbox, Glove-Talk,    </keywords>
  </document>
  <document>
    <name>nime2006_277.pdf</name>
    <abstract>
Is there a distinction between New Interfaces for Musical
Expression and New Interfaces for Controlling Sound? This
article begins with a brief overview of expression in musical
performance, and examines some of the characteristics of
effective "expressive" computer music instruments. It
becomes apparent that sophisticated musical expression
requires not only a good control interface but also virtuosic
mastery of the instrument it controls. By studying effective
acoustic instruments, choosing intuitive but complex
gesture-sound mappings that take advantage of established
instrumental skills, designing intelligent characterizations
of performance gestures, and promoting long-term dedicated
practice on a new interface, computer music instrument
designers can enhance the expressive quality of computer
music performance.

</abstract>
    <keywords> Expression, instrument design, performance, virtuosity.  </keywords>
  </document>
  <document>
    <name>nime2006_283.pdf</name>
    <abstract>
Why is a seemingly mundane issue such as airline baggage
allowance of great significance in regards to the performance
practice of electronic music? This paper discusses how a
performance practice has evolved that seeks to question the
binary and corporate digital world. New 'instruments' and
approaches have emerged that explore 'dirty electronics' and
'punktronics': DIY electronic instruments made from junk.
These instruments are not instruments in the traditional
sense, defined by physical dimensions or by a set number of
parameters, but modular systems, constantly evolving, never
complete, infinitely variable and designed to be portable. A
combination of lo- and hi-fi, analogue and digital,
synchronous and asynchronous devices offer new modes of
expression. The development of these new interfaces for
musical expression run side-by-side with an emerging post-
digital aesthetic.

</abstract>
  </document>
  <document>
    <name>nime2006_292.pdf</name>
    <abstract>
This paper is intended to introduce the system, which
combines "BodySuit" and "RoboticMusic," as well as its
possibilities and its uses in an artistic application.
"BodySuit" refers to a gesture controller in a Data Suit type.
"RoboticMusic" refers to percussion robots, which are applied
to a humanoid robot type. In this paper, I will discuss their
aesthetics and the concept, as well as the idea of the "Extended
Body".

</abstract>
    <keywords> Robot, Gesture Controller, Humanoid Robot, Artificial Intelligence, Interaction  </keywords>
  </document>
  <document>
    <name>nime2006_296.pdf</name>
  </document>
  <document>
    <name>nime2006_300.pdf</name>
    <keywords>  Robotics, computer control, MIDI, player pianos, mechanical  music, percussion, sound effects, Dadaism.  </keywords>
  </document>
  <document>
    <name>nime2006_304.pdf</name>
    <abstract>
This paper deals with the first musical usage of an
experimental system dedicated to the optical detection of
the position of a trombone's slide.

</abstract>
  </document>
  <document>
    <name>nime2006_308.pdf</name>
    <keywords> saxophone, augmented instrument, live electronics, perfor- mance, gestural control  </keywords>
  </document>
  <document>
    <name>nime2006_314.pdf</name>
    <keywords> khaen, sound synthesis control, mapping, musical acoustics  </keywords>
  </document>
  <document>
    <name>nime2006_318.pdf</name>
    <abstract>
In this paper we will report on the use of real-time sound
spatialization in Challenging Bodies, a trans-disciplinary
performance project at the University of Regina. Using
well-understood spatialization techniques mapped to a cus-
tom interface, a computer system was built that allowed
live spatial control of ten sound signals from on-stage per-
formers. This spatial control added a unique dynamic ele-
ment to an already ultramodern performance. The system
is described in detail, including the main advantages over
existing spatialization systems: simplicity, usability, cus-
tomization and scalability

</abstract>
  </document>
  <document>
    <name>nime2006_322.pdf</name>
    <keywords> mapping, planning, agent, Max/MSP  </keywords>
  </document>
  <document>
    <name>nime2006_326.pdf</name>
    <abstract>
In this article, we present the first step of our research work to
design a Virtual Assistant for Performers and Stage Directors,
able to give a feedback from performances. We use a
methodology to automatically construct fuzzy rules in a Fuzzy
Rule-Based System that detects contextual emotions from an
actor's performance during a show.

We collect video data from a lot of performances of the same
show from which it should be possible to visualize all the
emotions and intents or more precisely "intent graphs". To
perform this, the collected data defining low-level descriptors
are aggregated and converted into high-level characterizations.
Then, depending on the retrieved data and on their distribution
on the axis, we partition the universes into classes. The last step
is the building of the fuzzy rules that are obtained from the
classes and that permit to give conclusions to label the detected
emotions.

</abstract>
    <keywords> Virtual Assistant, Intents, Emotion detector, Fuzzy Classes,  Stage Director, Performance.  </keywords>
  </document>
  <document>
    <name>nime2006_330.pdf</name>
    <abstract>
This is a studio report of researches and projects in SUAC
(Shizuoka University of Art and Culture). SUAC was founded
in April 2000, and organized NIME04 as you know. SUAC
has "Faculty of Design" and "Department of Art and Science"
and all students study interactive systems and media arts.
SUAC has organized Media Art Festival (MAF) from 2001 to
2005. Domestic/overseas artists participated in SUAC MAF,
and SUAC students' projects also joined and exhibited their
works in MAF. I will introduce the production cases with
interactive media-installations by SUAC students' projects
from the aspect "experiences with novel interfaces in
education and entertainment" and "reports on students projects
in the framework of NIME related courses".

</abstract>
    <keywords> Interactive Installation, Sensors, Media Arts, Studio Reports   </keywords>
  </document>
  <document>
    <name>nime2006_334.pdf</name>
    <abstract> 
In this paper we describe the intentions, the design and 

functionality of an Acousmatic Composition Environment that 

allows children or musical novices to educate their auditory 

curiosity by recording, manipulating and mixing sounds of 

everyday life. The environment consists of three stands: A 

stand for sound recording with a soundproof box that ensure 

good recording facilities in a noisy environment; a stand for 

sound manipulation with five simple, tangible interfaces; a 

stand for sound mixing with a graphical computer interface 

presented on two touch screens. 

</abstract>
    <keywords>  Acousmatic listening, aesthetics, tangible interfaces.   </keywords>
  </document>
  <document>
    <name>nime2006_338.pdf</name>
    <keywords> Bioinformatics, composition, real-time score generation.  </keywords>
  </document>
  <document>
    <name>nime2006_346.pdf</name>
  </document>
  <document>
    <name>nime2006_352.pdf</name>
    <abstract> 
Hyper-shaku (Border-Crossing) is an interactive sensor 

environment that uses motion sensors to trigger immediate 

responses and generative processes augmenting the Japanese 

bamboo shakuhachi in both the auditory and visual domain. The 

latter differentiates this process from many hyper-instruments by 

building a performance of visual design as well as electronic 

music on top of the acoustic performance. It utilizes a 

combination of computer vision and wireless sensing technologies 

conflated from preceding works. This paper outlines the use of 

gesture in these preparatory sound and audio-visual performative, 

installation and sonification works, leading to a description of the 

Hyper-shaku environment integrating sonification and generative 
elements. 

</abstract>
    <keywords>  Gesture-controllers, sonification, hyper-instrument   </keywords>
  </document>
  <document>
    <name>nime2006_358.pdf</name>
    <abstract>
Three electro-acoustic systems were devised for a new

trombone work, Rouse. This paper presents the technical

systems and outlines their musical context and motivation. The

uSlide measures trombone slide-extension by a minimal-

hardware ultrasonic technique. An easy calibration procedure

maps linear extension to the slide "positions" of the player. The

eMouth is a driver that replaces the mouthpiece, with software

emulation of trombone  tone and algorithmic musical lines,

allowing the trombone to appear to play itself. The eMute is

built around a loudspeaker unit, driven so that it affects strongly

the player's embouchure, allowing fine control of complex beat

patterns. eMouth and eMute, under control of the uSlide, set up

improvisatory worlds that are part of the composed architecture

of Rouse.

</abstract>
  </document>
  <document>
    <name>nime2006_376.pdf</name>
    <abstract> 
This paper describes recent enhancements in an interactive 

system designed to improvise with saxophonist John Butcher 

[1]. In addition to musical parameters such as pitch and 

loudness, our system is able to analyze timbral characteristics of 

the saxophone tone in real-time, and use timbral information to 

guide the generation of response material. We capture each 

saxophone gesture on the fly, extract a set of gestural and 

timbral contours, and store them in a repository.  Improvising 

agents can consult the repository when generating responses. 

The gestural or timbral progression of a saxophone phrase can 

be remapped or transformed; this enables a variety of response 

material that also references audible contours of the original 

saxophone gestures. A single simple framework is used to 

manage gestural and timbral information extracted from 

analysis, and for expressive control of virtual instruments in a 

free improvisation context.   

</abstract>
    <keywords>  Interactive music systems, timbre analysis, instrument control.   </keywords>
  </document>
  <document>
    <name>nime2006_384.pdf</name>
  </document>
  <document>
    <name>nime2006_390.pdf</name>
    <abstract> 
In this paper, some of the more recent developments in musical 

instruments related to the violin family are described, and 

analyzed according to several criteria adapted from other 

publications. While it is impossible to cover all such 
developments, we have tried to sample a variety of instruments 

from the last decade or so, with a greater focus on those 

published in the computer music literature. Experiences in the 

field of string players focusing on such developments are 
presented. Conclusions are drawn in which further research into 

violin-related digital instruments for string players may benefit 

from the presented criteria as well as the experiences. 

</abstract>
    <keywords>  Violin, viola, cello, bass, digital, electronic, synthesis,  controller.   </keywords>
  </document>
  <document>
    <name>nime2006_396.pdf</name>
    <abstract>

In this paper we present progress of an ongoing
collaboration between researchers at the MIT Media
Laboratory and the Royal Academy of Music (RAM). The aim
of this project is to further explore the expressive musical
potential of the Hyperbow, a custom music controller first
designed for use in violin performance. Through the creation
of new repertoire, we hope to stimulate the evolution of this
interface, advancing its usability and refining its
capabilities. In preparation for this work, the Hyperbow
system has been adapted for cello (acoustic and electric)
performance. The structure of our collaboration is described,
and two of the pieces currently in progress are presented.
Feedback from the performers is also discussed, as well as
future plans.

</abstract>
    <keywords> Cello, bow, controller, electroacoustic music, composition.  </keywords>
  </document>
  <document>
    <name>nime2006_407.pdf</name>
    <abstract>
This is a description of a demonstration, regarding the
use of auditory illusions and psycho-acoustic phenomenon
used in the interactive work of Jean-Claude Risset, written
for violinist Mari Kimura.

</abstract>
    <keywords> Violin, psycho-acoustic phenomena, auditory illusions, sig- nal processing, subharmonics, Risset, Kimura.  </keywords>
  </document>
  <document>
    <name>nime2006_409.pdf</name>
    <abstract>
Software and hardware enhancements to an electric 6-string
cello are described with a focus on a new mechanical tuning
device, a novel rotary sensor for bow interaction and control
strategies to leverage a suite of polyphonic sound
processing effects.

</abstract>
    <keywords> Cello, chordophone, FSR, Rotary Absolute Position Encoder, Double Bowing, triple stops, double stops, convolution.  </keywords>
  </document>
  <document>
    <name>nime2007_027.pdf</name>
    <abstract>
Physical modeling has proven to be a successful method of
synthesizing highly expressive sounds. However, providing
deep methods of real time musical control remains a major
challenge. In this paper we describe our work towards an
instrument for percussion synthesis, in which a waveguide
mesh is both excited and damped by a 2D matrix of forces
from a sensor. By emulating a drum skin both as controller
and sound generator, our instrument has reproduced some
of the expressive qualities of hand drumming. Details of our
implementation are discussed, as well as qualitative results
and experience gleaned from live performances.

</abstract>
    <keywords> Physical modeling, instrument design, expressive control, multi-touch, performance  </keywords>
  </document>
  <document>
    <name>nime2007_031.pdf</name>
    <abstract>
In this paper we describe the design and implementation of
the PHYSMISM: an interface for exploring the possibilities
for improving the creative use of physical modelling sound
synthesis.

The PHYSMISM is implemented in a software and hard-
ware version. Moreover, four different physical modelling
techniques are implemented, to explore the implications of
using and combining different techniques.

In order to evaluate the creative use of physical models,
a test was performed using 11 experienced musicians as test
subjects. Results show that the capability of combining the
physical models and the use of a physical interface engaged
the musicians in creative exploration of physical models.

</abstract>
    <keywords> Physical models, hybrid instruments, excitation, resonator.  </keywords>
  </document>
  <document>
    <name>nime2007_037.pdf</name>
    <abstract> 
A novel electronic percussion synthesizer prototype is presented. 
Our ambition is to design an instrument that will produce a high 
quality, realistic sound based on a physical modelling sound 
synthesis algorithm. This is achieved using a real-time Field 
Programmable Gate Array (FPGA) implementation of the model 
coupled to an interface that aims to make efficient use of all the 
subtle nuanced gestures of the instrumentalist. It is based on a 
complex physical model of the vibrating plate - the source of 
sound in the majority of percussion instruments. A Xilinx Virtex 
II pro FPGA core handles the sound synthesis computations with 
an 8 billion operations per second performance and has been 
designed in such a way to allow a high level of control and 
flexibility. Strategies are also presented to that allow the 
parametric space of the model to be mapped to the playing 
gestures of the percussionist.    

</abstract>
    <keywords>  Physical Model, Electronic Percussion Instrument, FPGA.   </keywords>
  </document>
  <document>
    <name>nime2007_041.pdf</name>
  </document>
  <document>
    <name>nime2007_046.pdf</name>
    <abstract> 

Figure

We present Zstretch, a textile music controller that supports 
expressive haptic interactions. The musical controller takes 
advantage of the fabric's topological constraints to enable 
proportional control of musical parameters.  This novel interface 
explores ways in which one might treat music as a sheet of cloth. 
This paper proposes an approach to engage simple technologies 
for supporting ordinary hand interactions. We show that this 
combination of basic technology with general tactile movements 
can result in an expressive musical interface. 

a

</abstract>
    <keywords>  Tangible interfaces, textiles, tactile design, musical expressivity   </keywords>
  </document>
  <document>
    <name>nime2007_056.pdf</name>
    <abstract> 
In this paper, we present a new system, the Orchestra Explorer, 
enabling a novel paradigm for active fruition of sound and music 
content. The Orchestra Explorer allows users to physically 
navigate inside a virtual orchestra, to actively explore the music 
piece the orchestra is playing, to modify and mold the sound and 
music content in real-time through their expressive full-body 
movement and gesture. An implementation of the Orchestra 
Explorer was developed and presented in the framework of the 
science exhibition "Cimenti di Invenzione e Armonia", held at 
Casa Paganini, Genova, from October 2006 to January 2007. 

</abstract>
    <keywords>  Active listening of music, expressive interfaces, full-body motion  analysis and expressive gesture processing, multimodal interactive  systems for music and performing arts applications.   </keywords>
  </document>
  <document>
    <name>nime2007_062.pdf</name>
    <abstract> 
We present the Multimodal Music Stand (MMMS) for the 
untethered sensing of performance gestures and the interactive 
control of music.  Using e-field sensing, audio analysis, and 
computer vision, the MMMS captures a performer's continuous 
expressive gestures and robustly identifies discrete cues in a 
musical performance.  Continuous and discrete gestures are sent 
to an interactive music system featuring custom designed software 
that performs real-time spectral transformation of audio. 

</abstract>
    <keywords>  Multimodal, interactivity, computer vision, e-field sensing,  untethered control.   </keywords>
  </document>
  <document>
    <name>nime2007_066.pdf</name>
    <abstract>
This paper describes the T-Stick, a new family of digital
musical instruments. It presents the motivation behind the
project, hardware and software design, and presents insights
gained through collaboration with performers who have col-
lectively practised and performed with the T-Stick for hun-
dreds of hours, and with composers who have written pieces
for the instrument in the context of McGill University's Dig-
ital Orchestra project. Each of the T-Sticks is based on the
same general structure and sensing platform, but each also
differs from its siblings in size, weight, timbre and range.

</abstract>
    <keywords> gestural controller, digital musical instrument, families of instruments  </keywords>
  </document>
  <document>
    <name>nime2007_070.pdf</name>
    <keywords>  Musical Instrument Design, Mapping, Musicianship, evaluation,  testing.   </keywords>
  </document>
  <document>
    <name>nime2007_078.pdf</name>
    <abstract>
In this paper, we present a new bi-manual gestural control-

ler, called HandSketch, composed of purchasable devices :
pen tablet and pressure-sensing surfaces. It aims at achie-
ving real-time manipulation of several continuous and arti-
culated aspects of pitched sounds synthesis, with a focus on
expressive voice. Both prefered and non-prefered hand is-
sues are discussed. Concrete playing diagrams and mapping
strategies are described. These results are integrated and a
compact controller is proposed.

</abstract>
    <keywords> Pen tablet, FSR, bi-manual gestural control.  </keywords>
  </document>
  <document>
    <name>nime2007_082.pdf</name>
    <keywords> Portable keyboard, Additional black keys, Diapason change  </keywords>
  </document>
  <document>
    <name>nime2007_088.pdf</name>
    <abstract>
This paper presents a line of historic electronic musical in-
struments designed by Erkki Kurenniemi in the 1960's and
1970's. Kurenniemi's instruments were influenced by digital
logic and an experimental attitude towards user interface
design. The paper presents an overview of Kurenniemi's
instruments and a detailed description of selected devices.
Emphasis is put on user interface issues such as unconven-
tional interactive real-time control and programming meth-
ods.

</abstract>
    <keywords> Erkki Kurenniemi, Dimi, Synthesizer, Digital electronics, User interface design  </keywords>
  </document>
  <document>
    <name>nime2007_094.pdf</name>
    <abstract> 
This paper reports on a survey conducted in the autumn of 2006 
with the objective to understand people's relationship to their 
musical tools. The survey focused on the question of embodiment 
and its different modalities in the fields of acoustic and digital 
instruments. The questions of control, instrumental entropy, 
limitations and creativity were addressed in relation to people's 
activities of playing, creating or modifying their instruments. The 
approach used in the survey was phenomenological, i.e. we were 
concerned with the experience of playing, composing for and 
designing digital or acoustic instruments. At the time of analysis, 
we had 209 replies from musicians, composers, engineers, 
designers, artists and others interested in this topic. The survey 
was mainly aimed at instrumentalists and people who create their 
own instruments or compositions in flexible audio programming 
environments such as SuperCollider, Pure Data, ChucK, 
Max/MSP, CSound, etc.   

</abstract>
    <keywords>  Survey, musical instruments, usability, ergonomics, embodiment,  mapping, affordances, constraints, instrumental entropy, audio  programming.   </keywords>
  </document>
  <document>
    <name>nime2007_100.pdf</name>
    <abstract> 
We summarize a decade of musical projects and research 
employing Wacom digitizing tablets as musical controllers, 
discussing general implementation schemes using Max/MSP and 
OpenSoundControl, and specific implementations in musical 
improvisation, interactive sound installation, interactive 
multimedia performance, and as a compositional assistant. We 
examine two-handed sensing strategies and schemes for gestural 
mapping.  

</abstract>
  </document>
  <document>
    <name>nime2007_106.pdf</name>
    <abstract> 
We describe the prevailing model of musical expression, which 

assumes a binary formulation of "the text" and "the act," along 

with its implied roles of composer and performer. We argue that 

this model not only excludes some contemporary aesthetic values 

but also limits the communicative ability of new music interfaces. 

As an alternative, an ecology of musical creation accounts for 

both a diversity of aesthetic goals and the complex interrelation of 

human and non-human agents. An ecological perspective on 

several approaches to musical creation with interactive 

technologies reveals an expanded, more inclusive view of artistic 

interaction that facilitates novel, compelling ways to use 

technology for music. This paper is fundamentally a call to 

consider the role of aesthetic values in the analysis of artistic 

processes and technologies. 

</abstract>
    <keywords>  Expression, expressivity, non-expressive, emotion, discipline,   model, construct, discourse, aesthetic goal, experience,   transparency, evaluation, communication   </keywords>
  </document>
  <document>
    <name>nime2007_112.pdf</name>
    <abstract>
Live coding is almost the antithesis of immediate physical
musicianship, and yet, has attracted the attentions of a number
of computer-literate musicians, as well as the music-savvy
programmers that might be more expected. It is within the
context of live coding that I seek to explore the question of
practising a contemporary digital musical instrument, which i s
often raised as an aside but more rarely carried out in research
(though see [12]). At what stage of expertise are the members
of the live coding movement, and what practice regimes might
help them to find their true potential?  

</abstract>
    <keywords> Practice, practising, live coding  </keywords>
  </document>
  <document>
    <name>nime2007_124.pdf</name>
    <abstract> 
We present in this paper a complete gestural interface built to 
support music pedagogy. The development of this prototype 
concerned both hardware and software components: a small 
wireless sensor interface including accelerometers and 
gyroscopes, and an analysis system enabling gesture following 
and recognition. A first set of experiments was conducted with 
teenagers in a music theory class. The preliminary results were 
encouraging concerning the suitability of these developments in 
music education. 

</abstract>
    <keywords>  Technology-enhanced learning, music pedagogy, wireless  interface, gesture-follower, gesture recognition   </keywords>
  </document>
  <document>
    <name>nime2007_130.pdf</name>
    <abstract> 
Augmenting performances of live popular music with computer 
systems poses many new challenges. Here, "popular music" is 
taken to mean music with a mostly steady tempo, some 
improvisational elements, and largely predetermined melodies, 
harmonies, and other parts. The overall problem is studied by 
developing a framework consisting of constraints and 
subproblems that any solution should address. These problems 
include beat acquisition, beat phase, score location, sound 
synthesis, data preparation, and adaptation. A prototype system is 
described that offers a set of solutions to the problems posed by 
the framework, and future work is suggested. 

</abstract>
  </document>
  <document>
    <name>nime2007_136.pdf</name>
    <abstract>
We present a system for rhythmic analysis of human motion in
real-time. Using a combination of both spectral (Fourier) and
spatial analysis of onsets, we are able to extract repeating rhyth-
mic patterns from data collected using accelerometers. These ex-
tracted rhythmic patterns show the relative magnitudes of accen-
tuated movements and their spacing in time. Inspired by previous
work in automatic beat detection of audio recordings, we designed
our algorithms to be robust to changes in timing using multiple
analysis techniques and methods for sensor fusion, filtering and
clustering. We tested our system using a limited set of movements,
as well as dance movements collected from a professional, both
with promising results.

</abstract>
    <keywords> rhythm analysis, dance movement analysis, onset analysis  </keywords>
  </document>
  <document>
    <name>nime2007_142.pdf</name>
    <abstract>
Remote real-time musical interaction is a domain where end-
to-end latency is a well known problem. Today, the main
explored approach aims to keep it below the musicians per-
ception threshold. In this paper, we explore another ap-
proach, where end-to-end delays rise to several seconds, but
computed in a controlled (and synchronized) way depending
on the structure of the musical pieces. Thanks to our fully
distributed prototype called nJam, we perform user experi-
ments to show how this new kind of interactivity breaks the
actual end-to-end latency bounds.

</abstract>
    <keywords> Remote real-time musical interaction, end-to-end delays, syn- chronization, user experiments, distributed metronome, NMP.  </keywords>
  </document>
  <document>
    <name>nime2007_148.pdf</name>
    <abstract>
This paper describes the Ashitaka audiovisual instrument
and the process used to develop it. The main idea guiding
the design of the instrument is that motion can be used to
connect audio and visuals, and the first part of the paper
consists of an exploration of this idea. The issue of map-
pings is raised, discussing both audio-visual mappings and
the mappings between the interface and synthesis methods.
The paper concludes with a detailed look at the instrument
itself, including the interface, synthesis methods, and map-
pings used.

</abstract>
  </document>
  <document>
    <name>nime2007_154.pdf</name>
    <abstract>
This paper describes several example hybrid acoustic / elec-
tronic percussion instruments using realtime convolution to
augment and modify the apparent acoustics of damped phys-
ical objects. Examples of cymbal, frame drum, practice pad,
brush, and bass drum controllers are described.

</abstract>
    <keywords> Musical controllers, extended acoustic instruments  </keywords>
  </document>
  <document>
    <name>nime2007_160.pdf</name>
    <abstract>
CaMus2 allows collaborative performance with mobile cam-
era phones. The original CaMus project was extended to
support multiple phones performing in the same space and
generating MIDI signals to control sound generation and
manipulation software or hardware. Through an optical
flow technology the system can be used without a reference
marker grid. When using a marker grid, the use of dynamic
digital zoom extends the range of performance. Semantic
information display helps guide the performer visually.

</abstract>
    <keywords> Camera phone, mobile phone, music performance, mobile sound generation, sensing-based interaction, collaboration  </keywords>
  </document>
  <document>
    <name>nime2007_168.pdf</name>
    <abstract> 
In this paper the authors present the MIDI Scrapyard Challenge 
(MSC) workshop, a one-day hands-on experience which asks 
participants to create musical controllers out of cast-off 
electronics, found materials and junk. The workshop experience, 
principles, and considerations are detailed, along with sample 
projects which have been created in various MSC workshops. 
Observations and implications as well as future developments for 
the workshop are discussed.  

</abstract>
  </document>
  <document>
    <name>nime2007_172.pdf</name>
    <abstract>
We present REXband, an interactive music exhibit for collabora-
tive improvisation to medieval music. This audio-only system con-
sists of three digitally augmented medieval instrument replicas: the
hurdy gurdy, harp, and frame drum. The instruments communicate
with software that provides users with both musical support and
feedback on their performance using a "virtual audience" set in a
medieval tavern. REXband builds upon previous work in interactive
music exhibits by incorporating aspects of e-learning to educate, in
addition to interaction design patterns to entertain; care was also
taken to ensure historic authenticity. Feedback from user testing
in both controlled (laboratory) and public (museum) environments
has been extremely positive. REXband is part of the Regensburg
Experience, an exhibition scheduled to open in July 2007 to show-
case the rich history of Regensburg, Germany.

</abstract>
    <keywords> interactive music exhibits, medieval music, augmented instru- ments, e-learning, education  </keywords>
  </document>
  <document>
    <name>nime2007_178.pdf</name>
    <abstract>
This paper describes work on a newly created large-scale
interactive theater performance entitled Schwelle (Thresh-
olds). The authors discuss an innovative approach towards
the conception, development and implementation of dynamic
and responsive audio scenography : a constantly evolving,
multi-layered sound design generated by continuous input
from a series of distributed wireless sensors deployed both
on the body of a performer and placed within the physical
stage environment. The paper is divided into conceptual
and technological parts. We first describe the project's dra-
maturgical and conceptual context in order to situate the
artistic framework that has guided the technological system
design. Specifically, this framework discusses the team's ap-
proach in combining techniques from situated computing,
theatrical sound design practice and dynamical systems in
order to create a new kind of adaptive audio scenographic
environment augmented by wireless, distributed sensing for
use in live theatrical performance. The goal of this adap-
tive sound design is to move beyond both existing playback
models used in theatre sound as well as the purely human-
centered, controller-instrument approach used in much cur-
rent interactive performance practice.

</abstract>
    <keywords> Interactive performance, dynamical systems, wireless sens- ing, adaptive audio scenography, audio dramaturgy, situated computing, sound design  </keywords>
  </document>
  <document>
    <name>nime2007_185.pdf</name>
    <keywords> Architecture, installation, interaction, granular synthesis, adaptation, engagement.  </keywords>
  </document>
  <document>
    <name>nime2007_191.pdf</name>
    <abstract>
The distinctive features of interactive sound installations in
public space are considered, with special attention to the
rich, if undoubtedly difficult, environments in which they
exist. It is argued that such environments, and the social
contexts that they imply, are among the most valuable fea-
tures of these works for the approach that we have adopted
to creation as research practice. The discussion is articu-
lated through case studies drawn from two of our instal-
lations, Recycled Soundscapes (2004) and Skyhooks (2006).
Implications for the broader design of new musical instru-
ments are presented.

</abstract>
  </document>
  <document>
    <name>nime2007_197.pdf</name>
    <abstract> 
In this paper we introduce a System conceived to serve as the 
"musical brain" of autonomous musical robots or agent-based 
software simulations of robotic systems. Our research goal is to 
provide robots with the ability to integrate with the musical 
culture of their surroundings. In a multi-agent configuration, the 
System can simulate an environment in which autonomous agents 
interact with each other as well as with external agents (e.g., 
robots, human beings or other systems). The main outcome of 
these interactions is the transformation and development of their 
musical styles as well as the musical style of the environment in 
which they live. 

</abstract>
  </document>
  <document>
    <name>nime2007_203.pdf</name>
    <abstract>
  WISEAR  (Wireless  Sensor  Array)8,  provides  a  robust  and
scalable  platform for  virtually  limitless  types  of  data  input  to
software synthesis engines.  It is essentially a Linux based SBC
(Single  Board Computer)  with  802.11a/b/g wireless capability.
The device, with batteries, only weighs a few pounds and can be
worn by a dancer or other live performer.  Past work has focused
on  connecting  "conventional"  sensors  (eg.,  bend  sensors,
accelerometers, FSRs, etc...) to the board and using it as a data
relay, sending the data as real time control messages to synthesis
engines  like  Max/MSP  and  RTcmix1.   Current  research  has
extended the abilities of the device to take real-time audio and
video  data  from USB cameras  and  audio  devices,  as  well  as
running synthesis engines on board the  device itself.  Given its
generic network ability (eg., being an 802.11a/b/g device) there is
theoretically no limit to the number of WISEAR boxes that can
be  used  simultaneously  in  a  performance,  facilitating  multi-
performer compositions.

  This  paper  will  present  the  basic  design  philosophy  behind
WISEAR, explain some of the basic concepts and methods, as
well as provide a live demonstration of the running device, worn
by the author.

</abstract>
    <keywords> Wireless, sensors, embedded devices, linux, real-time audio, real- time video  </keywords>
  </document>
  <document>
    <name>nime2007_205.pdf</name>
    <keywords> Inertial Measurement Unit, IMU, Position Tracking, Interactive Dance Performance, Graphical Object, Mapping.  </keywords>
  </document>
  <document>
    <name>nime2007_209.pdf</name>
    <abstract>
This paper presents an approach to audio-haptic integra-
tion that utilizes Open Sound Control, an increasingly well-
supported standard for audio communication, to initialize
and communicate with dynamic virtual environments that
work with off-the-shelf force-feedback devices.

</abstract>
    <keywords> Haptics, control, multi-modal, audio, force-feedback  </keywords>
  </document>
  <document>
    <name>nime2007_213.pdf</name>
    <abstract> 
Chroma based representations of acoustic phenomenon are 
representations of sound as pitched acoustic energy.  A frame-
wise chroma distribution over an entire musical piece is a useful 
and straightforward representation of its musical pitch over time.  
This paper examines a method of condensing the block-wise 
chroma information of a musical piece into a two dimensional 
embedding.  Such an embedding is a representation or map of the 
different pitched energies in a song, and how these energies relate 
to each other in the context of the song.  The paper presents an 
interactive version of this representation as an exploratory 
analytical tool or instrument for granular synthesis.  Pointing and 
clicking on the interactive map recreates the acoustical energy 
present in the chroma blocks at that location, providing an 
effective way of both exploring the relationships between sounds 
in the original piece, and recreating a synthesized approximation 
of these sounds in an instrumental fashion.   

</abstract>
    <keywords>  Chroma, granular synthesis, dimensionality reduction   </keywords>
  </document>
  <document>
    <name>nime2007_220.pdf</name>
  </document>
  <document>
    <name>nime2007_224.pdf</name>
    <abstract>
This  report  presents  the  design  and  construct ion  of  Rage  in 
Conjunction  with  the  Machine,  a  simple  but  novel  pairing  of 
musical  interface  and sound sculpture.  The authors  discuss  the 
design  and creation of this  instrument ,  focusing  on the unique 
aspects  of  it,  including  the  use  of  physical  systems,  large 
gestural  input,  scale,  and the electronic coupling  of a physical  
input to a physical output.

</abstract>
  </document>
  <document>
    <name>nime2007_234.pdf</name>
    <abstract>
This paper describes the development of B-Keeper, a rea-
time beat tracking system implemented in Java and Max/MSP,
which is capable of maintaining synchronisation between an
electronic sequencer and a drummer. This enables musi-
cians to interact with electronic parts which are triggered
automatically by the computer from performance informa-
tion. We describe an implementation which functions with
the sequencer Ableton Live.

</abstract>
    <keywords> Human-Computer Interaction, Automatic Accompaniment, Performance  </keywords>
  </document>
  <document>
    <name>nime2007_238.pdf</name>
    <abstract> 
This paper describes a system enabling a human to perform music 
with a robot in real-time, in the context of North Indian classical 
music. We modify a traditional acoustic sitar into a 
hyperinstrument in order to capture performance gestures for 
musical analysis. A custom built four-armed robotic Indian 
drummer was built using a microchip, solenoids, aluminum and 
folk frame drums. Algorithms written towards "intelligent" 
machine musicianship are described. The final goal of this 
research is to have a robotic drummer accompany a professional 
human sitar player live in performance.  

</abstract>
    <keywords>  Musical Robotics, Electronic Sitar, Hyperinstruments, Music  Information Retrieval (MIR).   </keywords>
  </document>
  <document>
    <name>nime2007_242.pdf</name>
    <abstract>
The starting point for this project is the want to produce a
music controller that could be employed in such a manner that
even lay public could enjoy the possibilities of mobile art. All
of the works that are discussed here are in relation to a new
GPS-based controller, the Wrist-Conductor. The works are
technically based around the synchronizing possibilities
using the GPS Time Mark and are aesthetically rooted in works
that function in an open public space such as a city or a forest.
One of the works intended for the controller, China Gates, i s
discussed here in detail in order to describe how the GPS
Wrist-Controller is actually used in a public art context. The
other works, CitySonics, The Enchanted Forest and Get a Pot
&amp; a Spoon are described briefly in order to demonstrate that
even a simple controller can be used to create a body of works.
This paper also  addresses the breaking of the media bubble
via the concept of the "open audience", or how mobile art can
engage pedestrians as viewers or listeners within public space
and not remain an isolated experience for performers only.

</abstract>
    <keywords> Mobile Music, GPS, Controller, Collaborative Performance  </keywords>
  </document>
  <document>
    <name>nime2007_246.pdf</name>
    <abstract> 
This paper presents an electronic piano keyboard and computer 
mouse designed for use in a magnetic resonance imaging scanner.  
The interface allows neuroscientists studying motor learning of 
musical tasks to perform functional scans of a subject's brain 
while synchronizing the scanner, auditory and visual stimuli, and 
auditory feedback with the onset, offset, and velocity of the piano 
keys.  The design of the initial prototype and environment-specific 
issues are described, as well as prior work in the field.  
Preliminary results are positive and were unable to show the 
existence of image artifacts caused by the interface.  
Recommendations to improve the optical assembly are provided 
in order to increase the robustness of the design.   

</abstract>
    <keywords>  Input device, MRI-compatible, fMRI, motor learning, optical  sensing.   </keywords>
  </document>
  <document>
    <name>nime2007_254.pdf</name>
    <abstract> 
This study proposes new possibilities for interaction design 
pertaining to music piece creation.  Specifically, the study created 
an environment wherein a wide range of users are able to easily 
experience new musical expressions via a combination of newly 
developed software and the Nintendo Wii Remote controller. 

</abstract>
    <keywords>  Interactive systems, improvisation, gesture, composition   INTRODUCTION  Though music related research focusing on the interaction  between people and computers is currently experiencing wide  range development, the history of approaches wherein the creation  of new musical expression is made possible via the active  </keywords>
  </document>
  <document>
    <name>nime2007_256.pdf</name>
    <abstract>
Almost all traditional musical instruments have a one-to-one 
correspondence between a given fingering  and the pitch that 
sounds  for that fingering. The Samchillian Tip Tip Tip 
Cheeepeeeee does not - it  is a keyboard MIDI controller that is 
based on intervals  rather than fixed pitches. That is, a given 
keypress will sound a pitch a number of steps away from the last 
note sounded (within the key signature and scale selected) 
according to the 'delta'  value assigned to that key. The advantages 
of such a system are convenience, speed, and the ability to play 
difficult, unusual and/or unintended passages extemporaneously. 

</abstract>
    <keywords> samchillian, keyboard, MIDI controller, relative, interval,  microtonal, computer keyboard, pitch, musical instrument  </keywords>
  </document>
  <document>
    <name>nime2007_260.pdf</name>
    <abstract> 
Graph Theory links the creative music-making activities of web 
site visitors to the dynamic generation of an instrumental score for 
solo violin. Participants use a web-based interface to navigate 
among short, looping musical fragments to create their own 
unique path through the open-form composition. Before each 
concert performance, the violinist prints out a new copy of the 
score that orders the fragments based on the decisions made by 
web visitors. 

</abstract>
    <keywords>  Music, Composition, Residency, Audience Interaction,  Collaboration, Violin, Graph, Flash, Internet, Traveling Salesman.   </keywords>
  </document>
  <document>
    <name>nime2007_264.pdf</name>
    <abstract> 
This paper describes the design and implementation of a new 
interface prototype for live music mixing. The ColorDex system 
employs a completely new operational metaphor which allows the 
mix DJ to prepare up to six tracks at once, and perform mixes 
between up to three of those at a time. The basic premises of the 
design are: 1) Build a performance tool that multiplies the 
possible choices a DJ has in respect in how and when tracks are 
prepared and mixed; 2) Design the system in such a way that the 
tool does not overload the performer with unnecessary 
complexity, and 3) Make use of novel technology to make the 
performance of live music mixing more engaging for both the 
performer and the audience. The core components of the system 
are: A software program to load, visualize and playback digitally 
encoded tracks; the HDDJ device (built chiefly out of a 
repurposed hard disk drive), which provides tactile manipulation 
of the playback speed and position of tracks; and the Cubic 
Crossfader, a wireless sensor cube that controls of the volume of 
individual tracks, and allows the DJ to mix these in interesting 
ways. 

</abstract>
    <keywords>  Novel interfaces, live music-mixing, cube-based interfaces,  crossfading, repurposing HDDs, accelerometer-based cubic  control   </keywords>
  </document>
  <document>
    <name>nime2007_270.pdf</name>
    <keywords>  Musical controller, Puredata, scanned synthesis, flex sensors.   </keywords>
  </document>
  <document>
    <name>nime2007_273.pdf</name>
    <abstract> 
This paper proposes that the physicality of an instrument be 
considered an important aspect in the design of new interfaces for 
musical expression. The use of Laban's theory of effort in the 
design of new effortful interfaces, in particular looking at effort-
space modulation, is investigated, and a platform for effortful 
interface development (named the DAMPER) is described. 
Finally, future work is described and further areas of research are 
highlighted.  

</abstract>
    <keywords>  Effortful Interaction. Haptics. Laban Analysis. Physicality. HCI.   </keywords>
  </document>
  <document>
    <name>nime2007_277.pdf</name>
    <abstract>
This paper describes the design of Mimi, a multi-modal in-
teractive musical improvisation system that explores the po-
tential and powerful impact of visual feedback in performer-
machine interaction. Mimi is a performer-centric tool de-
signed for use in performance and teaching. Its key and
novel component is its visual interface, designed to provide
the performer with instantaneous and continuous informa-
tion on the state of the system. For human improvisation,
in which context and planning are paramount, the relevant
state of the system extends to the near future and recent
past. Mimi's visual interface allows for a peculiar blend
of raw reflex typically associated with improvisation, and
preparation and timing more closely affiliated with score-
based reading. Mimi is not only an effective improvisation
partner, it has also proven itself to be an invaluable platform
through which to interrogate the mental models necessary
for successful improvisation.

</abstract>
    <keywords> Performer-machine interaction, visualization design, machine improvisation  </keywords>
  </document>
  <document>
    <name>nime2007_281.pdf</name>
    <abstract>
Many fascinating new developments in the area bowed stringed in-
struments have been developed in recent years. However, the ma-
jority of these new applications are either not well known, used or
considered in a broader context by their target users. The necessary
exchange between the world of developers and the players is rather
limited. A group of performers, researchers, instrument developers
and composers was founded in order to share expertise and experi-
ences and to give each other feedback on the work done to develop
new instruments. Instruments incorporating new interfaces, synthe-
sis methods, sensor technology, new materials like carbon fiber and
wood composites as well as composite materials and research out-
come are presented and discussed in the group. This paper gives an
introduction to the group and reports about activities and outcomes
in the last two years.

</abstract>
    <keywords> Interdisciplinary user group, electronic bowed string instrument, evaluation of computer based musical instruments  </keywords>
  </document>
  <document>
    <name>nime2007_289.pdf</name>
    <abstract> 
In this paper, we describe the composition of a piece for choir and 
Integral Music Controller. We focus more on the aesthetic, 
conceptual, and practical aspects of the interface and less on the 
technological details. We especially stress the influence that the 
designed interface poses on the compositional process and how 
we approach the expressive organisation of musical materials 
during the composition of the piece, as well as the addition of 
nuances (personal real-time expression) by the musicians at 
performance time. 

</abstract>
    <keywords>  Composition, Integral Music Controller, Emotion measurement,  Physiological Measurement, Spatialisation.   </keywords>
  </document>
  <document>
    <name>nime2007_293.pdf</name>
    <abstract> 
We present an interactive sound spatialization and synthesis 
system based on Interaural Time Difference (ITD) model and 
Evolutionary Computation. We define a Sonic Localization Field 
using sound attenuation and ITD azimuth angle parameters and, in 
order to control an adaptive algorithm, we used pairs of these 
parameters as Spatial Sound Genotypes (SSG). They are extracted 
from waveforms which are considered individuals of a Population 
Set. A user-interface receives input from a generic gesture 
interface (such as a NIME device) and interprets them as ITD 
cues. Trajectories provided by these signals are used as Target 
Sets of an evolutionary algorithm. A Fitness procedure optimizes 
locally the distance between the Target Set and the SSG pairs. 
Through a parametric score the user controls dynamic changes in 
the sound output.  

</abstract>
    <keywords>  interactive, sound, spatialization, evolutionary, adaptation.   </keywords>
  </document>
  <document>
    <name>nime2007_299.pdf</name>
    <abstract>
In this project,  eye tracking researchers and computer music 
composers collaborate to create musical compositions that 
are played with the eyes.  A commercial eye tracker (LC 
Technologies Eyegaze) is connected to a music and 
multimedia authoring environment (Max/MSP/Jitter).  The 
project addresses issues of both noise and control:  How 
will the performance benefit from the noise inherent in eye 
trackers and eye movements, and to what extent should the 
composition encourage the performer to try to control a 
specific musical outcome?  Providing one set of answers to 
these two questions, the authors create an eye-controlled 
composition, EyeMusic v1.0, which was selected by juries 
for live performance at computer music conferences.

Author Keywords
Computer music, eye tracking, new media art, performance.

ACM Classification </abstract>
    <keywords> H.5.2 [Information Interfaces and Presentation] User  Interfaces - input devices and strategies, interaction styles.   J.5 [Arts and Humanities] Fine arts, performing arts.   </keywords>
  </document>
  <document>
    <name>nime2007_301.pdf</name>
    <abstract>
The FrankenPipe project is an attempt to convert a traditional
Highland Bagpipe into a controller capable of driving both real-

time synthesis on a laptop as well as a radio-controlled (RC) car.
Doing so engages musical creativity while enabling novel, often
humorous, performance art. The chanter is outfitted with
photoresistors (CdS photoconductive cells) underneath each hole,
allowing a full range of MIDI values to be produced with each
finger and giving the player a natural feel.  An air-pressure sensor
is also deployed in the bag to provide another element of control
while capturing a fundamental element of bagpipe performance.

The final product navigates the realm of both musical instrument
and toy, allowing the performer to create a novel yet rich
performance experience for the audience.

</abstract>
    <keywords> FrankenPipe, alternate controller, MIDI, bagpipe, photoresistor, chanter.  </keywords>
  </document>
  <document>
    <name>nime2007_305.pdf</name>
    <abstract>
EyesWeb XMI (for eXtended Multimodal Interaction) is the new 
version of the well-known EyesWeb platform. It has a main focus 
on multimodality and the main design target of this new release 
has been to improve the ability to process and correlate several 
streams of  data.  It  has  been used  extensively to  build  a set  of 
interactive systems for performing arts  applications for Festival 
della Scienza 2006, Genoa, Italy. The purpose of this paper is to 
describe the developed installations as well as the new EyesWeb 
features that helped in their development.

</abstract>
    <keywords> EyesWeb, multimodal interactive systems, performing arts.  </keywords>
  </document>
  <document>
    <name>nime2007_309.pdf</name>
    <abstract> 
A crucial set of decisions in digital musical instrument design 
deals with choosing mappings between parameters controlled by 
the performer and the synthesis algorithms that actually generate 
sound. Feature-based synthesis offers a way to parameterize audio 
synthesis in terms of the quantifiable perceptual characteristics, or 
features, the performer wishes the sound to take on. Techniques 
for accomplishing such mappings and enabling feature-based 
synthesis to be performed in real time are discussed. An example 
is given of how a real-time performance system might be designed 
to take advantage of feature-based synthesis's ability to provide 
perceptually meaningful control over a large number of synthesis 
parameters. 

</abstract>
    <keywords>  Feature, Synthesis, Analysis, Mapping, Real-time.   </keywords>
  </document>
  <document>
    <name>nime2007_313.pdf</name>
    <abstract>
This paper introduces jPop-E (java-based PolyPhrase En-
semble), an assistant system for the Pop-E performance
rendering system. Using this assistant system, MIDI data
including expressive tempo changes or velocity control can
be created based on the user's musical intention. Pop-E
(PolyPhrase Ensemble) is one of the few machine systems
devoted to creating expressive musical performances that
can deal with the structure of polyphonic music and the
user's interpretation of the music. A well-designed graphi-
cal user interface is required to make full use of the poten-
tial ability of Pop-E. In this paper, we discuss the necessary
elements of the user interface for Pop-E, and describe the
implemented system, jPop-E.

</abstract>
    <keywords> Performance Rendering, User Interface, Ensemble Music Ex- pression  </keywords>
  </document>
  <document>
    <name>nime2007_317.pdf</name>
    <abstract>
Playing music over the Internet, whether for real-time jam-
ming, network performance or distance education, is con-
strained by the speed of light which introduces, over long dis-
tances, time delays unsuitable for musical applications. Cur-
rent musical collaboration systems generally transmit com-
pressed audio streams over low-latency and high-bandwidth
networks to optimize musician synchronization. This paper
proposes an alternative approach based on pattern recogni-
tion and music prediction. Trained for a particular type
of music, here the Indian tabla drum, the system called
TablaNet identifies rhythmic patterns by recognizing indi-
vidual strokes played by a musician and mapping them dy-
namically to known musical constructs. Symbols represent-
ing these musical structures are sent over the network to
a corresponding computer system. The computer at the
receiving end anticipates incoming events by analyzing pre-
vious phrases and synthesizes an estimated audio output.
Although such a system may introduce variants due to pre-
diction approximations, resulting in a slightly different mu-
sical experience at both ends, we find that it demonstrates
a high level of playability with an immediacy not present in
other systems, and functions well as an educational tool.

</abstract>
    <keywords> network music performance, real-time online musical collab- oration, Indian percussions, tabla bols, strokes recognition, music prediction  </keywords>
  </document>
  <document>
    <name>nime2007_321.pdf</name>
    <keywords> JamiOki, PureJoy, collaborative performance, structured im- provisation, electronically-mediated performance, found sound  </keywords>
  </document>
  <document>
    <name>nime2007_330.pdf</name>
    <abstract>
The guitar pick has traditionally been used to strike or rake
the strings of a guitar or bass, and in rarer instances, a
shamisen, lute, or other stringed instrument. The pres-
sure exerted on it, however, has until now been ignored.
The MIDI Pick, an enhanced guitar pick, embraces this di-
mension, acting as a trigger for serial data, audio samples,
MIDI messages 1, Max/MSP patches, and on/off messages.
This added scope expands greatly the stringed instrument
player's musical dynamic in the studio or on stage.

</abstract>
    <keywords> guitar, MIDI, pick, plectrum, wireless, bluetooth, ZigBee, Arduino, NIME, ITP  </keywords>
  </document>
  <document>
    <name>nime2007_334.pdf</name>
    <abstract> 
This paper describes the design and experimentation of a Kalman 
Filter used to improve position tracking of a 3-D gesture-based 
musical controller known as the Radiodrum. The Singer dynamic 
model for target tracking is used to describe the evolution of a 
Radiodrum's stick position in time. The autocorrelation time 
constant of a gesture's acceleration and the variance of the gesture 
acceleration are used to tune the model to various performance 
modes. Multiple Kalman Filters tuned to each gesture type are run 
in parallel and an Interacting Multiple Model (IMM) is 
implemented to decide on the best combination of filter outputs to 
track the current gesture. Our goal is to accurately track 
Radiodrum gestures through noisy measurement signals.  

</abstract>
    <keywords>  Kalman Filtering, Radiodrum, Gesture Tracking, Interacting  Multiple Model   INTRODUCTION  Intention is a key aspect of traditional music performance. The  ability for an artist to reliably reproduce sound, pitch, rhythms,  and emotion is paramount to the design of any instrument. With  the </keywords>
  </document>
  <document>
    <name>nime2007_338.pdf</name>
    <abstract>
In this paper, design scenarios made possible by the use of an 
interactive illuminated floor as the basis of an audiovisual 
environment are presented. By interfacing a network  of pressure 
sensitive, light-emitting  tiles with a 7.1 channel  speaker system 
and requisite audio software, many avenues for collaborative 
expression emerge, as do heretofore unexplored modes of 
multiplayer music and dance gaming. By  giving users  light and 
sound cues that both guide and respond to their movement, a rich 
environment is created that playfully integrates the auditory, the 
visual, and the kinesthetic into a unified interactive experience.

</abstract>
    <keywords> Responsive Environments, Audiovisual  Play, Kinetic Games,  Movement Rich Game Play,  Immersive Dance, Smart Floor  </keywords>
  </document>
  <document>
    <name>nime2007_344.pdf</name>
    <abstract>
We present a new group of audio effects that use beat track-
ing, the detection of beats in an audio signal, to relate effect
parameters to the beats in an input signal. Conventional au-
dio effects are augmented so that their operation is related to
the output of a beat tracking system. We present a tempo-
synchronous delay effect and a set of beat synchronous low
frequency oscillator effects including tremolo, vibrato and
auto-wah. All effects are implemented in real-time as VST
plug-ins to allow for their use in live performance.

</abstract>
  </document>
  <document>
    <name>nime2007_346.pdf</name>
    <abstract> 
This article presents various custom software tools called 
Automatic Notation Generators (ANG's) developed by the 
authors to aid in the creation of algorithmic instrumental 
compositions. The unique possibilities afforded by ANG software 
are described, along with relevant examples of their compositional 
output. These avenues of exploration include: mappings of 
spectral data directly into notated music, the creation of software 
transcribers that enable users to generate multiple realizations of 
algorithmic compositions, and new types of spontaneous 
performance with live generated screen-based music notation. The 
authors present their existing software tools along with 
suggestions for future research and artistic inquiry. 

</abstract>
  </document>
  <document>
    <name>nime2007_352.pdf</name>
    <abstract>
This paper presents a newly created database containing
calibrated gesture and audio data corresponding to various
violin bowstrokes, as well as video and motion capture data
in some cases. The database is web-accessible and search-
able by keywords and subject. It also has several important
features designed to improve accessibility to the data and to
foster collaboration between researchers in fields related to
bowed string synthesis, acoustics, and gesture.

</abstract>
    <keywords> violin, bowed string, bowstroke, bowing, bowing parame- ters, technique, gesture, audio  </keywords>
  </document>
  <document>
    <name>nime2007_358.pdf</name>
    <abstract> 
This paper presents a methodology and a set of tools for gesture 
control of sources in 3D surround sound. The techniques for 
rendering acoustic events on multi-speaker or headphone-based 
surround systems have evolved considerably, making it possible 
to use them in real-time performances on light equipment. 
Controlling the placement of sound sources is usually done in 
idiosyncratic ways and has not yet been fully explored and 
formalized. This issue is addressed here with the proposition of a 
methodical approach. The mapping of gestures to source motion is 
implemented by giving the sources physical object properties and 
manipulating these characteristics with standard geometrical 
transforms through hierarchical or emergent relationships. 

</abstract>
    <keywords>  Gesture, Surround Sound, Mapping, Trajectory, Transform  Matrix, Tree Hierarchy, Emergent Structures.   </keywords>
  </document>
  <document>
    <name>nime2007_363.pdf</name>
    <abstract> 
This work presents an interactive device to control an adaptive 
tuning and synthesis system. The gestural controller is based on 
the theremin concept in which only an antenna is used as a 
proximity sensor. This interactive process is guided by sensorial 
consonance curves and adaptive tuning related to 
psychoacoustical studies.  We used an algorithm to calculate the 
dissonance values according to amplitudes and frequencies of a 
given sound spectrum. The theoretical background is presented 
followed by interactive composition strategies and sound results. 

</abstract>
    <keywords>  Interaction, adaptive tuning, theremin, sensorial dissonance,  synthesis.   </keywords>
  </document>
  <document>
    <name>nime2007_367.pdf</name>
    <abstract> 
In previous publications (see for example [2] and [3]), we 
described an interactive music system, designed to improvise with 
saxophonist John Butcher; our system analyzes timbral and 
gestural features in real-time, and uses this information to guide 
response generation. This paper overviews our recent work with 
the system's interaction management component (IMC). We 
explore several options for characterizing improvisation at a 
higher level, and managing decisions for interactive performance 
in a rich timbral environment. We developed a simple, efficient 
framework using a small number of features suggested by recent 
work in mood modeling in music. We describe and evaluate the 
first version of the IMC, which was used in performance at the 
Live Algorithms for Music (LAM) conference in December 2006. 
We touch on developments on the system since LAM, and discuss 
future plans to address perceived shortcomings in responsiveness, 
and the ability of the system to make long-term adaptations.  

</abstract>
    <keywords>  Interactive music systems, timbral analysis, free improvisation.   </keywords>
  </document>
  <document>
    <name>nime2007_371.pdf</name>
    <abstract> 
Until recently, the sonification of Virtual Environments had often 

been reduced to its simplest expression. Too often soundscapes 

and background music are predetermined, repetitive and 

somewhat predictable. Yet, there is room for more complex and 

interesting sonification schemes that can improve the sensation of 

presence in a Virtual Environment. In this paper we propose a 

system that automatically generates original background music in 

real-time called VR-RoBoser. As a test case we present the 

application of VR-RoBoser to a dynamic avatar that explores its 

environment. We show that the musical events are directly and 

continuously generated and influenced by the behavior of the 

avatar in three-dimensional virtual space, generating a context 

dependent sonification. 

</abstract>
    <keywords>  Real-time Composition, Interactive Sonification, Real-time   Neural Processing, Multimedia, Virtual Environment, Avatar.   </keywords>
  </document>
  <document>
    <name>nime2007_379.pdf</name>
    <abstract>
This paper describes musical experiments aimed at design-
ing control structures for navigating complex and continu-
ous sonic spaces. The focus is on sound processing tech-
niques which contain a high number of control parameters,
and which exhibit subtle and interesting micro-variations
and textural qualities when controlled properly. The exam-
ples all use a simple low-dimensional controller - a standard
graphics tablet - and the task of initimate and subtle textu-
ral manipulations is left to the design of proper mappings,
created using a custom toolbox of mapping functions. This
work further acts to contextualize past theoretical results by
the given musical presentations, and arrives at some conclu-
sions about the interplay between musical intention, control
strategies and the process of their design.

</abstract>
    <keywords> Mapping, Control, Sound Texture, Musical Gestures  </keywords>
  </document>
  <document>
    <name>nime2007_384.pdf</name>
    <abstract>
In  this  paper  we present  the  concept  and  prototype  of  a  new 
musical  interface  that  utilizes  the  close  relationship  between 
gestural  expression in  the act  of painting and that  of playing a 
musical  instrument  in  order  to  provide  non-musicians  the 
opportunity to create musical expression.  A physical brush on a 
canvas acts as the instrument. The characteristics of its stroke are 
intuitively mapped to a conductor  program, defining expressive 
parameters  of  the  tone  in  real-time.  Two  different  interaction 
modes highlight the importance of bodily expression in making 
music as well as the value of a metaphorical visual representation.

</abstract>
    <keywords> musical interface, musical expression, expressive gesture, musical  education, natural interface  </keywords>
  </document>
  <document>
    <name>nime2007_386.pdf</name>
    <abstract>
Freqtric Drums is a new musical, corporal electronic instru-
ment that allows us not only to recover face-to-face commu-
nication, but also makes possible body-to-body communica-
tion so that a self image based on the sense of being a sepa-
rate body can be signicant altered through an openness to
and even a sense of becoming part of another body. Freqtric
Drums is a device that turns audiences surrounding a per-
former into drums so that the performer, as a drummer, can
communicate with audience members as if they were a set
of drums. We describe our concept and the implementation
and process of evolution of Freqtric Drums.

</abstract>
    <keywords> interpersonal communication, musical instrument, interac- tion design, skin contact, touch  </keywords>
  </document>
  <document>
    <name>nime2007_390.pdf</name>
    <abstract> 
In this paper we describe a system which allows users to use their 
full-body for controlling in real-time the generation of an 
expressive audio-visual feedback. The system extracts expressive 
motion features from the user's full-body movements and 
gestures. The values of these motion features are mapped both 
onto acoustic parameters for the real-time expressive rendering of 
a piece of music, and onto real-time generated visual feedback 
projected on a screen in front of the user.  

</abstract>
    <keywords>  Expressive interaction; multimodal environments; interactive  music systems   </keywords>
  </document>
  <document>
    <name>nime2007_394.pdf</name>
    <abstract>
This paper reports our experiments on using a dual-core
DSP processor in the construction of a user-programmable
musical instrument and controller called the TouchBox.

</abstract>
    <keywords> dual-core, DSP, touch-screen, synthesizer, controller  </keywords>
  </document>
  <document>
    <name>nime2007_396.pdf</name>
    <keywords> Turntable, D.J.  ,Phenakistoscope,  multimedia performance  </keywords>
  </document>
  <document>
    <name>nime2007_401.pdf</name>
    <abstract>
Eowave and Ircam have been deeply involved into gesture
analysis and sensing for a few years by now, as several
artistic projects demonstrate (1). In 2004, Eowave has been
working with Ircam on the development of the Eobody
sensor system, and since that, Eowave's range of sensors has
been increased with new sensors sometimes developed in
narrow collaboration with artists for custom sensor systems
for installations and performances. This demo-paper
describes the recent design of a new USB/MIDI-to-sensor
interface called Eobody2.

</abstract>
    <keywords> Gestural controller, Sensor, MIDI, USB, Computer music, Relays, Motors, Robots, Wireless.  </keywords>
  </document>
  <document>
    <name>nime2007_403.pdf</name>
    <abstract> 
The WISP is a novel wireless sensor that uses 3 axis 
magnetometers, accelerometers, and rate gyroscopes to provide a 
real-time measurement of its own orientation in space. Orientation 
data are transmitted via the Open Sound Control protocol (OSC) 
to a synthesis engine for interactive live dance performance. 

</abstract>
    <keywords>  Music Controller, Human-Computer Interaction, Wireless  Sensing, Inertial Sensing.   </keywords>
  </document>
  <document>
    <name>nime2007_407.pdf</name>
    <abstract>
This paper introduces a system for improvisational musical
expression that enables all users, novice and experienced, to
perform intuitively and expressively. Users can generate mu-
sically consistent results through intuitive action, inputting
rhythm in a decent tempo. We demonstrate novel mapping
ways that reect user's input information more interactively
and eectively in generating the music. We also present var-
ious input devices that allow users more creative liberty.

</abstract>
    <keywords> Improvisation, interactive music, a sense of tempo  </keywords>
  </document>
  <document>
    <name>nime2007_409.pdf</name>
    <abstract> 
We proposed a circle canon system for enjoying a musical 
ensemble supported by a computer and network. Using the song 
"Frog round", which is a popular circle canon chorus originated 
from a German folk song, we produced a singing ensemble 
opportunity where everyone plays the music together at the same 
time. The aim of our system is that anyone can experience the 
joyful feeling of actually playing the music as well as sharing it 
with others. 

</abstract>
    <keywords>  Circle canon, Chorus, Song, Frog round, Ensemble, Internet,  Max/MSP, MySQL database.   </keywords>
  </document>
  <document>
    <name>nime2007_411.pdf</name>
    <abstract>
Loop-R  is  a  real-time  video  performance  tool,  based  in  the 
exploration  of  low-tech,  used  technology  and  human 
engineering research.  With this tool its author is giving a shout 
to  industry,  using  existing  and  mistreated  technology  in 
innovative  ways,  combining  concepts  and  interfaces:  blending 
segregated  interfaces  (GUI  and  Physical)  into  one.  After 
graspable  interfaces  and  the  "end"  of  WIMP  interfaces, 
hardware  and  software  blend  themselves  in  a  new  genre 
providing  free  control  of  video-loops  in  an  expressive  hybrid 
tool. 

</abstract>
    <keywords> Real-time; video; interface; live-visuals; loop;   </keywords>
  </document>
  <document>
    <name>nime2007_415.pdf</name>
    <abstract>
The Music Cre8tor is an interactive music composition system
controlled by motion sensors specifically designed for
children with disabilities although not exclusively for this
population.  The player(s) of the Music Cre8tor can either hold
or attach accelerometer sensors to trigger a variety of
computer-generated sounds, MIDI instruments and/or pre-
recorded sound files.  The sensitivity of the sensors can be
modified for each unique individual so that even the smallest
movement can control a sound.  The flexibility of the system
is such that either four people can play simultaneously and/or
one or more players can use up to four sensors.  The original
goal of this program was to empower students with disabilities
to create music and encourage them to perform with other
musicians, however this same goal has expanded to include
other populations.

</abstract>
    <keywords> Music Education, disabilities, special education, motion sensors, music composition, interactive performance.  </keywords>
  </document>
  <document>
    <name>nime2007_417.pdf</name>
    <abstract>
In this demonstration, I exemplify how a musical channel of
communication can be established in computer-mediated
interaction between musicians and dancers in real time. This
channel of communication uses a software library
implemented as a library of external objects for Max/MSP[1],
that processes data from an object or library that performs
frame-differencing analysis of a video stream in real time in
this programming environment.

</abstract>
  </document>
  <document>
    <name>nime2008_003.pdf</name>
    <abstract>
The rapid development of network communication
technologies has allowed composers to create new ways in
which to directly engage participants in the exploration of new
musical environments. A number of distinctive aesthetic
approaches to the musical application of networks will be
outlined in this paper each of which is mediated and
conditioned by the technical and aesthetic foundations of the
network technologies themselves. Recent work in the field by
artists such as Atau Tanaka and Metraform will be examined, as
will some of the earlier pioneering work in the genre by Max
Neuhaus. While recognizing the historical context of
collaborative work, the author will examine how the strategies
employed in the work of these artists have helped redefine a
new aesthetics of engagement in which play, spatial and
temporal dislocation are amongst the genre's defining
characteristics.

</abstract>
    <keywords> Networks, collaborative, open-form, play, interface.  </keywords>
  </document>
  <document>
    <name>nime2008_009.pdf</name>
    <abstract>
This paper presents the latest developments of the Public Sound 
Objects (PSOs) system, an experimental framework to implement 
and test new concepts for Networked Music. The project of a 
Public interactive installation using the PSOs system was 
commissioned in 2007 by Casa da Musica, the main concert hall 
space in Porto. It resulted in a distributed musical structure with 
up to ten interactive performance terminals distributed along the 
Casa da Musica's hallways, collectively controlling a shared 
acoustic piano. The installation allows the visitors to collaborate 
remotely with each other, within the building, using a software 
interface custom developed to facilitate collaborative music 
practices and with no requirements in terms previous knowledge 
of musical performance. 

</abstract>
  </document>
  <document>
    <name>nime2008_013.pdf</name>
    <abstract>
New application spaces and artistic forms can emerge when
users are freed from constraints. In the general case of
human-computer interfaces, users are often confined to a
fixed location, severely limiting mobility. To overcome this
constraint in the context of musical interaction, we present
a system to manage large-scale collaborative mobile audio
environments, driven by user movement. Multiple partici-
pants navigate through physical space while sharing over-
laid virtual elements. Each user is equipped with a mobile
computing device, GPS receiver, orientation sensor, micro-
phone, headphones, or various combinations of these tech-
nologies. We investigate methods of location tracking, wire-
less audio streaming, and state management between mobile
devices and centralized servers. The result is a system that
allows mobile users, with subjective 3-D audio rendering,
to share virtual scenes. The audio elements of these scenes
can be organized into large-scale spatial audio interfaces,
thus allowing for immersive mobile performance, locative
audio installations, and many new forms of collaborative
sonic activity.

</abstract>
    <keywords> sonic navigation, mobile music, spatial interaction, wireless audio streaming, locative media, collaborative interfaces  </keywords>
  </document>
  <document>
    <name>nime2008_019.pdf</name>
    <abstract>
Open Sound Control (OSC) is being used successfully as a
messaging protocol among many computers, gestural
controllers and multimedia systems. Although OSC has
addressed some of the shortcomings of MIDI, OSC cannot
deliver on its promises as a real-time communication protocol
for constrained embedded systems. This paper will examine
some of the advantages but also dispel some of the myths
concerning OSC. The paper will also describe how some of the
best features of OSC can be used to develop a lightweight
protocol that is microcontroller friendly.

</abstract>
  </document>
  <document>
    <name>nime2008_024.pdf</name>
    <abstract>
The continuous evolutions in the human-computer inter-
faces field have allowed the development of control devices
that let have a more and more intuitive, gestural and non-
invasive interaction.

Such devices find a natural employment also in the music
applied informatics and in particular in the electronic music,
always searching for new expressive means.

This paper presents a prototype of a system for the real-
time control of sound spatialization in a multichannel con-
figuration with a multimodal interaction interface. The spa-
tializer, called SMuSIM, employs interaction devices that
range from the simple and well-established mouse and key-
board to a classical gaming used joystick (gamepad), finally
exploiting more advanced and innovative typologies based
on image analysis (as a webcam).

</abstract>
    <keywords> Sound spatialization, multimodal interaction, interaction interfaces, EyesWeb, Pure data.  </keywords>
  </document>
  <document>
    <name>nime2008_028.pdf</name>
    <abstract>
Over the last century, composers have made increasingly 
ambitious experiments with musical time, but have been 
impeded in expressing more temporally-complex musical 
processes by the limitations of both music notations and human 
performers. In this paper, we describe a computer-based 
notation and gestural control system for independently 
manipulating the tempi of musical parts within a piece, at 
performance time. We describe how the problem was 
approached, drawing upon feedback and suggestions from 
consultations across multiple disciplines, seeking analogous 
problems in other fields. Throughout, our approach is guided 
and, ultimately, assessed by an established professional 
composer, who was able to interact with a working prototype of 
the system. 

</abstract>
  </document>
  <document>
    <name>nime2008_034.pdf</name>
    <keywords> synthesis control, expressive timing, playing styles  </keywords>
  </document>
  <document>
    <name>nime2008_038.pdf</name>
    <abstract>
A new interface for visualizing and analyzing percussion ges-
tures is presented, proposing enhancements of existing mo-
tion capture analysis tools. This is achieved by offering a
percussion gesture analysis protocol using motion capture.
A virtual character dynamic model is then designed in or-
der to take advantage of gesture characteristics, yielding to
improve gesture analysis with visualization and interaction
cues of different types.

</abstract>
    <keywords> Gesture and sound, interface, percussion gesture, virtual character, interaction.  </keywords>
  </document>
  <document>
    <name>nime2008_044.pdf</name>
    <keywords> bowing, gesture, playing technique, principal component anal- ysis, classification  </keywords>
  </document>
  <document>
    <name>nime2008_049.pdf</name>
    <abstract>
This article discusses a virtual slide guitar instrument, re-
cently introduced in [7]. The instrument consists of a novel
physics-based synthesis model and a gestural user interface.
The synthesis engine uses energy-compensated time-varying
digital waveguides. The string algorithm also contains a
parametric model for synthesizing the tube-string contact
sounds. The real-time virtual slide guitar user interface em-
ploys optical gesture recognition, so that the user can play
this virtual instrument simply by making slide guitar play-
ing gestures in front of a camera.

</abstract>
    <keywords> Sound synthesis, slide guitar, gesture control, physical mod- eling  </keywords>
  </document>
  <document>
    <name>nime2008_053.pdf</name>
    <keywords> Augmented instrument, electric guitar, gesture-sound relationship  </keywords>
  </document>
  <document>
    <name>nime2008_057.pdf</name>
    <abstract>
This paper describes the Sormina, a new virtual and tangible
instrument, which has its origins in both virtual technology and
the heritage of traditional instrument design. The motivation
behind the project is presented, as well as hardware and
software design. Insights gained through collaboration with
acoustic musicians are presented, as well as comparison to
historical instrument design.

</abstract>
    <keywords> Gestural controller, digital musical instrument, usability, music history, design.  </keywords>
  </document>
  <document>
    <name>nime2008_061.pdf</name>
    <abstract>
The music community has long had a strong interest in hap-
tic technology. Recently, more effort has been put into mak-
ing it more and more accessible to instrument designers.
This paper covers some of these technologies with the aim
of helping instrument designers add haptic feedback to their
instruments. We begin by giving a brief overview of practical
actuators. Next, we compare and contrast using embedded
microcontrollers versus general purpose computers as con-
trollers. Along the way, we mention some common software
environments for implementing control algorithms. Then we
discuss the fundamental haptic control algorithms as well as
some more complex ones. Finally, we present two practical
and effective haptic musical instruments: the haptic drum
and the Cellomobo.

</abstract>
    <keywords> haptic, actuator, practical, immersion, embedded, sampling rate, woofer, haptic drum, Cellomobo  </keywords>
  </document>
  <document>
    <name>nime2008_071.pdf</name>
    <abstract>
Phya is an open source C++ library originally designed for
adding physically modeled contact sounds into computer
game environments equipped with physics engines. We re-
view some aspects of this system, and also consider it from
the purely aesthetic perspective of musical expression.

</abstract>
    <keywords> NIME, musical expression, virtual reality, physical model- ing, audio synthesis  </keywords>
  </document>
  <document>
    <name>nime2008_077.pdf</name>
    <abstract>
In this paper I discuss the importance of and need for
pedagogical materials to support the development of new
interfaces and new instruments for electronic music. I describe
my method for creating a graduated series of pedagogical
etudes composed using Max/MSP. The etudes will help
performers and instrument designers learn the most commonly
used basic skills necessary to perform with interactive
electronic music instruments. My intention is that the final
series will guide a beginner from these initial steps through a
graduated method, eventually incorporating some of the more
advanced techniques regularly used by electronic music
composers.

I describe the order of the series, and discuss the benefits (both
to performers and to composers) of having a logical sequence of
skill-based etudes. I also connect the significance of skilled
performers to the development of two essential areas that I
perceive are still just emerging in this field: the creation of a
composed repertoire and an increase in musical expression
during performance.

</abstract>
  </document>
  <document>
    <name>nime2008_081.pdf</name>
    <abstract>
The expressive and creative affordances of an interface are
difficult to evaluate, particularly with quantitative methods.
However, rigorous qualitative methods do exist and can be
used to investigate such topics. We present a methodology
based around user studies involving Discourse Analysis of
speech. We also present an example of the methodology
in use: we evaluate a musical interface which utilises vocal
timbre, with a user group of beatboxers.

</abstract>
  </document>
  <document>
    <name>nime2008_087.pdf</name>
    <abstract>
There is small but useful body of research concerning the
evaluation of musical interfaces with HCI techniques. In
this paper, we present a case study in implementing these
techniques; we describe a usability experiment which eval-
uated the Nintendo Wiimote as a musical controller, and
reflect on the effectiveness of our choice of HCI methodolo-
gies in this context. The study offered some valuable results,
but our picture of the Wiimote was incomplete as we lacked
data concerning the participants' instantaneous musical ex-
perience. Recent trends in HCI are leading researchers to
tackle this problem of evaluating user experience; we review
some of their work and suggest that with some adaptation it
could provide useful new tools and methodologies for com-
puter musicians.

</abstract>
    <keywords> HCI Methodology, Wiimote, Evaluating Musical Interac- tion  </keywords>
  </document>
  <document>
    <name>nime2008_091.pdf</name>
    <abstract>
We combine two concepts, the musical instrument as metaphor
and technology probes, to explore how tangible interfaces can
exploit the semantic richness of sound. Using participatory
design methods from Human-Computer Interaction (HCI), we
designed and tested the A20, a polyhedron-shaped, multi-
channel audio input/output device. The software maps sound
around the edges and responds to the user's gestural input,
allowing both aural and haptic modes of interaction as well as
direct manipulation of media content. The software is designed
to be very flexible and can be adapted to a wide range of
shapes. Our tests of the A20's perceptual and interaction
properties showed that users can successfully detect sound
placement, movement and haptic effects on this device. Our
participatory design workshops explored the possibilities of the
A20 as a generative tool for the design of an extended,
collaborative personal music player. The A20 helped users to
enact scenarios of everyday mobile music player use and to
generate new design ideas.

</abstract>
    <keywords>  Generative design tools, Instrument building, Multi-faceted audio, Personal music devices, Tangible user interfaces, Technology probes  </keywords>
  </document>
  <document>
    <name>nime2008_097.pdf</name>
    <abstract>
The described project is a new approach to use highly sensitive 
low force pressure sensor matrices for malposition, cramping and 
tension of hands and fingers, gesture and keystroke analysis and 
for new musical expression. In the latter, sensors are used as 
additional touch sensitive switches and keys. In pedagogical 
issues, new ways of technology enhanced teaching, self teaching 
and exercising are described. The used sensors are custom made 
in collaboration with the ReactiveS Sensorlab. 

</abstract>
    <keywords> Pressure Measurement, Force, Sensor, Finger, Violin, Strings,  Piano, Left Hand, Right Hand, Time Line, Cramping, Gesture and  Posture Analysis.   </keywords>
  </document>
  <document>
    <name>nime2008_103.pdf</name>
    <abstract>
In this paper, we describe an algorithm for the numerical
evaluation of the orientation of an object to which a cluster
of accelerometers, gyroscopes and magnetometers has been
attached. The algorithm is implemented through a set of
Max/Msp and pd new externals. Through the successful
implementation of the algorithm, we introduce Pointing-
at, a new gesture device for the control of sound in a 3D
environment. This work has been at the core of the Celeri-
tas Project, an interdisciplinary research project on motion
tracking technology and multimedia live performances be-
tween the Tyndall Institute of Cork and the Interaction
Design Centre of Limerick.

</abstract>
  </document>
  <document>
    <name>nime2008_107.pdf</name>
    <abstract>

The paper introduces new fiber and malleable materials,
including piezoresistive fabric and conductive heat-shrink
tubing, and shows techniques and examples of how they may
be used for rapid prototyping and agile development of musical
instrument controllers. New implementations of well-known
designs are covered as well as enhancements of existing
controllers. Finally, two new controllers are introduced that are
made possible by these recently available materials and
construction techniques.

</abstract>
    <keywords> Agile Development, Rapid Prototyping, Conductive fabric, Piezoresistive fabric, conductive heatshrink tubing, augmented instruments.  </keywords>
  </document>
  <document>
    <name>nime2008_113.pdf</name>
    <abstract>
In this paper, we describe a set of hardware and software tools for 
creating musical controllers with any flat surface or simple object, 
such as tables, walls, metallic plates, wood boards, etc. The 
system makes possible to transform such physical objects and 
surfaces into virtual control interfaces, by using computer vision 
technologies to track the interaction made by the musician, either 
with the hands, mallets or sticks. These new musical interfaces, 
freely reconfigurable, can be used to control standard sound 
modules or effect processors, by defining zones on their surface 
and assigning them musical commands, such as the triggering of 
notes or the modulation of parameters.

</abstract>
    <keywords> Computer Vision, Multi-touch Interaction, Musical Interfaces.   </keywords>
  </document>
  <document>
    <name>nime2008_117.pdf</name>
    <abstract>
This paper presents a comparison of the movement styles of two 
theremin players based on observation and analysis of video 
recordings. The premise behind this research is that a 
consideration of musicians' movements could form the basis for a 
new framework for the design of new instruments. Laban 
Movement Analysis is used to qualitatively analyse the movement 
styles of the musicians and to argue that the Recuperation phase 
of their phrasing is essential to achieve satisfactory performance. 

</abstract>
    <keywords>  Effort Phrasing, Recuperation, Laban Movement Analysis,  Theremin  </keywords>
  </document>
  <document>
    <name>nime2008_122.pdf</name>
    <keywords> affective computing, interactive performance, HMM, gesture recognition, intelligent mapping, affective interface  </keywords>
  </document>
  <document>
    <name>nime2008_128.pdf</name>
    <abstract>
It started with an idea to create an empty space in which you 
activated music and light as you moved around. In responding to 
the music and lighting you would activate more or different 
sounds and thereby communicate with the space through your 
body. This led to an artistic research project in which children's 
spontaneous movement was observed, a choreography made 
based on the children's movements and music written and 
recorded for the choreography. This music was then decomposed 
and choreographed into an empty space at Botkyrka konsthall 
creating an interactive dance installation. It was realized using an 
interactive sound and light system in which 5 video cameras were 
detecting the motion in the room connected to a 4-channel sound 
system and a set of 14 light modules.  During five weeks people 
of all ages came to dance and move around in the installation. The 
installation attracted a wide range of people of all ages and the 
tentative evaluation indicates that it was very positively received 
and that it encouraged free movement in the intended way. 
Besides observing the activity in the installation interviews were 
made with schoolchildren age 7 who had participated in the 
installation.  

</abstract>
    <keywords> Installation, dance, video recognition, children's movement,  interactive multimedia   </keywords>
  </document>
  <document>
    <name>nime2008_134.pdf</name>
    <keywords>  Active listening of music, expressive interfaces, full-body  motion analysis and expressive gesture processing, multimodal  interactive systems for music and performing arts applications,  collaborative environments, social interaction.   </keywords>
  </document>
  <document>
    <name>nime2008_140.pdf</name>
    <abstract> 
Musical open works can be often thought like sequences of 

musical structures, which can be arranged by anyone who 

had access to them and who wished to realize the work. 

This paper proposes an innovative agent-based system to 

model the information and organize it in structured 

knowledge; to create effective, graph-centric browsing 

perspectives and views for the user; to use authoring tools 

for the performance of open work of electro-acoustic 

music. 

</abstract>
    <keywords>  Musical Open Work, Multimedia Information Systems,   Software Agents, zz-structures.   </keywords>
  </document>
  <document>
    <name>nime2008_144.pdf</name>
    <abstract>
This paper presents an agent-based architecture for robotic 
musical instruments that generate polyphonic rhythmic patterns 
that continuously evolve and develop in a musically 
"intelligent" manner. Agent-based software offers a new 
method for real-time composition that allows for complex 
interactions between individual voices while requiring very 
little user interaction or supervision. The system described, 
Kinetic Engine, is an environment in which individual software 
agents, emulate drummers improvising within a percussion 
ensemble. Player agents assume roles and personalities within 
the ensemble, and communicate with one another to create 
complex rhythmic interactions. In this project, the ensemble is 
comprised of a 12-armed musical robot, MahaDeviBot, in 
which each limb has its own software agent controlling what it 
performs.  

</abstract>
    <keywords> Robotic Musical Instruments, Agents, Machine Musicianship.  </keywords>
  </document>
  <document>
    <name>nime2008_150.pdf</name>
    <abstract>
In this paper, we investigate the relationships between gesture and 
sound by means of an elementary gesture sonification. This work 
takes inspiration from Bauhaus' ideals and Paul Klee's 
investigation into forms and pictorial representation.  In line with 
these ideas, the main aim of this work is to reduce gesture to a 
combination of a small number of elementary components 
(gestalts) used to control a corresponding small set of sounds. By 
means of a demonstrative tool, we introduce here a line of 
research that is at its initial stage. The envisaged goal of future 
developments is a novel system that could be a 
composing/improvising tool as well as an interface for interactive 
dance and performance.  

</abstract>
    <keywords> Bauhaus, Klee, gesture analysis, sonification.    </keywords>
  </document>
  <document>
    <name>nime2008_154.pdf</name>
    <abstract>
We present our work with augmented everyday objects
transformed into sound sources for music generation. The idea is
to give voice to objects through technology. More specifically, the
paradigm of the birth of musical instruments as a sonification of
objects used in domestic or work everyday environments is here
considered and transposed into the technologically augmented
scenarios of our contemporary world.

</abstract>
    <keywords> Rag-time washboard, sounding objects, physics-based sound synthesis, interactivity, sonification, augmented everyday objects.  </keywords>
  </document>
  <document>
    <name>nime2008_158.pdf</name>
    <abstract>
This paper describes a generalized motion-based framework for
the generation of large musical control fields from imaging data.
The framework is general in the sense that it does not depend on
a particular source of sensing data. Real-time images of stage
performers, pre-recorded and live video, as well as more exotic
data from imaging systems such as thermography, pressure
sensor arrays, etc. can be used as a source of control. Feature
points are extracted from the candidate images, from which
motion vector fields are calculated. After some processing, these
motion vectors are mapped individually to sound synthesis
parameters. Suitable synthesis techniques include granular and
microsonic algorithms, additive synthesis and micro-polyphonic
orchestration. Implementation details of this framework is
discussed, as well as suitable creative and artistic uses and
approaches.

</abstract>
    <keywords> Computer vision, control field, image analysis, imaging, mapping, microsound, motion flow, sonification, synthesis  </keywords>
  </document>
  <document>
    <name>nime2008_164.pdf</name>
    <keywords> Poetry, language sonification, psychoanalysis, linguistics, Freud,  realtime poetry.   </keywords>
  </document>
  <document>
    <name>nime2008_168.pdf</name>
    <abstract>
Moving out of doors with digital tools and electronic music and 
creating musically rich experiences is made possible by the 
increased availability of ever smaller and more powerful mobile 
computers. Composing music for and in a landscape instead of for 
a closed architectural space offers new perspectives but also raises 
questions about interaction and composition of electronic music. 
The work we present here was commissioned by a festival and ran 
on a daily basis over a period of three months. A GPS-enabled 
embedded Linux system is assembled to serve as a location-aware 
sound platform. Several challenges have to be overcome both 
technically and artistically to achieve a seamless experience and 
provide a simple device to be handed to the public. By building 
this interactive experience, which relies as much on the user's 
willingness to explore the invisible sonic landscape as on the 
ability to deploy the technology, a number of new avenues for 
exploring electronic music and interactivity in location-based 
media open up. New ways of composing music for and in a 
landscape and for creating audience interaction are explored. 

</abstract>
    <keywords> Location-based, electronic music, composition, embedded Linux,  GPS, Pure Data, interaction, mapping, soundscape  </keywords>
  </document>
  <document>
    <name>nime2008_175.pdf</name>
    <abstract>
A general-purpose firmware for a low cost microcontroller is 
described that employs the Open Sound Control protocol over 
USB. The firmware is designed with considerations for 
integration in new musical interfaces and embedded devices. 
Features of note include stateless design, efficient floating-point 
support, temporally correct data handling, and protocol 
completeness. A timing performance analysis is conducted.

</abstract>
  </document>
  <document>
    <name>nime2008_181.pdf</name>
    <abstract>
An approach for creating structured Open Sound Control
(OSC) messages by separating the addressing of node values
and node properties is suggested. This includes a method
for querying values and properties. As a result, it is possible
to address complex nodes as classes inside of more complex
tree structures using an OSC namespace. This is particu-
larly useful for creating flexible communication in modular
systems. A prototype implementation is presented and dis-
cussed.

</abstract>
  </document>
  <document>
    <name>nime2008_185.pdf</name>
    <abstract>
Many mobile devices, specifically mobile phones, come equip-
ped with a microphone. Microphones are high-fidelity sen-
sors that can pick up sounds relating to a range of physi-
cal phenomena. Using simple feature extraction methods,
parameters can be found that sensibly map to synthesis al-
gorithms to allow expressive and interactive performance.
For example blowing noise can be used as a wind instru-
ment excitation source. Also other types of interactions
can be detected via microphones, such as striking. Hence
the microphone, in addition to allowing literal recording,
serves as an additional source of input to the developing
field of mobile phone performance.

</abstract>
    <keywords> mobile music making, microphone, mobile-stk  </keywords>
  </document>
  <document>
    <name>nime2008_193.pdf</name>
    <abstract>

</abstract>
    <keywords>  </keywords>
  </document>
  <document>
    <name>nime2008_197.pdf</name>
  </document>
  <document>
    <name>nime2008_203.pdf</name>
  </document>
  <document>
    <name>nime2008_207.pdf</name>
    <abstract>
This paper describes a project started for implementing DJ
scratching techniques on the reactable. By interacting with
objects representing scratch patterns commonly performed
on the turntable and the crossfader, the musician can play
with DJ techniques and manipulate how they are executed
in a performance. This is a novel approach to the digital DJ
applications and hardware. Two expert musicians practised
and performed on the reactable in order to both evaluate the
playability and improve the design of the DJ techniques.

</abstract>
  </document>
  <document>
    <name>nime2008_211.pdf</name>
    <abstract>
This paper reports on a Short-Term Scientific Mission (STSM)
sponsored by the Sonic Interaction Design (SID) European
COST Action IC601.

Prototypes of objects for the novel instrument Reactable
were developed, with the goal of studying sonification of
movements on this platform using physical models. A phys-
ical model of frictional interactions between rubbed dry sur-
faces was used as an audio generation engine, which allowed
development in two directions - a set of objects that affords
motions similar to sliding, and a single object aiming to
sonify contact friction sound. Informal evaluation was ob-
tained from a Reactable expert user, regarding these sets of
objects. Experiments with the objects were also performed
- related to both audio filtering, and interfacing with other
objects for the Reactable.

</abstract>
    <keywords> Reactable, physical model, motion sonification, contact fric- tion  </keywords>
  </document>
  <document>
    <name>nime2008_215.pdf</name>
    <abstract>
This research focuses on real-time gesture learning and recog-
nition. Events arrive in a continuous stream without ex-
plicitly given boundaries. To obtain temporal accuracy, we
need to consider the lag between the detection of an event
and any effects we wish to trigger with it. Two methods
for real time gesture recognition using a Nintendo Wii con-
troller are presented. The first detects gestures similar to a
given template using either a Euclidean distance or a cosine
similarity measure. The second method uses novel informa-
tion theoretic methods to detect and categorize gestures in
an unsupervised way. The role of supervision, detection lag
and the importance of haptic feedback are discussed.

</abstract>
    <keywords> Gesture recognition, supervised and unsupervised learning, interaction, haptic feedback, information dynamics, HMMs  </keywords>
  </document>
  <document>
    <name>nime2008_219.pdf</name>
    <abstract>
This paper describes the compositional process for creating
the interactive work for violin entitled VITESSIMO using the
Augmented Violin [1].

</abstract>
    <keywords> Augmented Violin, gesture tracking, interactive performance  </keywords>
  </document>
  <document>
    <name>nime2008_221.pdf</name>
    <abstract>
Sound libraries for music synthesizers easily comprise one
thousand or more programs ("patches"). Thus, there are
enough raw data to apply data mining to reveal typical
settings and to extract dependencies. Intelligent user inter-
faces for music synthesizers can be based on such statistics.
This paper proposes two approaches: First, the user sets
any number of parameters and then lets the system find the
nearest sounds in the database, a kind of patch autocomple-
tion. Second, all parameters are "live" as usual, but turning
one knob or setting a switch will also change the settings
of other, statistically related controls. Both approaches can
be used with the standard interface of the synthesizer. On
top of that, this paper introduces alternative or additional
interfaces based on data visualization.

</abstract>
    <keywords> Information visualization, mutual information, intelligent user interfaces  </keywords>
  </document>
  <document>
    <name>nime2008_225.pdf</name>
    <abstract>
This paper presents a project called i-Maestro  
(www.i-maestro.org) which develops interactive multimedia 
environments for technology enhanced music education. The 
project explores novel solutions for music training in both 
theory and performance, building on recent innovations 
resulting from the development of computer and information 
technologies, by exploiting new pedagogical paradigms with 
cooperative and interactive self-learning environments, gesture 
interfaces, and augmented instruments. This paper discusses the 
general context along with the background and current 
developments of the project, together with an overview of the 
framework and discussions on a number of selected tools to 
support technology-enhanced music learning and teaching.   

</abstract>
  </document>
  <document>
    <name>nime2008_229.pdf</name>
    <abstract>
This paper describes the HOP system. It consists of a wireless 
module built up by multiple nodes and a base station. The nodes 
detect acceleration of e.g. human movement. At a rate of 100 
Hertz the base station collects the acceleration samples. The data 
can be acquired in real-time software like Pure Data and 
Max/MSP. The data can be used to analyze and/or sonify 
movement. 

</abstract>
    <keywords> Digital Musical Instrument, Wireless Sensors, Inertial Sensing,  Hop Sensor   </keywords>
  </document>
  <document>
    <name>nime2008_233.pdf</name>
    <keywords>  Ubiquitous computing, context -awareness, networking,  embedded systems, chairs, digital artefacts, emotional state  sensing, affective computing, biosignals.  </keywords>
  </document>
  <document>
    <name>nime2008_237.pdf</name>
    <abstract>
We present examples of a wireless sensor network as applied 
to wearable digital music controllers. Recent advances in 
wireless Personal Area Networks (PANs) have precipitated the 
IEEE 802.15.4 standard for low-power, low-cost wireless 
sensor networks. We have applied this new technology to 
create a fully wireless, wearable network of accelerometers 
which are small enough to be hidden under clothing. Various 
motion analysis and machine learning techniques are applied 
to the raw accelerometer data in real-time to generate and 
control music on the fly. 

</abstract>
    <keywords> Wearable computing, personal area networks, accelerometers,  802.15.4, motion analysis, human-computer interaction, live  performance, digital musical controllers, gestural control   </keywords>
  </document>
  <document>
    <name>nime2008_241.pdf</name>
    <abstract>
This research aims to develop a wearable musical interface
which enables to control audio and video signals by using
hand gestures and human body motions. We have been
developing an audio-visual manipulation system that real-
izes tracks control, time-based operations and searching for
tracks from massive music library. It aims to build an emo-
tional and affecting musical interaction, and will provide
a better method of music listening to people. A sophisti-
cated glove-like device with an acceleration sensor and sev-
eral strain sensors has been developed. A realtime signal
processing and musical control are executed as a result of
gesture recognition. We also developed a stand-alone de-
vice that performs as a musical controller and player at the
same time. In this paper, we describe the development of a
compact and sophisticated sensor device, and demonstrate
its performance of audio and video signals control.

</abstract>
    <keywords> Embodied Sound Media, Music Controller, Gestures, Body Motion, Musical Interface  </keywords>
  </document>
  <document>
    <name>nime2008_245.pdf</name>
    <abstract>
This paper proposes the creation of a method book for tablet-
based instruments, evaluating pedagogical materials for
traditional instruments as well as research in human-computer
interaction and tablet interfaces.

</abstract>
    <keywords> Wacom tablet, digitizing tablet, expressivity, gesture, mapping, pedagogy, practice  </keywords>
  </document>
  <document>
    <name>nime2008_249.pdf</name>
    <abstract>
We present an audio waveform editor that can be oper-
ated in real time through a tabletop interface. The system
combines multi-touch and tangible interaction techniques in
order to implement the metaphor of a toolkit that allows di-
rect manipulation of a sound sample. The resulting instru-
ment is well suited for live performance based on evolving
loops.

</abstract>
    <keywords> tangible interface, tabletop interface, musical performance, interaction techniques  </keywords>
  </document>
  <document>
    <name>nime2008_253.pdf</name>
  </document>
  <document>
    <name>nime2008_257.pdf</name>
    <abstract>
This paper is about GeoGraphy, a graph-based system for
the control of both musical composition and interactive per-
formance and its implementation in a real-time, interactive
application. The implementation includes a flexible user
interface system.

</abstract>
  </document>
  <document>
    <name>nime2008_261.pdf</name>
    <abstract>
In this paper, we describe the development of multi-platform tools 
for Audiovisual and Kinetic installations. These involve the 
connection of three development environments: Python, 
SuperCollider and Processing, in order to drive kinetic art 
installations and to combine these with digital synthesis of sound 
and image in real time. By connecting these three platforms via 
the OSC protocol, we enable the control in real time of analog 
physical media (a device that  draws figures on sand), sound 
synthesis  and image synthesis. We worked on  the development of 
algorithms for drawing figures and synthesizing images and sound 
on  all three platforms and experimented with various  mechanisms 
for coordinating synthesis and rendering in different media. 
Several problems were addressed:  How to coordinate the timing 
between different platforms? What configuration to use?  Client-
server (who is the client  who the server?), equal  partners, mixed 
configurations. A library was developed in SuperCollider to 
enable the packaging of algorithms into modules with automatic 
generation of GUI from specifications, and the saving of 
configurations of modules into session files as scripts in 
SuperCollider code. The application of this library as a framework 
for both driving graphic synthesis  in Processing and receiving 
control  data from it  resulted in an environment for 
experimentation that is also being used successfully in teaching 
interactive audiovisual media. 

</abstract>
    <keywords> kinetic art, audiovisual installations, python, SuperCollider,  Processing, algorithmic art, tools for multi-platform development   </keywords>
  </document>
  <document>
    <name>nime2008_265.pdf</name>
    <abstract>
Through the developing of tools for analyzing the performers
sonic and movement-based gestures, research into the system-
performer interaction has focused on the computer's ability to
respond to the performer. Where as such work shows interest
within the community in developing an interaction paradigm
modeled on the player, by focusing on the perception and
reasoning of the system, this research assumes that the
performer's manner of interaction is in agreement with this
computational model. My study presents an alternative model of
interaction designed for improvisatory performance centered on
the perception of the performer as understood by theories taken
from performance practices and cognitive science.

</abstract>
    <keywords> Interactive performance, Perception, HCI  </keywords>
  </document>
  <document>
    <name>nime2008_277.pdf</name>
    <abstract>
One of the advantages of case-based systems is that they
can generate expressions even if the user doesn't know how
the system applies expression rules. However, the systems
cannot avoid the problem of data sparseness and do not
permit a user to improve the expression of a certain part of
a melody directly. After discussing the functions required
for user-oriented interface for performance rendering sys-
tems, this paper proposes a directable case-based perfor-
mance rendering system, called Itopul. Itopul is character-
ized by 1) a combination of the phrasing model and the
pulse model, 2) the use of a hierarchical music structure for
avoiding from the data sparseness problem, 3) visualization
of the processing progress, and 4) music structures directly
modifiable by the user.

</abstract>
    <keywords> Performance Rendering, User Interface, Case-based Approach  </keywords>
  </document>
  <document>
    <name>nime2008_281.pdf</name>
    <abstract>
In this work we describe our initial explorations in build-
ing a musical instrument specifically for providing listeners
with simple, but useful, ambient information. The term
Ambient Musical Information Systems (AMIS) is proposed
to describe this kind of research. Instruments like these dif-
fer from standard musical instruments in that they are to
be perceived indirectly from outside one's primary focus of
attention. We describe our rationale for creating such a de-
vice, a discussion on the appropriate qualities of sound for
delivering ambient information, and a description of an in-
strument created for use in a series of experiments that we
will use to test out ideas. We conclude with a discussion of
our initial findings, and some further directions we wish to
explore.

</abstract>
    <keywords> Ambient Musical Information Systems, musical instruments, human computer interaction, Markov chain, probability, al- gorithmic composition  </keywords>
  </document>
  <document>
    <name>nime2008_285.pdf</name>
    <abstract>
The Elbow Piano distinguishes two types of piano touch: a touch
with movement in the elbow joint and a touch without. A played
note is first mapped to the left or right hand by visual tracking.
Custom-built goniometers attached to the player's arms are used
to detect the type of touch. The two different types of touches
are sonified by different instrument sounds. This gives the
player an increased awareness of his elbow movements, which is
considered valuable for piano education. We have implemented
the system and evaluated it with a group of music students.

</abstract>
    <keywords> Piano, education, sonification, feedback, gesture.  </keywords>
  </document>
  <document>
    <name>nime2008_289.pdf</name>
    <abstract>
Musical keyboard instruments have a long history, which
resulted in many kinds of keyboards (claviers) today. Since
the hardware of conventional musical keyboards cannot be
changed, such as the number of keys, musicians have to
carry these large keyboards for playing music that requires
only a small diapason. To solve this problem, the goal of
our study is to construct UnitKeyboard, which has only 12
keys (7 white keys and 5 black keys) and connectors for
docking with other UnitKeyboards. We can build various
kinds of musical keyboard configurations by connecting one
UnitKeyboard to others, since they have automatic settings
for multiple keyboard instruments. We discuss the usability
of the UnitKeyboard from reviews by several amateur and
professional pianists who used the UnitKeyboard.

</abstract>
    <keywords> Portable keyboard instruments, block interface, Automatic settings  </keywords>
  </document>
  <document>
    <name>nime2008_293.pdf</name>
    <abstract>
After eight years of practice on the first hyper-flute proto-
type (a flute extended with sensors), this article presents
a retrospective of its instrumental practice and the new
developments planned from both technological and musi-
cal perspectives. Design, performance skills, and mapping
strategies are discussed, as well as interactive composition
and improvisation.

</abstract>
  </document>
  <document>
    <name>nime2008_299.pdf</name>
    <abstract>
We introduce physically motivated interfaces for playing vir-
tual musical instruments, and we suggest that they lie some-
where in between commonplace interfaces and haptic inter-
faces in terms of their complexity. Next, we review guitar-
like interfaces, and we design an interface to a virtual string.
The excitation signal and pitch are sensed separately using
two independent string segments. These parameters control
a two-axis digital waveguide virtual string, which models
vibrations in the horizontal and vertical transverse axes as
well as the coupling between them. Finally, we consider the
advantages of using a multi-axis pickup for measuring the
excitation signal.

</abstract>
    <keywords> physically motivated, physical, models, modeling, vibrating string, guitar, pitch detection, interface, excitation, coupled strings, haptic  </keywords>
  </document>
  <document>
    <name>nime2008_303.pdf</name>
    <abstract>
Being one of the earliest electronic instruments the basic 
principles of the Theremin have often been used to design new 
musical interfaces. We present the structured design and 
evaluation of a set of 3D interfaces for a virtual Theremin, the 
VRemin. The variants differ in the size of the interaction space, 
the interface complexity, and the applied IO devices. We 
conducted a formal evaluation based on the well-known 
AttrakDiff questionnaire for evaluating the hedonic and pragmatic 
quality of interactive products. The presented work is a first 
approach towards a participatory design process for musical 
interfaces that includes user evaluation at early design phases.  

</abstract>
  </document>
  <document>
    <name>nime2008_311.pdf</name>
    <keywords>  Mapping, database, audiovisual, radio, installation art.   </keywords>
  </document>
  <document>
    <name>nime2008_315.pdf</name>
    <abstract>
Monalisa is a software platform that enables to "see the sound, 
hear the image". It consists of three software: Monalisa 
Application, Monalisa-Audio Unit, and Monalisa-Image Unit, and 
an installation: Monalisa "shadow of the sound". In this paper, we 
describe the implementation of each software and installation with 
the explanation of the basic algorithms to treat the image data and 
the sound data transparently.

</abstract>
    <keywords> Sound and Image Processing Software, Plug-in, Installation  </keywords>
  </document>
  <document>
    <name>nime2008_319.pdf</name>
    <keywords>  Automatic Accompaniment, Beat Tracking, Human-Computer Interaction, Musical Interface Evaluation  </keywords>
  </document>
  <document>
    <name>nime2008_325.pdf</name>
    <abstract>
In this paper, we present a pitch space based musical interface 
approach. A pitch space arranges tones in a way that meaningful 
tone combinations can be easily generated. Using a touch 
sensitive surface or a 3D-Joystick a player can move through the 
pitch space and create the desired sound by selecting tones. The 
more optimal the tones are geometrically arranged, the less 
control parameters are required to move through the space and to 
select the desired pitches. For this the quality of pitch space based 
musical interfaces depends on two factors: 1. the way how the 
tones are organized within the pitch space and 2. the way how the 
parameters of a given controller are used to move through the 
space and to select pitches. This paper presents a musical 
interface based on a tonal pitch space derived from a four 

dimensional model found by the music psychologists [11], [2]. 
The proposed pitch space particularly eases the creation of tonal 
harmonic music. Simultaneously it outlines music psychological 
and theoretical principles of music.  

</abstract>
    <keywords> Pitch space, musical interface, Carol L. Krumhansl, music  psychology, music theory, western tonal music, 3D tonality  model, spiral of thirds, 3D, Hardware controller, Symmetry model   </keywords>
  </document>
  <document>
    <name>nime2008_331.pdf</name>
    <abstract>
We describe a system that can listen to a performance of In-
dian music and recognize the raag, the fundamental melodic
framework that Indian classical musicians improvise within.
In addition to determining the most likely raag being per-
formed, the system displays the estimated the likelihood
of each of the other possible raags, visualizing the changes
over time. The system computes the pitch-class distribution
and uses a Bayesian decision rule to classify the resulting
twelve dimensional feature vector, where each feature repre-
sents the relative use of each pitch class. We show that the
system achieves high performance on a variety of sources,
making it a viable tool for interactive performance.

</abstract>
  </document>
  <document>
    <name>nime2008_335.pdf</name>
    <abstract>
A general CAC1-environment charged with physical-model-
ling capabilities is described. It combines CommonMusic,
ODE and Fluxus in a modular way, making a powerful and
flexible environment for experimenting with physical models
in composition.

Composition in this respect refers to the generation and
manipulation of structure typically on or above a note-
, phrase or voice-level. Compared to efforts in synthesis
and performance little work has gone into applying physical
models to composition. Potentials in composition-applica-
tions are presumably large.

The implementation of the physically equipped CAC-en-
vironment is described in detail.

</abstract>
    <keywords> Physical Models in composition, CommonMusic, Musical mapping  </keywords>
  </document>
  <document>
    <name>nime2008_339.pdf</name>
    <abstract>
The Color of Waiting is an interactive theater work
with music, dance, and video which was developed at
STEIM in Amsterdam and further refined at CMMAS
in Morelia Mexico with funding from Meet the
Composer. Using Max/MSP/ Jitter a cellist is able to
control sound and video during the performance
while performing a structured improvisation in
response to the dancer's movement. In order to
ensure. repeated performances of The Color o f
Waiting , Kinesthetech Sense created the score
contained in this paper. Performance is essential to
the practice of time-based art as a living form, but
has been complicated by the unique challenges in
interpretation and re-creation posed by works
incorporating technology. Creating a detailed score
is one of the ways artists working with technology
can combat obsolescence.

</abstract>
  </document>
  <document>
    <name>nime2008_345.pdf</name>
    <abstract>
We developed a rhythmic instruments ensemble simulator 
generating animation using game controllers. The motion of a 
player is transformed into musical expression data of MIDI to 
generate sounds, and MIDI data are transformed into animation 
control parameters to generate movies. These animations and 
music are shown as the reflection of player performance. Multiple 
players can perform a musical ensemble to make more varied 
patterns of animation. Our system is so easy that everyone can 
enjoy performing a fusion of music and animation. 

</abstract>
    <keywords> Wii Remote, Wireless game controller, MIDI, Max/MSP, Flash  movie, Gesture music and animation.   </keywords>
  </document>
  <document>
    <name>nime2008_347.pdf</name>
    <abstract>
The demonstration of a series of properly weighted and balanced 
Bluetooth sensor bows for violin, viola, cello and bass. 

</abstract>
    <keywords> Sensor bow, stringed instruments, bluetooth   </keywords>
  </document>
  <document>
    <name>nime2008_349.pdf</name>
    <abstract>
Plink Jet is a robotic musical instrument made from scavenged 
inkjet printers and guitar parts. We investigate the expressive 
capabilities of everyday machine technology by re-
contextualizing the relatively high-tech mechanisms of typical 
office debris into an electro-acoustic musical instrument. We also 
explore the performative relationship between human and 
machine.

</abstract>
    <keywords> Interaction Design, Repurposing of Consumer Technology, DIY,  Performing Technology, Robotics, Automation, Infra-Instrument   </keywords>
  </document>
  <document>
    <name>nime2008_352.pdf</name>
    <keywords> umbrella, musical expression, sound generating device, 3D sound  system, sound-field arrangement.  </keywords>
  </document>
  <document>
    <name>nime2008_360.pdf</name>
    <abstract>
This research aims to develop a novel instrument for socio-
musical interaction where a number of participants can pro-
duce sounds by feet in collaboration with each other. The
developed instrument, beacon, is regarded as embodied sound
media product that will provide an interactive environment
around it. The beacon produces laser beams lying on the
ground and rotating. Audio sounds are then produced when
the beams pass individual performer's foot. As the perform-
ers are able to control the pitch and sound length according
to the foot location and angles facing the instrument, the
performer's body motion and foot behavior can be trans-
lated into sound and music in an intuitive manner.

</abstract>
    <keywords> Embodied sound media, Hyper-instrument, Laser beams  </keywords>
  </document>
  <document>
    <name>nime2008_362.pdf</name>
    <abstract>
This paper describes the development of a wireless wearable
controller, GO, for both sound processing and interaction
with wearable lights. Pure Data is used for sound processing.
The GO prototype is built using a PIC microcontroller using
various sensors for receiving information from physical
movements.

</abstract>
    <keywords> Wireless controller, Pure Data, Gestural interface, Interactive Lights.  </keywords>
  </document>
  <document>
    <name>nime2008_364.pdf</name>
    <keywords> Graphical Interface, Computer Game, MIDI Display  </keywords>
  </document>
  <document>
    <name>nime2008_366.pdf</name>
    <abstract>
This demonstration presents three new augmented and meta
saxophone interface/instruments, built by the Bent Leather
Band. The instruments are designed for virtuosic live
performance and make use of Sukandar Kartadinata's Gluion
[OSC] interfaces. The project rationale and research outcomes
for the first twelve months is discussed. Instruments/interfaces
described include the Gluisop, Gluialto and Leathersop.

</abstract>
    <keywords> Augmented saxophone, Gluion, OSC, virtuosic performance systems  </keywords>
  </document>
  <document>
    <name>nime2008_372.pdf</name>
    <abstract>
The Musical Synchrotron is a software interface that connects 
wireless motion sensors to a real-time interactive environment 
(Pure Data, Max/MSP). In addition to the measurement of 
movement, the system provides audio playback and visual 
feedback. The Musical Synchrotron outputs a score with the 
degree in which synchronization with the presented music is 
successful. The interface has been used to measure how people 
move in response to music. The system was used for experiments 
at public events. 

</abstract>
    <keywords> Wireless sensors, tempo perception, social interaction, music and  movement, embodied music cognition  </keywords>
  </document>
  <document>
    <name>nime2009_001.pdf</name>
    <abstract> 
This paper presents an evaluation and comparison of four 
input devices for percussion tasks: a standard tom drum, 
Roland V-Drum, and two established examples of gestural 
controllers: the Buchla Lightning II, and the Radio Baton. 
The primary goal of this study was to determine how 
players' actions changed when moving from an acoustic 
instrument like the tom drum, to a gestural controller like 
the Buchla Lightning, which bears little resemblance to an 
acoustic percussion instrument. Motion capture data was 
analyzed by comparing a subject's hand height variability 
and timing accuracy across the four instruments as they 
performed simple musical tasks. Results suggest that 
certain gestures such as hand height amplitude can be 
adapted to these gestural controllers with little change and 
that in general subjects' timing variability is significantly 
affected when playing on the Lightning and Radio Baton 
when compared to the more familiar tom drum and V-
Drum. Possible explanations and other observations are 
also presented. 

</abstract>
    <keywords> Evaluation of Input Devices, Motion Capture,  Buchla Lightning II, Radio Baton.   </keywords>
  </document>
  <document>
    <name>nime2009_007.pdf</name>
    <abstract>
Measurement of pianists' arm movement provides a signal,
which is composed of controlled movements and noise. The
noise is composed of uncontrolled movement generated by
the interaction of the arm with the piano action and mea-
surement error. We propose a probabilistic model for arm
touch movements, which allows to estimate the amount of
noise in a joint. This estimation helps to interpret the move-
ment signal, which is of interest for augmented piano and
piano pedagogy applications.

</abstract>
    <keywords> Piano, arm movement, gesture, classification, augmented instrument, inertial sensing.  </keywords>
  </document>
  <document>
    <name>nime2009_013.pdf</name>
    <abstract>
This paper presents a HCI inspired evaluation of simple phys-
ical interfaces used to control physical models. Specifi-
cally knobs and sliders are compared in a creative and ex-
ploratory framework, which simulates the natural environ-
ment in which an electronic musician would normally ex-
plore a new instrument. No significant difference was mea-
sured between using knobs and sliders for controlling pa-
rameters of a physical modeling electronic instrument. The
reported difference between the tested instruments were mostly
due to the sound synthesis models.

</abstract>
    <keywords> Evaluation, Interfaces, Sliders, Knobs, Physi- cal Modeling, Electronic Musicians, Exploration, Creativ- ity, Affordances.  </keywords>
  </document>
  <document>
    <name>nime2009_019.pdf</name>
    <abstract> 

Haptic feedback is an important element that needs to be 
carefully designed in computer music interfaces. This 
paper presents an evaluation of several force renderings for 
target acquisition in space when used to support a music 
related task. The study presented here addresses only one 
musical aspect: the need to repeat elements accurately in 
time and in content. Several force scenarios will be 
rendered over a simple 3D target acquisition task and 
users' performance will be quantitatively and qualitatively 
evaluated. The results show how the users' subjective 
preference for a particular kind of force support does not 
always correlate to a quantitative measurement of 
performance enhancement. We describe a way in which a 
control mapping for a musical interface could be achieved 
without contradicting the users' preferences as obtained 
from the study. 
</abstract>
    <keywords> music interfaces, force feedback, tempo,  comfort, target acquisition.   </keywords>
  </document>
  <document>
    <name>nime2009_025.pdf</name>
    <abstract> 
In this paper, we discuss a number of issues related to the 
design of evaluation tests for comparing interactive music 
systems for improvisation. Our testing procedure covers 
rehearsal and performance environments, and captures the 
experiences of a musician/participant as well as an 
audience member/observer. We attempt to isolate salient 
components of system behavior, and test whether the 
musician or audience are able to discern between systems 
with significantly different behavioral components. We 
report on our experiences with our testing methodology, in 
comparative studies of our London and ARHS 
improvisation systems [1]. 

</abstract>
    <keywords> Interactive music systems, human computer  interaction, evaluation tests.   </keywords>
  </document>
  <document>
    <name>nime2009_031.pdf</name>
    <abstract> 
Motivated by previous work aimed at developing 

mathematical models to describe expressive timing in 

music, and specifically the final ritardandi, using 

measured kinematic data, we further investigate the 

linkage of locomotion and timing in music. The natural 

running behavior of four subjects is measured with a 

wearable sensor prototype and analyzed to create 

normalized tempo curves. The resulting curves are then 

used to modulate the final ritard of MIDI scores, which are 

also performed by an expert musician. A Turing-inspired 

listening test is conducted to observe a human listener's 

ability to determine the nature of the performer.  

</abstract>
    <keywords> Musical kinematics, expressive tempo,   machine music.   </keywords>
  </document>
  <document>
    <name>nime2009_033.pdf</name>
    <abstract> 
Vibetone is a musical input device which was build to 
explore tactile feedback in gesture based interaction. It is a 
prototype aimed to allow the performer to play both 
continuously and discrete pitched sounds in the same 
space. Our primary focus is on tactile feedback to guide 
the artist's movements during his performance. Thus, also 
untrained users are enabled to musical expression through 
bodily actions and precisely arm movements, guided 
through tactile feedback signals. 

</abstract>
    <keywords> tactile feedback, intuitive interaction, gestural  interaction, MIDI controller   </keywords>
  </document>
  <document>
    <name>nime2009_035.pdf</name>
    <keywords> Sonification, Interactive Sonification,  Auditory Display.   </keywords>
  </document>
  <document>
    <name>nime2009_037.pdf</name>
    <abstract>
In this project we have developed reactive instruments for 
performance. Reactive instruments provide feedback for 
the performer thereby providing a more dynamic 
experience. This is achieved through the use of haptics and 
robotics. Haptics provide a feedback system to the control 
surface. Robotics provides a way to actuate the instruments 
and their control surfaces. This allows a highly coordinated 
"dance" between performer and the instrument. An 
application for this idea is presented as a linear slide 
interface. Reactive interfaces represent a dynamic way for 
music to be portrayed in performance.  

</abstract>
    <keywords> haptics, robotics, dynamic interfaces   </keywords>
  </document>
  <document>
    <name>nime2009_039.pdf</name>
    <abstract> 
Hands On Stage, designed from a percussionist's 
perspective, is a new performance interface designed for 
audiovisual improvisation. It comprises a custom-built 
table interface and a performance system programmed in 
two environments, SuperCollider 3 and Isadora. This 
paper traces the interface's evolution over matters of 
relevant technology, concept, construction, system design, 
and its creative outcomes.   

</abstract>
    <keywords> audiovisual, interface design, performance.   </keywords>
  </document>
  <document>
    <name>nime2009_041.pdf</name>
    <abstract>
The Vibrobyte is a wireless haptic interface specialized for
co-located musical performance. The hardware is designed
around the open source Arduino platform, with haptic con-
trol data encapsulated in OSC messages, and OSC/hardware
communications handled by Processing. The Vibrobyte was
featured at the International Computer Music Conference
2008 (ICMC) in a telematic performance between ensem-
bles in Belfast, Palo Alto (California, USA), and Troy (New
York, USA).

</abstract>
  </document>
  <document>
    <name>nime2009_043.pdf</name>
    <abstract> 
This paper describes a cost-effective, modular, open source 
framework for a laser interface design that is open to 
community development, interaction and user 
modification. The following paper highlights ways in 
which we are implementing the multi-laser gestural 
interface in musical, visual, and robotic contexts.  

</abstract>
    <keywords> Lasers, photocell sensor, UltraSound, Open  Source controller design, digital gamelan, digital tanpura   </keywords>
  </document>
  <document>
    <name>nime2009_045.pdf</name>
    <keywords> Interaction, audience, performer, visualize, sen- sor, physical, gesture.  </keywords>
  </document>
  <document>
    <name>nime2009_048.pdf</name>
    <abstract> 
This is intended to introduce the system, which combines 
BodySuit, especially Powered Suit, and Second Life, as 
well as its possibilities and its uses in a musical 
performance application. The system which we propose 
contains both a gesture controller and robots at the same 
time. In this system, the Data Suit, BodySuit controls the 
avatar in Second Life and Second Life controls the 
exoskeleton, Powered Suit in real time. These are related 
with each other in conjunction with Second Life in 
Internet. BodySuit doesn't contain a hand-held controller. 
A performer, for example a dancer, wears a suit. Gestures 
are transformed into electronic signals by sensors. Powered 
Suit is another suit that a dancer wears, but gestures are 
generated by motors. This is a sort of wearable robot. 
Second Life is software that is developed by Linden Lab. It 
allows creating a virtual world and a virtual human 
(avatar) in Internet. Working together with BodySuit, 
Powered Suit, and Second Life the idea behind the system 
is that a human body is augmented by electronic signals 
and is reflected in a virtual world in order to be able to 
perform interactively.  
</abstract>
  </document>
  <document>
    <name>nime2009_050.pdf</name>
    <abstract> 
We developed a system called Life Game Orchestra that 
generates music by translating cellular patterns of 
Conway's Game of Life into musical scales. A performer 
can compose music by controlling varying cell patterns 
and sounds with visual and auditory fun. A performer 
assigns the elements of tone to two-dimensional cell 
patterns in the matrix of the Game of Life. Our system 
searches defined cell patterns in the varying matrix 
dynamically. If the patterns are matched, corresponding 
tones are generated. A performer can make cells in the 
matrix by moving in front of a camera and interactively 
influencing the generation of music. The progress of the 
Game of Life is controlled with a clock defined by the 
performer to configure the groove of the music. By 
running multiple matrices with different pattern mapping, 
clock timing, and instruments, we can perform an 
ensemble. The Life Game Orchestra is a fusion system of 
the design of a performer and the emergence of cellular 
automata as a complex system. 

</abstract>
    <keywords> Conway's Game of Life, Cellular automata,  Cell pattern, scale, Interactive composition, performance.   </keywords>
  </document>
  <document>
    <name>nime2009_052.pdf</name>
    <abstract> 
This article describes the implications of design and 
materials of computer controllers used in the context of 
interactive dance performance. Size, shape, and layout all 
influence audience perception of the performer, and 
materials imply context for further interpretation of the 
interactive performance work. It describes the construction 
of the "Control/Recorder" and the "VideoLyre," two 
custom computer control surfaces made for Leonardo's 
Chimes, a work by Toenjes, Marchant and Smith, and how 
these controllers contribute to theatrical aesthetic intent. 

</abstract>
    <keywords> control surface, interface, tactile, natural,  organic, interactive dance.   </keywords>
  </document>
  <document>
    <name>nime2009_054.pdf</name>
    <abstract> 
Deviate generates multiple streams of melodic and 
rhythmic output in real-time, according to user-specified 
control parameters. This performance system has been 
implemented using Max 5 [1] within the genre of popular 
contemporary electronic music, incorporating techno, 
IDM, and related forms. The aim of this project is not 
musical style synthesis, but to construct an environment 
in which a range of creative and musical goals may be 
achieved. A key aspect is control over generative 
processes, as well as consistent yet varied output. An 
approach is described which frees the user from 
determining note-level output while allowing control to be 
maintained over larger structural details, focusing 
specifically on the melodic aspect of this system. Audio 
examples are located online at 
http://www.cetenbaath.com/cb/about-deviate/. 

</abstract>
    <keywords> generative, performance, laptop, popular music   </keywords>
  </document>
  <document>
    <name>nime2009_056.pdf</name>
    <abstract>
SpiralSet is a sound toy incorporating game engine
software used in conjunction with a spectral synthesis
sound engine constructed in Max/MSP/Jitter. SpiralSet
was presented as an interactive installation piece at the
Sonic Arts Expo 2008, in Brighton, UK. A custom made
sensor-based interface is used for control of the system.
The user interactions are designed to be quickly accessible
in an installation context, yet allowing the potential for
sonic depth and variation.

</abstract>
    <keywords>  Sound Toys, Game Engines, Animated Interfaces,  Spectral Synthesis, Open Work, Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2009_058.pdf</name>
    <abstract> 
This paper explores a rapidly developed, new musical 
interface involving a touch-screen, 32 pressure sensitive 
button pads, infrared sensor, 8 knobs and cross-fader. We 
provide a versatile platform for computer-based music 
performance and production using a human computer 
interface that has strong visual and tactile feedback as well 
as robust software that exploits the strengths of each 
individual system component. 

</abstract>
  </document>
  <document>
    <name>nime2009_060.pdf</name>
    <abstract>
This paper presents the SARC EyesWeb Catalog (SEC), a
group of blocks designed for real-time gesture recognition
that have been developed for the open source program Eye-
sWeb. We describe how the recognition of real-time body
movements can be used for musician-computer-interaction.

</abstract>
    <keywords> SARC EyesWeb Catalog, gesture recognition  </keywords>
  </document>
  <document>
    <name>nime2009_062.pdf</name>
    <abstract> 
We describe a new method for 2D fiducial tracking. We 
use region adjacency information together with angles 
between regions to encode IDs inside fiducials, whereas 
previous research by Kaltenbrunner and Bencina utilize 
region adjacency tree. Our method supports a wide ID 
range and is fast enough to accommodate real-time video. 
It is also very robust against false positive detection. 

</abstract>
    <keywords> fiducial tracking, computer vision, tangible  user interface, interaction techniques.   </keywords>
  </document>
  <document>
    <name>nime2009_064.pdf</name>
    <abstract> 
During several decades, the research at Waseda University 
has been focused on developing anthropomorphic robots 
capable performing musical instruments. As a result of our 
research efforts, the Waseda Flutist Robot WF-4RIV and 
the Waseda Saxophonist Robot WAS-1 have been 
designed to reproduce the human player performance. As a 
long-term goal, we are proposing to enable the interaction 
between musical performance robots as well as with 
human players. In general the communication of humans 
within a band is a special case of conventional human 
social behavior. Rhythm, harmony and timbre of the music 
played represent the emotional states of the musicians. So 
the development of an artificial entity that participates in 
such an interaction may contribute to the better 
understanding of some of the mechanisms that enable the 
communication of humans in musical terms. Therefore, we 
are not considering a musical performance robot (MPR) 
just as a mere sophisticated MIDI instrument. Instead, its 
human-like design and the integration of perceptual 
capabilities may enable to act on its own autonomous 
initiative based on models which consider its own physical 
constrains. In this paper, we present an overview of our 
research approaches towards enabling the interaction 
between musical performance robots as well as with 
musicians. 

</abstract>
  </document>
  <document>
    <name>nime2009_070.pdf</name>
    <abstract> 
This paper presents an interactive and improvisational jam 
session, including human players and two robotic 
musicians. The project was developed in an effort to create 
novel and inspiring music through human-robot 
collaboration. The jam session incorporates Shimon, a 
newly-developed socially-interactive robotic marimba 
player, and Haile, a perceptual robotic percussionist 
developed in previous work. The paper gives an overview 
of the musical perception modules, adaptive improvisation 
modes and human-robot musical interaction models that 
were developed for the session. The paper also addresses 
the musical output that can be created from increased 
interconnections in an expanded multiple-robot multiple-
human ensemble, and suggests directions for future work.    
</abstract>
    <keywords> Robotic musicianship, Shimon, Haile.   </keywords>
  </document>
  <document>
    <name>nime2009_074.pdf</name>
    <abstract>
In this project, we have developed a real-time writing 
instrument for music control. The controller, MusicGrip, 
can capture the subtle dynamics of the user's grip while 
writing or drawing and map this to musical control signals 
and sonic outputs. This paper discusses this conversion of 
the common motor motion of handwriting into an 
innovative form of music expression. The presented 
example instrument can be used to integrate the composing 
aspect of music with painting and writing, creating a new 
art form from the resultant aural and visual representation 
of the collaborative performing process. 

</abstract>
    <keywords> Interactive music control, writing instrument,  pen controller, MIDI, group performing activity.   </keywords>
  </document>
  <document>
    <name>nime2009_078.pdf</name>
    <keywords> Tabletop computers, collaborative instruments, collaborative composition, group improvisation, spatial au- dio interfaces, customizable instruments.  </keywords>
  </document>
  <document>
    <name>nime2009_082.pdf</name>
    <abstract>
It is surely not difficult for anyone with experience in the
subject known as Music Theory to realize that there is a
very definite and precise relationship between music and
mathematics. This paper describes the SoriSu, a new
electronic musical instrument based on Sudoku puzzles,
which probe the expressive possibilities of mathematical
concepts in music. The concept proposes a new way of
mapping numbers to sound. This interface was designed to
provide easy and pleasing access to music for users who
are unfamiliar or uncomfortable with current musical
devices. The motivation behind the project is presented, as
well as hardware and software design.

</abstract>
    <keywords> Numbers, Game Interfaces, Mathematics and Sound, Mathematics in Music, Puzzles, Tangible User Interfaces.  </keywords>
  </document>
  <document>
    <name>nime2009_090.pdf</name>
    <abstract>

This paper presents a method for using a runner's pace
for real-time control of the time-scaling facility of a phase
vocoder, resulting in the automated synchronization of an
audio track tempo to the generated control signal. The in-
crease in usage of portable music players during exercise
has given rise to the development of new personal exercise
aids, most notably the Nike+iPod system, which relies on
embedded sensor technologies to provide kinematic work-
out statistics. There are also systems that select songs based
on the measured step frequency of a runner. The proposed
system also uses the pace of a runner, but this information is
used to change the tempo of the music.

</abstract>
    <keywords> NIME, synchronization, exercise, time-scaling.  </keywords>
  </document>
  <document>
    <name>nime2009_098.pdf</name>
    <abstract>
 We  present  the  Kalichord:  a  small,  handheld 
electro/acoustic instrument in which the player's right hand 
plucks virtual  strings while his left  hand uses  buttons to 
play  independent  bass  lines.   The  Kalichord  uses  the 
analog  signal  from  plucked  acoustic  tines  to  excite  a 
physical  string  model,  allowing  a  nuanced  and  intuitive 
plucking experience.  First, we catalog instruments related 
to  the  Kalichord.   Then  we  examine  the  use  of  analog 
signals to excite a physical string model and discuss the 
expressiveness and form factors that this technique affords. 
We then describe the overall construction of the Kalichord 
and possible playing styles, and finally we consider  ways 
we hope to improve upon the current prototype. 

</abstract>
    <keywords> Kalichord, physical model, tine, piezo, plucked  string, electro-acoustic instruments, kalimba, accordion  </keywords>
  </document>
  <document>
    <name>nime2009_102.pdf</name>
    <abstract>
In this paper we describe an approach for introducing new
electronic percussive sound possibilities for string
instruments by "listening" to the sounds of the instrument's
body and extracting audio and data from the wood's
acoustic vibrations. A method for capturing, localizing and
analyzing the percussive hits on the instrument's body is
presented, in connection with an audio-driven electronic
percussive sound module. The system introduces a new
gesture-sound relationship in the electric string instrument
playing environment, namely the use of percussive
techniques on the instrument's body which are null in
regular circumstances due to selective and exclusive
microphone use for the strings. Instrument body
percussions are widely used in the acoustic instrumental
praxis. They yield a strong potential for providing an
extended soundscape via instrument augmentation, directly
controlled by the musician through haptic manipulation of
the instrument itself. The research work was carried out on
the electric guitar, but the method used can apply to any
string instrument with a resonating body.

</abstract>
  </document>
  <document>
    <name>nime2009_106.pdf</name>
    <abstract>
Large vibrating plates are used as thunder sheets in orches-
tras. We have extended the use of flat plates by cementing a
flat panel electroacoustic transducer on a large brass sheet.
Because of the thickness of the panel, the output is subject to
nonlinear distortion. When combined with a real-time input
and signal processing algorithm, the active brass plate can
become an effective musical instrument for performance of
new music.

</abstract>
    <keywords> Electroacoustics, flat panel  </keywords>
  </document>
  <document>
    <name>nime2009_110.pdf</name>
    <abstract> 

This paper gives a historical overview of the development 
of alternative sonic display systems at Princeton 
University; in particular, the design, construction, and use 
in live performance of a series of spherical and 
hemispherical speaker systems.  We also provide a DIY 
guide to constructing the latest series of loudspeakers that 
we are currently using in our research and music making. 

</abstract>
    <keywords> loudspeakers, hemispherical speakers, sonic  display systems, laptop orchestras.   </keywords>
  </document>
  <document>
    <name>nime2009_116.pdf</name>
    <abstract> 
The history and future of Open Sound Control (OSC) is 
discussed and the next iteration of the OSC specification is 
introduced with discussion of new features to support 
NIME community activities. The roadmap to a major 
revision of OSC is developed. 

</abstract>
    <keywords> Open Sound Control, Time Tag, OSC,  Reservation Protocols.   </keywords>
  </document>
  <document>
    <name>nime2009_121.pdf</name>
    <abstract>
An on-the-fly reconfigurable low-level embedded service
architecture is presented as a means to improve scalabil-
ity, improve conceptual comprehensibility, reduce human
error and reduce development time when designing new
sensor-based electronic musical instruments with real-time
responsiveness. The implementation of the concept in
a project called micro-OSC is described. Other sensor
interfacing products are evaluated in the context of DIY
prototyping of musical instruments. The capabilities of
the micro-OSC platform are demonstrated through a set of
examples including resistive sensing, mixed digital-analog
systems, many-channel sensor interfaces and time-based
measurement methods.

</abstract>
    <keywords> real-time musical interface, DIY design, em- bedded web services, rapid prototyping, reconfigurable firmware  </keywords>
  </document>
  <document>
    <name>nime2009_125.pdf</name>
    <abstract>
Firmata is a generic protocol for communicating with mi-
crocontrollers from software on a host computer. The cen-
tral goal is to make the microcontroller an extension of the
programming environment on the host computer in a man-
ner that feels natural in that programming environment. It
was designed to be open and flexible so that any program-
ming environment can support it, and simple to implement
both on the microcontroller and the host computer to ensure
a wide range of implementations. The current reference im-
plementation is a library for Arduino/Wiring and is included
with Arduino software package since version 0012. There
are matching software modules for a number of languages,
like Pd, OpenFrameworks, Max/MSP, and Processing.

</abstract>
  </document>
  <document>
    <name>nime2009_131.pdf</name>
    <keywords> Data exchange, collaborative performance, in- teractive performance, interactive art works, sensor data, Open- SoundControl, SuperCollider, Max/MSP  Introduction and Background The SenseWorld Data Network addresses one of the major challenges in the research/creation of interactive live per- formance work the sharing and manipulation of raw and/or conditioned sensor data among different media systems (real time audio and video, lighting, mechatronics, show control, etc). While the </keywords>
  </document>
  <document>
    <name>nime2009_135.pdf</name>
    <abstract>
Low-latency streaming of high-quality audio has the poten-
tial to dramatically transform the world of interactive musi-
cal applications. We provide methods for accurately mea-
suring the end-to-end latency and audio quality of a deli-
vered audio stream and apply these methods to an empiri-
cal evaluation of several streaming engines. In anticipation
of future demands for emerging applications involving au-
dio interaction, we also review key features of streaming
engines and discuss potential challenges that remain to be
overcome.

</abstract>
    <keywords> Networked Musical Performance, high-fidelity audio streaming, glitch detection, latency measurement  </keywords>
  </document>
  <document>
    <name>nime2009_141.pdf</name>
    <abstract>
"Extension du corps sonore" is long-term project initiated
by Musiques Nouvelles [4], a contemporary music ensem-
ble in Mons. It aims at giving instrumental music perform-
ers an extended control over the sound of their instrument by
extending the understanding of the sound body from the in-
strument only to the combination of the instrument and the
whole body of the performer. The development started at
ARTeM and got the benefit of a three month numediart
research project [1] that focused on three axes of research:
pre-processing of sensor data, gesture recognition and map-
ping through interpolation. The objectives were the devel-
opment of computing methods and flexible Max/MSP ex-
ternals to be later integrated in the ARTeM software frame-
work for the concerts with viola player Dominica Eyck-
mans. They could be used in a variety of other artistic works
and will be made available on the numediart website [1],
where more detailed information can be found in the Quar-
terly Progress Scientific Report #4.

</abstract>
    <keywords> Sensor data pre-processing, gesture recognition, mapping, interpolation, extension du corps sonore  </keywords>
  </document>
  <document>
    <name>nime2009_147.pdf</name>
    <abstract> 
We describe initial prototypes and a design strategy for 

new, user-customized audio-manipulation and editing 
tools. These tools are designed to enable intuitive control 
of audio-processing tasks while anthropomorphically 
matching the target user. 

</abstract>
    <keywords> user modeling, user customization   </keywords>
  </document>
  <document>
    <name>nime2009_151.pdf</name>
    <abstract>
In  this  poster  we  present  the  early  prototype  of  the 
augmented  Psychophone  -  a  saxophone  with  various 
applied sensors,  allowing the saxophone player  to  attach 
effects like pitch shifting, wah-wah and ring modulation to 
the saxophone,  simply by moving the saxophone as  one 
would do when really being enthusiastic and involved in 
the  performance.  The  possibility  of  scratching  on  the 
previously recorded sound is also possible directly on the 
saxophone.   
   
</abstract>
    <keywords>  Augmented  saxophone,  Physical  computing,  hyper instruments, mapping.  </keywords>
  </document>
  <document>
    <name>nime2009_153.pdf</name>
    <abstract>
Catch Your Breath is an interactive audiovisual bio-feedback
system adapted from a project designed to reduce respira-
tory irregularity in patients undergoing 4D CT scans for on-
cological diagnosis. The system is currently implemented
and assessed as a potential means to reduce motion-induced
distortion in CT images.

A museum installation based on the same principle was
created in which an inexpensive wall-mounted web cam-
era tracks an IR sensor embedded into a pendant worn by
the user. The motion of the subjects breathing is tracked
and interpreted as a real-time variable tempo adjustment to
a stored musical file. The subject can then adjust his/her
breathing to synchronize with a separate accompaniment
line. When the breathing is regular and is at the desired
tempo, the audible result sounds synchronous and harmo-
nious. The accompaniment's tempo progresses and gradu-
ally decrease which causes the breathing to synchronize and
slow down, thus increasing relaxation.

</abstract>
    <keywords> sensor, music, auditory display.  </keywords>
  </document>
  <document>
    <name>nime2009_155.pdf</name>
    <abstract>
With the increase of sales of Wii game consoles, it is be-
coming commonplace for the Wii remote to be used as an
alternative input device for other computer systems. In this
paper, we present a system which makes use of the infrared
camera within the Wii remote to capture the gestures of a
conductor using a baton with an infrared LED and battery.
Our system then performs data analysis with gesture classi-
fication and following, and finally displays the gestures us-
ing visual baton trajectories and audio feedback. Gesture
trajectories are displayed in real time and can be compared
to the corresponding diagram shown in a textbook. In addi-
tion, since a conductor normally does not look at a screen
while conducting, tones are played to represent a certain
beat in a conducting gesture. Further, the system can be con-
trolled entirely with the baton, removing the need to switch
from baton to mouse. The interface is intended to be used
for pedagogy purposes.

</abstract>
    <keywords> Conducting, Gesture, Infrared, Learning, Wii.  </keywords>
  </document>
  <document>
    <name>nime2009_157.pdf</name>
    <abstract> 
"Music for 32 Chess Pieces" is a software system that 
supports composing, performing and improvising music by 
playing a chess game. A game server stores a 
representation of the state of a game, validates proposed 
moves by players, updates game state, and extracts a graph 
of piece-to-piece relationships. It also loads a plugin code 
module that acts as a composition. A plugin maps pieces 
and relationships on the board, such as support or attack 
relationships, to a timed sequence of notes and accents. 
The server transmits notes in a sequence to an audio 
renderer process via network datagrams. Two players can 
perform a composition by playing chess, and a player can 
improvise by adjusting a plugin's music mapping 
parameters via a graphical user interface. A composer can 
create a new composition by writing a new plugin that uses 
a distinct algorithm for mapping game rules and states to 
music. A composer can also write a new note-to-sound 
mapping program in the audio renderer language. This 
software is available at 
http://faculty.kutztown.edu/parson/music/ParsonMusic.html. 

</abstract>
    <keywords> algorithmic composition, chess, ChucK,  improvisation, Max/MSP, SuperCollider.   </keywords>
  </document>
  <document>
    <name>nime2009_159.pdf</name>
    <abstract>
This paper reports on work in progress on the creative
project MagNular, part of a wider practical study of the
potential collaborative compositional applications of game
engine technologies. MagNular is a sound toy utilizing
computer game and physics engine technologies to create
an animated interface used in conjunction with an external
sound engine developed within Max/MSP. The player
controls virtual magnets that attract or repel numerous
particle objects, moving them freely around the virtual
space. Particle object collision data is mapped to control
sound onsets and synthesis/DSP (Digital Signal
Processing) parameters. The user "composes" by
controlling and influencing the simulated physical
behaviors of the particle objects within the animated
interface.

</abstract>
    <keywords> Sound Toys, Open Work, Game Engines, Animated Interfaces,  Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2009_161.pdf</name>
    <abstract> 

AUDIO ORIENTEERING is a collaborative performance 

environment in which physical tokens are used to navigate 

an invisible sonic landscape. In this paper, I describe the 

hardware and software used to implement a prototype 

audio terrain with multiple interaction modes and sonic 

behaviors mapped onto three-dimensional space. 

</abstract>
    <keywords> wii, 3-d positioning, audio terrain,   collaborative performance.   </keywords>
  </document>
  <document>
    <name>nime2009_163.pdf</name>
    <abstract>
This  paper  presents  developments  in  the  technology 
underlying  the  cyclotactor,  a  finger-based  tactile  I/O 
device for musical  interaction.  These include significant 
improvements  both in  the basic characteristics  of tactile 
interaction and in  the related (vibro)tactile sample rates, 
latencies, and timing precision. After presenting the new 
prototype's tactile output force landscape, some of the new 
possibilities for interaction are discussed, especially those 
for musical interaction with zero audio/tactile latency.

</abstract>
    <keywords> Musical controller, tactile interface.  </keywords>
  </document>
  <document>
    <name>nime2009_165.pdf</name>
    <abstract>
A MIDI-to-OSC converter is implemented on a commer-
cially available embedded linux system, tighly integrated
with a microcontroller. A layered method is developed which
permits the conversion of serial data such as MIDI to OSC
formatted network packets with an overall system latency
below 5 milliseconds for common MIDI messages.

The Gumstix embedded computer provide an interest-
ing and modular platform for the development of such an
embedded applications. The project shows great potential
to evolve into a generic sensors-to-OSC ethernet converter
which should be very useful for artistic purposes and could
be used as a fast prototyping interface for gesture acquisition
devices.

</abstract>
    <keywords> MIDI, Open Sound Control, converter, gumstix  </keywords>
  </document>
  <document>
    <name>nime2009_169.pdf</name>
    <abstract>
This is a technical and experimental report of parallel 
processing, using the "Propeller" chip. Its eight 32 bits 
processors (cogs) can operate simultaneously, either 
independently or cooperatively,  sharing common resources 
through a central hub. I introduce this unique processor 
and discuss about the possibility to develop interactive 
systems and smart interfaces in media arts,  because we 
need many kinds of tasks at a same time with NIME-
related systems and installations.  I will report about (1) 
Propeller chip and its powerful IDE, (2) external  
interfaces for analog/digital inputs/outputs,  (3) VGA/
NTSC/PAL video generation, (4) audio signal processing, 
and (5) originally-developed MIDI input/output method. I 
also introduce three experimental prototype systems.

</abstract>
    <keywords> Propeller, parallel processing,  MIDI, sensor,  interfaces.  </keywords>
  </document>
  <document>
    <name>nime2009_171.pdf</name>
    <abstract>
The development of new interfaces for musical expression
has created a need to study how spectators comprehend new
performance technologies and practices. As part of a larger
project examining how interactions with technology can be
communicated with the spectator, we relate our model of
spectator understanding of error to the NIME discourse sur-
rounding transparency, mapping, skill and success.

</abstract>
    <keywords> performance, skill, transparency, design, HCI  </keywords>
  </document>
  <document>
    <name>nime2009_173.pdf</name>
    <abstract>
In this paper we present new issues and challenges related
to the vertical tablet playing. The approach is based on a
previously presented instrument, the HANDSKETCH. This
instrument has now been played regularly for more than two
years by several performers. Therefore this is an opportunity
to propose a better understanding of the performing strategy.
We present the behavior of the whole body as an underlying
aspect in the manipulation of the instrument.

</abstract>
    <keywords> graphic tablet, playing position, techniques  </keywords>
  </document>
  <document>
    <name>nime2009_177.pdf</name>
    <abstract>
Haptic technology, providing force cues and creating a pro-
grammable physical instrument interface, can assist musi-
cians in making gestures. The finite reaction time of the
human motor control system implies that the execution of a
brief musical gesture does not rely on immediate feedback
from the senses, rather it is preprogrammed to some degree.
Consequently, we suggest designing relatively simple and
deterministic interfaces for providing haptic assistance.

In this paper, we consider the specific problem of assist-
ing a musician in selecting pitches from a continuous range.
We build on a prior study by O'Modhrain of the accuracy
of pitches selected by musicians on a Theremin-like haptic
interface. To improve the assistance, we augment the inter-
face with programmed detents so that the musician can feel
the locations of equal tempered pitches. Nevertheless, the
musician can still perform arbitrary pitch inflections such as
glissandi, falls, and scoops. We investigate various forms
of haptic detents, including fixed detent levels and force-
sensitive detent levels. Preliminary results from a subject
test confirm improved accuracy in pitch selection brought
about by detents.

</abstract>
    <keywords> Haptic, detent, pitch selection, human motor system, feedback control, response time, gravity well  </keywords>
  </document>
  <document>
    <name>nime2009_183.pdf</name>
    <abstract>
A haptic musical instrument is an electronic musical instru-
ment that provides the musician not only with audio feed-
back but also with force feedback. By programming feed-
back controllers to emulate the laws of physics, many hap-
tic musical instruments have been previously designed that
mimic real acoustic musical instruments. The controller
programs have been implemented using finite difference and
(approximate) hybrid digital waveguide models. We present
a novel method for constructing haptic musical instruments
in which a haptic device is directly interfaced with a con-
ventional digital waveguide model by way of a junction el-
ement, improving the quality of the musician's interaction
with the virtual instrument. We introduce both the explicit
digital waveguide control junction and the implicit digital
waveguide control junction.

</abstract>
    <keywords> haptic musical instrument, digital waveguide, control junction, explicit, implicit, teleoperation  </keywords>
  </document>
  <document>
    <name>nime2009_187.pdf</name>
    <abstract> 

The carillon is one of the few instruments that elicit 

sophisticated haptic interaction from amateur and 

professional players alike. Like the piano keyboard, the 

velocity of a player's impact on each carillon key, or 

baton, affects the quality of the resultant tone; unlike the 

piano, each carillon baton returns a different force-

feedback. Force-feedback varies widely from one baton to 

the next across the entire range of the instrument and with 

further idiosyncratic variation from one instrument to 

another. This makes the carillon an ideal candidate for 

haptic simulation. The application of synthesized force-

feedback based on an analysis of forces operating in a 

typical carillon mechanism offers a blueprint for the design 

of an electronic practice clavier and with it the solution to 

a problem that has vexed carillonists for centuries, namely 

the inability to rehearse repertoire in private. This paper 

will focus on design and implementation of a haptic 

carillon clavier derived from an analysis of the Australian 

National Carillon in Canberra.  

</abstract>
    <keywords> Haptics, force-feedback, mechanical analysis.   </keywords>
  </document>
  <document>
    <name>nime2009_193.pdf</name>
    <abstract> 
The Electrumpet is an enhancement of a normal trumpet 
with a variety of electronic sensors and buttons. It is a new 
hybrid instrument that facilitates simultaneous acoustic and 
electronic playing. The normal playing skills of a trumpet 
player apply to the new instrument. The placing of the 
buttons and sensors is not a hindrance to acoustic use of 
the instrument and they are conveniently located. The 
device can be easily attached to and detached from a 
normal Bb-trumpet. The device has a wireless connection 
with the computer through Bluetooth-serial (Arduino). 
Audio and data processing in the computer is effected by 
three separate instances of MAX/MSP connected through 
OSC (controller data) and Soundflower (sound data). 
The current prototype consists of 7 analogue sensors (4 
valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" 
controller) and 9 digital switches. An LCD screen that is 
controlled by a separate Arduino (mini) is attached to the 
trumpet and displays the current controller settings that are 
sent through a serial connection. 
 
</abstract>
    <keywords> Trumpet, multiple Arduinos, Bluetooth, LCD,  low latency, OSC, MAX/MSP.   </keywords>
  </document>
  <document>
    <name>nime2009_199.pdf</name>
    <keywords> Controller, Sensor, MIDI, USB, Computer  Music, ribbon controllers, ribbon cello.   </keywords>
  </document>
  <document>
    <name>nime2009_203.pdf</name>
    <keywords> sensor, gestural, technology, performance,  piano, motors, interactive   </keywords>
  </document>
  <document>
    <name>nime2009_207.pdf</name>
    <abstract>
In this paper we describe an interaction framework which
classifies musicians' interactions with virtual musical in-
struments into three modes: instrumental, ornamental and
conversational. We argue that conversational interactions
are the most difficult to design for, but also the most in-
teresting. To illustrate our approach to designing for con-
versational interactions we describe the performance work
Partial Reflections 3 for two clarinets and interactive soft-
ware. This software uses simulated physical models to cre-
ate a virtual sound sculpture which both responds to and
produces sounds and visuals.

</abstract>
    <keywords> Music, instruments, interaction.  </keywords>
  </document>
  <document>
    <name>nime2009_213.pdf</name>
    <abstract>
In this paper we discuss the concept of style, focusing in par-
ticular on methods of designing new instruments that facili-
tate the cultivation and recognition of style. We distinguish
between style and structure of an interaction and discuss the
significance of this formulation within the context of NIME.
Two workshops that were conducted to explore style in in-
teraction design are described, from which we identify ele-
ments of style that can inform and influence the design pro-
cess. From these, we suggest steps toward designing for
style in new musical interactions.

</abstract>
    <keywords> expression, style, structure, skill, virtuosity  </keywords>
  </document>
  <document>
    <name>nime2009_222.pdf</name>
    <abstract> 
This paper reports on initial stages of research leading to 
the development of an intermedia performance 
Counterlines - a duet for Disklavier and Wacom Cintiq, in 
which both performers generate audiovisual gestures that 
relate to each other contrapuntally. The pianist generates 
graphic elements while playing music and the graphic 
performer generates piano notes by drawing lines. The 
paper focuses on interfacing sounds and images performed 
by the pianist. It provides rationale for the choice of 
materials of great simplicity and describes our approach to 
mapping. 

</abstract>
    <keywords> intermedia, Disklavier, piano, Wacom Cintiq,  mapping, visual music   </keywords>
  </document>
  <document>
    <name>nime2009_226.pdf</name>
    <abstract> 
Music composition on computer is a challenging task, 

involving a range of data types to be managed within a 
single software tool. A composition typically comprises a 
complex arrangement of material, with many internal 
relationships between data in different locations - 
repetition, inversion, retrograde, reversal and more 
sophisticated transformations.  The creation of such 
complex artefacts is labour intensive, and current systems 
typically place a significant cognitive burden on the 
composer in terms of maintaining a work as a coherent 
whole. FrameWorks 3D is an attempt to improve support 
for composition tasks within a Digital Audio Workstation 
(DAW) style environment via a novel three-dimensional 
(3D) user-interface. In addition to the standard paradigm of 
tracks, regions and tape recording analogy, FrameWorks 
displays hierarchical and transformational information in a 
single, fully navigable workspace. The implementation 
combines Java with Max/MSP to create a cross-platform, 
user-extensible package and will be used to assess the 
viability of such a tool and to develop the ideas further. 

</abstract>
    <keywords> Digital Audio Workstation, graphical user- interfaces, 3D graphics, Max/MSP, Java.   </keywords>
  </document>
  <document>
    <name>nime2009_230.pdf</name>
    <abstract> 
A compendium of foundational circuits for interfacing 
resistive pressure and position sensors is presented with 
example applications for music controllers and tangible 
interfaces. 

</abstract>
    <keywords> Piezoresistive Touch Sensor Pressure Sensing  Current Steering Multitouch.   </keywords>
  </document>
  <document>
    <name>nime2009_236.pdf</name>
    <abstract>
This paper presents a new force-sensitive surface designed
for playing music. A prototype system has been imple-
mented using a passive capacitive sensor, a commodity mul-
tichannel audio interface, and decoding software running on
a laptop computer. This setup has been a successful, low-
cost route to a number of experiments in intimate musical
control.

</abstract>
    <keywords> Multitouch, sensors, tactile, capacitive, percus- sion controllers.  </keywords>
  </document>
  <document>
    <name>nime2009_242.pdf</name>
    <abstract> 
This paper introduces a flexible mapping editor, which 
transforms multi-touch devices into musical instruments.  
The editor enables users to create interfaces by dragging 
and dropping components onto the interface and attaching 
actions to them, which will be executed when certain user-
defined conditions obtain.  The editor receives touch 
information via the non-proprietary communication 
protocol, TUIO [9], and can, therefore, be used together 
with a variety of different multi-touch input devices. 

</abstract>
    <keywords> NIME, multi-touch, multi-modal interface,  sonic interaction design.   </keywords>
  </document>
  <document>
    <name>nime2009_250.pdf</name>
    <abstract> 
The UBS Virtual Maestro is an interactive conducting 
system designed by Immersion Music to simulate the 
experience of orchestral conducting for the general public 
attending a classical music concert.  The system utilizes 
the Wii Remote, which users hold and move like a 
conducting baton to affect the tempo and dynamics of an 
orchestral video/audio recording. The accelerometer data 
from the Wii Remote is used to control playback speed and 
volume in real-time.   The system is housed in a UBS-
branded kiosk that has toured classical performing arts 
venues throughout the United States and Europe in 2007 
and 2008. In this paper we share our experiences in 
designing this standalone system for thousands of users, 
and lessons that we learned from the project. 

</abstract>
    <keywords> conducting, gesture, interactive installations,  Wii Remote   </keywords>
  </document>
  <document>
    <name>nime2009_256.pdf</name>
    <abstract> 
This paper describes The Vocal Augmentation and 

Manipulation Prosthesis (VAMP) a gesture-based wearable 

controller for live-time vocal performance.  This controller 

allows a singer to capture and manipulate single notes that 

he or she sings, using a gestural vocabulary developed 

from that of choral conducting.  By drawing from a 

familiar gestural vocabulary, this controller and the 

associated mappings can be more intuitive and expressive 

for both performer and audience.  

</abstract>
    <keywords> musical expressivity, vocal performance,   gestural control, conducting.   </keywords>
  </document>
  <document>
    <name>nime2009_264.pdf</name>
    <abstract>
This paper introduces the new audiovisual sequencing 
system "Versum" that allows users to compose in three 
dimensions. In the present paper the conceptual soil from 
which this system has sprung is discussed first. Secondly, 
the basic concepts with which Versum operates are 
explained, providing a general idea of what is meant by 
sequencing in three dimensions and explaining what 
compositions made in Versum can look and sound like. 
Thirdly, the practical ways in which a composer can use 
Versum to make his own audiovisual compositions are 
presented by means of a more detailed description of the 
different graphical user interface elements. Fourthly, a 
short description is given of the modular structure of the 
software underlying Versum. Finally, several foresights 
regarding the directions in which Versum will continue 
to develop in the near future are presented. 

</abstract>
    <keywords> audiovisual, sequencing, collaboration.   </keywords>
  </document>
  <document>
    <name>nime2009_266.pdf</name>
    <abstract>
In this paper we describe findings related to user interface
requirements for live electronic music arising from research
conducted as part of the first three-year phase of the EU-
funded Integra project. A number of graphical user interface
(GUI) prototypes developed during the Integra project ini-
tial phase are described and conclusions drawn about their
design and implementation.

</abstract>
    <keywords> Integra, User Interface, Usability, Design, Live Electronics, Music Technology  </keywords>
  </document>
  <document>
    <name>nime2009_268.pdf</name>
    <keywords> Interactive music instruments, visual  interfaces, visual feedback, tangible interfaces, augmented  reality, collaborative music, networked musical  instruments, real-time musical systems, musical sequencer.   </keywords>
  </document>
  <document>
    <name>nime2009_270.pdf</name>
  </document>
  <document>
    <name>nime2009_276.pdf</name>
    <abstract>
Phonetic symbols describe movements of the vocal tract,

tongue and lips, and are combined into complex movements
forming the words of language. In music, vocables are words
that describe musical sounds, by relating vocal movements
to articulations of a musical instrument. We posit that vo-
cable words allow the composers and listeners to engage
closely with dimensions of timbre, and that vocables could
see greater use in electronic music interfaces. A preliminary
system for controlling percussive physical modelling syn-
thesis with textual words is introduced, with particular ap-
plication in expressive specification of timbre during com-
puter music performances.

</abstract>
  </document>
  <document>
    <name>nime2009_280.pdf</name>
    <abstract> 
Supervised learning methods have long been used to allow 
musical interface designers to generate new mappings by 
example. We propose a method for harnessing machine 
learning algorithms within a radically interactive 
paradigm, in which the designer may repeatedly generate 
examples, train a learner, evaluate outcomes, and modify 
parameters in real-time within a single software 
environment. We describe our meta-instrument, the 
Wekinator, which allows a user to engage in on-the-fly 
learning using arbitrary control modalities and sound 
synthesis environments. We provide details regarding the 
system implementation and discuss our experiences using 
the Wekinator for experimentation and performance. 

</abstract>
    <keywords> Machine learning, mapping, tools.   </keywords>
  </document>
  <document>
    <name>nime2009_286.pdf</name>
    <abstract>
In this paper mappings and adaptation in the context of 
interactive sound installations are discussed. Starting from 
an ecological perspective on non-expert audience 
interaction a brief overview and discussion of mapping 
strategies with a special focus on adaptive systems using 
machine learning algorithms is given. An audio-visual 
interactive installation is analyzed and its implementation 
used to illustrate the issues of audience engagement and to 
discuss the efficiency of adaptive mappings.  

</abstract>
    <keywords> Interaction, adaptive mapping, machine  learning, audience engagement  </keywords>
  </document>
  <document>
    <name>nime2009_297.pdf</name>
    <abstract>
The Fragmented Orchestra is a distributed musical instru-

ment which combines live audio streams from geographi-
cally disparate sites, and granulates each according to the
spike timings of an artificial spiking neural network. This
paper introduces the work, outlining its historical context,
technical architecture, neuronal model and network infras-
tructure, making specific reference to modes of interaction
with the public.

</abstract>
  </document>
  <document>
    <name>nime2009_303.pdf</name>
    <abstract> 

The Smule Ocarina is a wind instrument designed for the 

iPhone, fully leveraging its wide array of technologies: 

microphone input (for breath input), multitouch (for 

fingering), accelerometer, real-time sound synthesis, high-

performance graphics, GPS/location, and persistent data 

connection.  In this mobile musical artifact, the 

interactions of the ancient flute-like instrument are both 

preserved and transformed via breath-control and 

multitouch finger-holes, while the onboard global 

positioning and persistent data connection provide the 

opportunity to create a new social experience, allowing 

the users of Ocarina to listen to one another.  In this way, 

Ocarina is also a type of social instrument that enables a 

different, perhaps even magical, sense of global 

connectivity. 

</abstract>
  </document>
  <document>
    <name>nime2009_308.pdf</name>
    <abstract> 
This paper presents "Scratch-Off", a new musical 
multiplayer DJ game that has been designed for a mobile 
phone.  We describe how the game is used as a test 
platform for experimenting with various types of 
multimodal feedback.  The game uses movement gestures 
made by the players to scratch a record and control cross-
fades between tracks, with the objective of the game to 
make the correct scratch at the correct time in relation to 
the music.  Gestures are detected using the devices built-in 
tri-axis accelerometer and multi-touch screen display.  The 
players receive visual, audio and various types of 
vibrotactile feedback to help them make the correct scratch 
on the beat of the music track.  We also discuss the results 
of a pilot study using this interface. 

</abstract>
    <keywords> Mobile devices, gesture, audio games.   </keywords>
  </document>
  <document>
    <name>nime2009_312.pdf</name>
    <abstract> 
ZooZBeat is a gesture-based mobile music studio.  It is 
designed to provide users with expressive and creative 
access to music making on the go. ZooZBeat users shake 
the phone or tap the screen to enter notes. The result is 
quantized, mapped onto a musical scale, and looped. Users 
can then use tilt and shake movements to manipulate and 
share their creation in a group. Emphasis is placed on 
finding intuitive metaphors for mobile music creation and 
maintaining a balance between control and ease-of-use that 
allows non-musicians to begin creating music with the 
application immediately. 

</abstract>
    <keywords> mobile music, gestural control   </keywords>
  </document>
  <document>
    <name>nime2009_316.pdf</name>
    <abstract> 
It has been shown that collaborative musical interfaces 
encourage novice users to explore the sound space and 
promote their participation as music performers. 
Nevertheless, such interfaces are generally physically 
situated and can limit the possibility of movements on the 
stage, a critical factor in live music performance. In this 
paper we introduce the Drummer, a networked digital 
musical interface that allows multiple performers to design 
and play drum kits simultaneously while, at the same time, 
keeping their ability to freely move on the stage. The 
system consists of multiple Nintendo DS clients with an 
intuitive, user-configurable interface and a server computer 
which plays drum sounds. The Drummer Machine, a small 
piece of hardware to augment the performance of the 
Drummer, is also introduced. 

</abstract>
  </document>
  <document>
    <name>nime2010_001.pdf</name>
    <abstract> 

The aim of this paper is to define the process of iterative 
interface design as it pertains to musical performance. 
Embodying this design approach, the Monome OSC/MIDI USB 
controller represents a minimalist, open-source hardware 
device. The open-source nature of the device has allowed for a 
small group of Monome users to modify the hardware, 
firmware, and software associated with the interface. These user 
driven modifications have allowed the re-imagining of the 
interface for new and novel purposes, beyond even that of the 
device's original intentions. With development being driven by 
a community of users, a device can become several related but 
unique generations of musical controllers, each one focused on 
a specific set of needs. 

</abstract>
    <keywords> Iterative Design, Monome, Arduinome, Arduino.   </keywords>
  </document>
  <document>
    <name>nime2010_007.pdf</name>
    <keywords> Musical instruments, Script language  </keywords>
  </document>
  <document>
    <name>nime2010_013.pdf</name>
  </document>
  <document>
    <name>nime2010_019.pdf</name>
    <abstract>
This paper, describes the second phase of an ongoing research 
project  dealing  with  the  implementation  of  an  interactive 
interface. It is a "hands free" instrument, utilizing a non-contact 
tactile feedback method based on airborne ultrasound. The three 
main  elements/components  of  the  interface  that  will  be 
discussed in this paper are: 1. Generation of audible sound by 
self-demodulation  of  an  ultrasound  signal  during  its 
propagation through air; 2. The condensation of the ultrasound 
energy  in  one  spatial  point  generating  a  precise  tactile 
reproduction  of  the  audible  sound;  and  3.  The  feed-forward 
method enabling a real-time intervention of the musician,  by 
shaping the tactile (ultra)sound directly with his hands.

</abstract>
    <keywords> haptics,  vibro-tactility,  feedback,  ultrasound,  hands-free  interface, nonlinear acoustics, parametric array.  </keywords>
  </document>
  <document>
    <name>nime2010_023.pdf</name>
    <abstract>
The Neurohedron is a multi-modal interface for a nonlinear 
sequencer software model, embodied physically in a 
dodecahedron.  The faces of the dodecahedron are both inputs 
and outputs, allowing the device to visualize the activity of the 
software model as well  as convey input to it.  The software 
model maps MIDI notes to the faces of the device, and defines 
and controls the behavior of the sequencer's progression around 
its surface, resulting in a unique instrument for computer-based 
performance and composition.  

</abstract>
  </document>
  <document>
    <name>nime2010_026.pdf</name>
    <abstract>
We introduce an interactive interface for the custom design
of metallophones. The shape of each plate must be deter-
mined in the design process so that the metallophone will
produce the proper tone when struck with a mallet. Un-
fortunately, the relationship between plate shape and tone is
complex, which makes it difficult to design plates with arbi-
trary shapes. Our system addresses this problem by running
a concurrent numerical eigenanalysis during interactive ge-
ometry editing. It continuously presents a predicted tone to
the user with both visual and audio feedback, thus making
it possible to design a plate with any desired shape and tone.
We developed this system to demonstrate the effectiveness
of integrating real-time finite element method analysis into
geometric editing to facilitate the design of custom-made
musical instruments. An informal study demonstrated the
ability of technically unsophisticated user to apply the sys-
tem to complex metallophone design.

</abstract>
    <keywords> Modeling - Modeling Interfaces, Modeling - Geometric Mod- eling, Modeling - CAD, Methods and Applications - Edu- cation, Real-time FEM  </keywords>
  </document>
  <document>
    <name>nime2010_031.pdf</name>
    <abstract> 
The field of mixed-reality interface design is relatively young 
and in regards to music, has not been explored in great depth. 
Using computer vision and collision detection techniques, 
Freepad further explores the development of mixed-reality 
interfaces for music. The result is an accessible user-definable 
MIDI interface for anyone with a webcam, pen and paper, 
which outputs MIDI notes with velocity values based on the 
speed of the strikes on drawn pads. 

</abstract>
    <keywords>  Computer vision, form recognition, collision detection, mixed- reality, custom interface, MIDI   </keywords>
  </document>
  <document>
    <name>nime2010_037.pdf</name>
    <abstract>
Our team realized that a need existed for a music program-
ming interface in the Minim audio library of the Processing
programming environment. The audience for this new in-
terface would be the novice programmer interested in using
music as part of the learning experience, though the inter-
face should also be complex enough to benefit experienced
artist-programmers. We collected many ideas from cur-
rently available music programming languages and libraries
to design and create the new capabilities in Minim. The
basic mechanisms include chained unit generators, instru-
ments, and notes. In general, one "patches" unit generators
(for example, oscillators, delays, and envelopes) together in
order to create synthesis algorithms. These algorithms can
then either create continuous sound, or be used in instru-
ments to play notes with specific start time and duration.
We have written a base set of unit generators to enable
a wide variety of synthesis options, and the capabilities of
the unit generators, instruments, and Processing allow for
a wide range of composition techniques.

</abstract>
    <keywords> Minim, music programming, audio library, Processing, mu- sic software  </keywords>
  </document>
  <document>
    <name>nime2010_043.pdf</name>
    <abstract> 
The analysis of digital music systems has traditionally been 
characterized by an approach that can be defined as 
phenomenological. The focus has been on the body and its 
relationship to the machine, often neglecting the system's 
conceptual design. This paper brings into focus the epistemic 
features of digital systems, which implies emphasizing the 
cognitive, conceptual and music theoretical side of our musical 
instruments. An epistemic dimension space for the analysis of 
musical devices is proposed. 

</abstract>
    <keywords>  Epistemic tools, music theory, dimension space, analysis.   </keywords>
  </document>
  <document>
    <name>nime2010_047.pdf</name>
    <abstract> 
Human agency, our capacity for action, has been at the hub of 

discussions centring upon philosophical enquiry for a long pe-

riod of time. Sensory supplementation devices can provide us 

with unique opportunities to investigate the different aspects of 

our agency by enabling new modes of perception and facilita-

ting the emergence of novel interactions, all of which is impos-

sible without the aforesaid devices. Our preliminary study in-

vestigates the non-verbal strategies employed for negotiation of 

our capacity for action with other bodies and the surrounding 

space through body-to-body and body-to-space couplings en-

abled by sensory supplementation devices. We employed a low-

fi rapid prototyping approach to build this device, enabling 

distal perception by sonic and haptic feedback. Further, we 

conducted a workshop in which participants equipped with this 

device engaged in game-like activities. 

</abstract>
    <keywords>  Human agency, sensory supplementation, distal perception,   sonic feedback, tactile feedback, enactive interfaces   </keywords>
  </document>
  <document>
    <name>nime2010_051.pdf</name>
  </document>
  <document>
    <name>nime2010_057.pdf</name>
    <keywords> AlloSphere, mapping, performance, HCI, interactivity, Vir- tual Reality, OSC, multi-user, network  </keywords>
  </document>
  <document>
    <name>nime2010_063.pdf</name>
  </document>
  <document>
    <name>nime2010_069.pdf</name>
    <abstract>
This  paper  articulates  an  interest  in  a  kind  of  interactive 
musical  instrument  and artwork that defines the mechanisms 
for instrumental interactivity from the iconic morphologies of 
"ready-mades",  casting  historical  utilitarian  objects  as  the 
basis  for  performed  musical  experiences  by  spectators.  The 
interactive  repertoires  are  therefore  partially  pre-determined 
through  enculturated  behaviors  that  are  associated  with 
particular objects, but more importantly, inextricably linked to 
the  thematic  and  meaningful  assemblage  of  the  work  itself. 
Our  new  work  epi-thet  gathers  data  from  individual 
interactions  with  common  microscopes  placed  on  platforms 
within  a  large  space.  This  data  is  correlated  with  public 
domain genetic datasets obtained from micro-array analysis. A 
sonification  algorithm  generates  unique  compositions 
associated  with  the  spectator  "as  measured"  through  their 
individual specification in performing an iconic measurement 
action.  The apparatus is a receptacle for unique compositions 
in sound, and invites a participatory choreography of stillness 
that is available for reception as a live musical performance.  

</abstract>
    <keywords> Sonification  installation  spectator-choreography  micro-array  ready-mades morphology stillness   </keywords>
  </document>
  <document>
    <name>nime2010_072.pdf</name>
    <abstract>
The propriety of articulation, especially of notes that lack
annotations, is influenced by the origin of the particular
music. This paper presents a rule system for articulation
derived from late Baroque and early Classic treatises on per-
formance. Expressive articulation, in this respect, is under-
stood as a combination of alterable tone features like dura-
tion, loudness, and timbre. The model differentiates global
characteristics and local particularities, provides a general
framework for human-like music performances, and, there-
fore, serves as a basis for further and more complex rule
systems.

</abstract>
    <keywords> Articulation, Historically Informed Performance, Expres- sive Performance, Synthetic Performance  </keywords>
  </document>
  <document>
    <name>nime2010_076.pdf</name>
    <abstract> 
We discuss how the environment urMus was designed to allow 
creation of mobile musical instruments on multi-touch 
smartphones. The design of a mobile musical instrument 
consists of connecting sensory capabilities to output modalities 
through various means of processing. We describe how the 
default mapping interface was designed which allows to set up 
such a pipeline and how visual and interactive multi-touch UIs 
for musical instruments can be designed within the system. 

</abstract>
    <keywords>  Mobile music making, meta-environment, design, mapping,  user interface   </keywords>
  </document>
  <document>
    <name>nime2010_082.pdf</name>
    <abstract>
In this paper, we describe the development of the Stan-
ford Mobile Phone Orchestra (MoPhO) since its inception
in 2007. As a newly structured ensemble of musicians with
iPhones and wearable speakers, MoPhO takes advantage
of the ubiquity and mobility of smartphones as well as
the unique interaction techniques offered by such devices.
MoPhO offers a new platform for research, instrument de-
sign, composition, and performance that can be juxtaposed
to that of a laptop orchestra. We trace the origins of MoPhO,
describe the motivations behind the current hardware and
software design in relation to the backdrop of current trends
in mobile music making, detail key interaction concepts
around new repertoire, and conclude with an analysis on
the development of MoPhO thus far.

</abstract>
    <keywords> mobile phone orchestra, live performance, iPhone, mobile music  </keywords>
  </document>
  <document>
    <name>nime2010_088.pdf</name>
    <abstract> 
This paper reviews and extends questions of the scope of an 
interactive musical instrument and mapping strategies for 
expressive performance. We apply notions of embodiment and 
affordance to characterize gestural instruments. We note that 
the democratization of sensor technology in consumer devices 
has extended the cultural contexts for interaction. We revisit 
questions of mapping drawing upon the theory of affordances to 
consider mapping and instrument together. This is applied to 
recent work by the author and his collaborators in the 
development of instruments based on mobile devices designed 
for specific performance situations. 

</abstract>
    <keywords>  Musical affordance, NIME, mapping, instrument definition,  mobile, multimodal interaction.   </keywords>
  </document>
  <document>
    <name>nime2010_094.pdf</name>
    <abstract>
This paper describes a novel method for composing and
improvisation with real-time chaotic oscillators. Recently
discovered algebraically simple nonlinear third-order differ-
ential equations are solved and acoustical descriptors relat-
ing to their frequency spectrums are determined according
to the MPEG-7 specification. A second nonlinearity is then
added to these equations: a real-time audio signal. Descrip-
tive properties of the complex behaviour of these equations
are then determined as a function of difference tones de-
rived from a Just Intonation scale and the amplitude of
the audio signal. By using only the real-time audio signal
from live performer/s as an input the causal relationship
between acoustic performance gestures and computer out-
put, including any visual or performer-instruction output,
is deterministic even if the chaotic behaviours are not.

</abstract>
  </document>
  <document>
    <name>nime2010_100.pdf</name>
    <keywords> Interactive  music  interface,  real-time,  percussion,  machine learning, Markov models, MIDI.  </keywords>
  </document>
  <document>
    <name>nime2010_106.pdf</name>
    <abstract>
A qualitative study to investigate the development of style
in performance with a highly constrained musical instru-
ment is described. A new one-button instrument was de-
signed, with which several musicians were each asked to
practice and develop a solo performance. Observations of
trends in attributes of these performances are detailed in re-
lation to participants' statements in structured interviews.
Participants were observed to develop stylistic variations
both within the domain of activities suggested by the con-
straint, and by discovering non-obvious techniques through
a variety of strategies. Data suggest that stylistic variations
occurred in spite of perceived constraint, but also because
of perceived constraint. Furthermore, participants tended
to draw on unique experiences, approaches and perspectives
that shaped individual performances.

</abstract>
    <keywords> design, interaction, performance, persuasive technology  </keywords>
  </document>
  <document>
    <name>nime2010_112.pdf</name>
    <abstract> 
We propose an environment that allows users to create music 
by leveraging playful visualization and organic interaction. Our 
attempt to improve ideas drawn from traditional sequencer 
paradigm has been made in terms of extemporizing music and 
associating with visualization in real-time. In order to offer 
different user experience and musical possibility, this system 
incorporates many techniques, including; flocking simulation, 
nondeterministic finite automata (NFA), score file analysis, 
vector calculation, OpenGL animation, and networking. We 
transform a sequencer into an audiovisual platform for 
composition and performance, which is furnished with artistry 
and ease of use. Thus we believe that it is suitable for not only 
artists such as algorithmic composers or audiovisual 
performers, but also anyone who wants to play music and 
imagery in a different way.	 

</abstract>
  </document>
  <document>
    <name>nime2010_116.pdf</name>
    <abstract> 
In this paper, we introduce a wireless musical interface driven 
by grasping forces and human motion. The sounds generated by 
the traditional digital musical instruments are dependent on the 
physical shape of the musical instruments. The freedom of the 
musical performance is restricted by its structure. Therefore, the 
sounds cannot be generated with the body expression like the 
dance. We developed a ball-shaped interface, TwinkleBall, to 
achieve the free-style performance. A photo sensor is embedded 
in the translucent rubber ball to detect the grasping force of the 
performer. The grasping force is translated into the luminance 
intensity for processing. Moreover, an accelerometer is also 
embedded in the interface for motion sensing. By using these 
sensors, a performer can control the note and volume by varying 
grasping force and motion respectively. The features of the 
proposed interface are ball-shaped, wireless, and handheld size. 
As a result, the proposed interface is able to generate the sound 
from the body expression such as dance. 

</abstract>
    <keywords>  Musical Interface, Embodied Sound Media, Dance Performance.   </keywords>
  </document>
  <document>
    <name>nime2010_125.pdf</name>
    <keywords> contrary, beat tracking, stream analysis, musical agent  </keywords>
  </document>
  <document>
    <name>nime2010_130.pdf</name>
    <abstract> 
The tools for spatial composition typically model just a small 
subset of the spatial audio cues known to researchers. As 
composers explore this medium it has become evident that the 
nature of spatial sound perception is complex. Yet interfaces for 
spatial composition are often simplistic and the end results can be 
disappointing. This paper presents an interface that is designed to 
liberate the composer from thinking of spatialised sound as points 
in space. Instead, visual images are used to define sound in terms 
of shape, size and location. Images can be sequenced into video, 
thereby creating rich and complex temporal soundscapes. The 
interface offers both the ability to craft soundscapes and also 
compose their evolution in time. 

</abstract>
    <keywords>  Spatial audio, surround sound, ambisonics, granular synthesis,  decorrelation, diffusion.   </keywords>
  </document>
  <document>
    <name>nime2010_136.pdf</name>
    <abstract> 
Multi-point devices are rapidly becoming a practical interface 
choice for electronic musicians. Interfaces that generate 
multiple simultaneous streams of point data present a unique 
mapping challenge. This paper describes an analysis system for 
point relationships that acts as a bridge between raw streams of 
multi-point data and the instruments they control, using a multi-
point trackpad to test various configurations. The aim is to 
provide a practical approach for instrument programmers 
working with multi-point tools, while highlighting the 
difference between mapping systems based on point coordinate 
streams, grid evaluations, or object interaction and mapping 
systems based on multi-point data relationships.  

</abstract>
    <keywords>  Multi-point, multi-touch interface, instrument mapping,  multi- point data analysis, trackpad instrument    </keywords>
  </document>
  <document>
    <name>nime2010_140.pdf</name>
    <abstract> 
An important part of building interactive sound models is 

designing the interface and control strategy. The 

multidimensional structure of the gestures natural for a musical 

or physical interface may have little obvious relationship to the 

parameters that a sound synthesis algorithm exposes for control. 

A common situation arises when there is a nonlinear synthesis 

technique for which a traditional instrumental interface with 

quasi-independent control of pitch and expression is desired. 

This paper presents a semi-automatic meta-modeling tool called 

the Instrumentalizer for embedding arbitrary synthesis 

algorithms in a control structure that exposes traditional 

instrument controls for pitch and expression.   

</abstract>
    <keywords>  Musical interface, parameter mapping, expressive control.   </keywords>
  </document>
  <document>
    <name>nime2010_144.pdf</name>
    <abstract>
scoreLight is a playful musical instrument capable of gene-
rating sound from the lines of drawings as well as from the
edges of three-dimensional objects nearby (including every-
day objects, sculptures and architectural details, but also
the performer's hands or even the moving silhouettes of
dancers). There is no camera nor projector: a laser spot
explores shapes as a pick-up head would search for sound
over the surface of a vinyl record - with the significant dif-
ference that the groove is generated by the contours of the
drawing itself.

</abstract>
    <keywords> H5.2 [User Interfaces] interaction styles / H.5.5 [Sound and Music Computing] Methodologies and techniques / J.5 [Arts and Humanities] performing arts  </keywords>
  </document>
  <document>
    <name>nime2010_150.pdf</name>
    <keywords> turntable, dial, encoder, re-purposed, hard drive, scratch- ing, inherent dynamics, DIY  </keywords>
  </document>
  <document>
    <name>nime2010_156.pdf</name>
    <abstract> 
Since 2007, our research is related to the development of an 
anthropomorphic saxophonist robot, which it has been designed 
to imitate the saxophonist playing by mechanically reproducing 
the organs involved for playing a saxophone. Our research aims 
in understanding the motor control from an engineering point of 
view and enabling the communication. In this paper, the 
Waseda Saxophone Robot No. 2 (WAS-2) which is composed 
by 22-DOFs is detailed. The lip mechanism of WAS-2 has been 
designed with 3-DOFs to control the motion of the lower, upper 
and sideway lips. In addition, a human-like hand (16 DOF-s) 
has been designed to enable to play all the keys of the 
instrument. Regarding the improvement of the control system, a 
feed-forward control system with dead-time compensation has 
been implemented to assure the accurate control of the air 
pressure. In addition, the implementation of an auditory 
feedback control system has been proposed and implemented in 
order to adjust the positioning of the physical parameters of the 
components of the robot by providing a pitch feedback and 
defining a recovery position (off-line). A set of experiments 
were carried out to verify the mechanical design improvements 
and the dynamic response of the air pressure. As a result, the 
range of sound pressure has been increased and the proposed 
control system improved the dynamic response of the air 
pressure control. 

</abstract>
    <keywords>  Humanoid Robot, Auditory Feedback, Music, Saxophone.   </keywords>
  </document>
  <document>
    <name>nime2010_162.pdf</name>
    <abstract> 
This paper describes the making of a class to teach the 
history and art of musical robotics. The details of the 
curriculum are described as well as designs for our 
custom schematics for robotic solenoid driven 
percussion. This paper also introduces four new robotic 
instruments that were built during the term of this course. 
This paper also introduces the Machine Orchestra, a 
laptop orchestra with ten human performers and our five 
robotic instruments.     

</abstract>
  </document>
  <document>
    <name>nime2010_166.pdf</name>
    <abstract>
This paper proposes a novel method to realize an initiative
exchange for robot. A humanoid robot plays vibraphone ex-
changing initiative with a human performer by perceiving
multimodal cues in real time. It understands the initia-
tive exchange cues through vision and audio information.
In order to achieve the natural initiative exchange between
a human and a robot in musical performance, we built the
system and the software architecture and carried out the ex-
periments for fundamental algorithms which are necessary
to the initiative exchange.

</abstract>
    <keywords> Human-robot interaction, initiative exchange, prediction  </keywords>
  </document>
  <document>
    <name>nime2010_174.pdf</name>
    <abstract>
The Mobile Music (MoMu) toolkit is a new open-source
software development toolkit focusing on musical interac-
tion design for mobile phones. The toolkit, currently im-
plemented for iPhone OS, emphasizes usability and rapid
prototyping with the end goal of aiding developers in cre-
ating real-time interactive audio applications. Simple and
unified access to onboard sensors along with utilities for
common tasks found in mobile music development are pro-
vided. The toolkit has been deployed and evaluated in the
Stanford Mobile Phone Orchestra (MoPhO) and serves as
the primary software platform in a new course exploring
mobile music.

</abstract>
    <keywords> instrument design, iPhone, mobile music, software develop- ment, toolkit  </keywords>
  </document>
  <document>
    <name>nime2010_178.pdf</name>
    <abstract> 
The use of metaphor has a prominent role in HCI, both as a 
device to help users understand unfamiliar technologies, and as 
a tool to guide the design process. Creators of new computer-
based instruments face similar design challenges as those in 
HCI. In the course of creating a new piece for Mobile Phone 
Orchestra we propose the metaphor of a sound as a ball and 
explore the interactions and sound mappings it suggests. These 
lead to the design of a gesture-controlled instrument that allows 
players to "bounce" sounds, "throw" them to other players, and 
compete in a game to "knock out" others' sounds. We 
composed the piece SoundBounce based on these interactions, 
and note that audiences seem to find performances of the piece 
accessible and engaging, perhaps due to the visibility of the 
metaphor. 

 

</abstract>
    <keywords>  Mobile music, design, metaphor, performance, gameplay.   </keywords>
  </document>
  <document>
    <name>nime2010_182.pdf</name>
    <abstract> 
Impact force is an important dimension for percussive musical 
instruments such as the piano. We explore three possible 
mechanisms how to get impact forces on mobile multi-touch 
devices: using built-in accelerometers, the pressure sensing 
capability of Android phones, and external force sensing 
resistors. We find that accelerometers are difficult to control for 
this purpose. Android's pressure sensing shows some promise, 
especially when combined with augmented playing technique. 
Force sensing resistors can offer good dynamic resolution but 
this technology is not currently offered in commodity devices 
and proper coupling of the sensor with the applied impact is 
difficult. 

</abstract>
    <keywords>  Force, impact, pressure, multi-touch, mobile phone, mobile  music making.   </keywords>
  </document>
  <document>
    <name>nime2010_186.pdf</name>
    <abstract> 
The evolution of networked audio technologies has created 
unprecedented opportunities for musicians to improvise with 
instrumentalists from a diverse range of cultures and 
disciplines. As network speeds increase and latency is 
consigned to history, tele-musical collaboration, and in 
particular improvisation will be shaped by new methodologies 
that respond to this potential. While networked technologies 
eliminate distance in physical space, for the remote improviser, 
this creates a liminality of experience through which their 
performance is mediated. As a first step in understanding the 
conditions arising from collaboration in networked audio 
platforms, this paper will examine selected case studies of 
improvisation in a variety of networked interfaces. The author 
will examine how platform characteristics and network 
conditions influence the process of collective improvisation and 
the methodologies musicians are employing to negotiate their 
networked experiences.  

</abstract>
  </document>
  <document>
    <name>nime2010_192.pdf</name>
    <abstract>
We present Drile, a multiprocess immersive instrument built upon
the hierarchical live-looping technique and aimed at musical per-
formance. This technique consists in creating musical trees whose
nodes are composed of sound effects applied to a musical content.
In the leaves, this content is a one-shot sound, whereas in higher-
level nodes this content is composed of live-recorded sequences
of parameters of the children nodes. Drile allows musicians to
interact efficiently with these trees in an immersive environment.
Nodes are represented as worms, which are 3D audiovisual ob-
jects. Worms can be manipulated using 3D interaction techniques,
and several operations can be applied to the live-looping trees. The
environment is composed of several virtual rooms, i.e. group of
trees, corresponding to specific sounds and effects. Learning Drile
is progressive since the musical control complexity varies accord-
ing to the levels in live-looping trees. Thus beginners may have
limited control over only root worms while still obtaining musi-
cally interesting results. Advanced users may modify the trees and
manipulate each of the worms.

</abstract>
    <keywords> Drile, immersive instrument, hierarchical live-looping, 3D interac- tion  </keywords>
  </document>
  <document>
    <name>nime2010_198.pdf</name>
    <abstract>
This research is concerned with issues of privacy, aware-
ness and the emergence of roles in the process of digitally
mediated collaborative music making. Specifically we are
interested in how providing collaborators with varying de-
grees of privacy and awareness of one another influences
the group interaction. A study is presented whereby nine
groups of co-located musicians compose music together us-
ing three different interface designs. We use qualitative and
quantitative data to study and characterise the musician's
interaction with each other and the software. We show that
when made available to them, participants make extensive
use of a private working area to develop musical contribu-
tions before they are introduced to the group. We also argue
that our awareness mechanisms change the perceived qual-
ity of the musical interaction, but have no impact on the
way musicians interact with the software. We then reflect
on implications for the design of new collaborative music
making tools which exploit the potential of digital tech-
nologies, while at the same time support creative musical
interaction.

</abstract>
    <keywords> Awareness, Privacy, Collaboration, Music, Interaction, En- gagement, Group Music Making, Design, Evaluation.  </keywords>
  </document>
  <document>
    <name>nime2010_204.pdf</name>
    <abstract>
In 2009 the cross artform group, Last Man to Die, presented
a series of performances using new interfaces and networked
performance to integrate the three artforms of its members
(actor, Hanna Cormick, visual artist, Benjamin Forster and
percussionist, Charles Martin). This paper explains our
artistic motivations and design for a computer vision surface
and networked heartbeat sensor as well as the experience of
mounting our first major work, Vital LMTD.

</abstract>
    <keywords> cross-artform performance, networked performance, physi- cal computing  </keywords>
  </document>
  <document>
    <name>nime2010_208.pdf</name>
    <abstract>
We report on a study of perceptual and acoustic features
related to the placement of microphones around a custom
made glass instrument. Different microphone setups were
tested: above, inside and outside the instrument and at dif-
ferent distances. The sounds were evaluated by an expert
performer, and further qualitative and quantitative anal-
yses have been carried out. Preference was given to the
recordings from microphones placed close to the rim of the
instrument, either from the inside or the outside.

</abstract>
    <keywords> glass instruments, microphone placement, sound analysis  </keywords>
  </document>
  <document>
    <name>nime2010_212.pdf</name>
    <keywords>  Hyper-Instruments, Glitch Music, Interactive Systems,  Electronic Music Performance.    </keywords>
  </document>
  <document>
    <name>nime2010_217.pdf</name>
    <abstract>
This paper presents the magnetic resonator piano, an aug-
mented instrument enhancing the capabilities of the acous-
tic grand piano. Electromagnetic actuators induce the strings
to vibration, allowing each note to be continuously con-
trolled in amplitude, frequency, and timbre without exter-
nal loudspeakers. Feedback from a single pickup on the
piano soundboard allows the actuator waveforms to remain
locked in phase with the natural motion of each string. We
also present an augmented piano keyboard which reports
the continuous position of every key. Time and spatial res-
olution are sufficient to capture detailed data about key
press, release, pretouch, aftertouch, and other extended ges-
tures. The system, which is designed with cost and setup
constraints in mind, seeks to give pianists continuous con-
trol over the musical sound of their instrument. The in-
strument has been used in concert performances, with the
electronically-actuated sounds blending with acoustic in-
struments naturally and without amplification.

</abstract>
    <keywords> Augmented instruments, piano, interfaces, electromagnetic actuation, gesture measurement  </keywords>
  </document>
  <document>
    <name>nime2010_223.pdf</name>
    <abstract> 
In this paper I describe aspects that have been involved in my 
experience of developing a hybrid instrument. The process of 
transformation and extension of the instrument is informed by 
ideas concerning the intrinsic communication aspects of 
musical activities. Decisions taken for designing the instrument 
and performing with it take into account the hypothesis that 
there are ontological levels of human reception in music that are 
related to the intercorporeal. Arguing that it is necessary to 
encounter resistances for achieving expression, it is suggested 
that new instrumental development ought to reflect on the 
concern for keeping the natural connections of live 
performances. 
</abstract>
  </document>
  <document>
    <name>nime2010_229.pdf</name>
    <abstract>
This paper presents a virtual violin for real-time perfor-
mances consisting of two modules: a violin spectral model
and a control interface. The interface is composed by a
sensing bow and a tube with drawn strings in substitution
of a real violin. The spectral model is driven by the bowing
controls captured with the control interface and it is able
to predict spectral envelopes of the sound corresponding to
those controls. The envelopes are filled with harmonic and
noisy content and given to an additive synthesizer in order
to produce violin sounds. The sensing system is based on
two motion trackers with 6 degrees of freedom. One tracker
is attached to the bow and the other to the tube. Bowing
controls are computed after a calibration process where the
position of virtual strings and the hair-ribbon of the bow
is obtained. A real time implementation was developed as
a MAX/MSP patch with external objects for each of the
modules.

</abstract>
    <keywords> violin, synthesis, control, spectral, virtual  </keywords>
  </document>
  <document>
    <name>nime2010_233.pdf</name>
    <abstract> 
This research is an initial effort in showing how a multimodal 
approach can improve systems for gaining insight into a 
musician's practice and technique. Embedding a variety of 
sensors inside musical instruments and synchronously recording 
the sensors' data along with audio, we gather a database of 
gestural information from multiple performers, then use 
machine-learning techniques to recognize which musician is 
performing. Our multimodal approach (using both audio and 
sensor data) yields promising performer classification results, 
which we see as a first step in a larger effort to gain insight into 
musicians' practice and technique.  

</abstract>
    <keywords> Performer Recognition, Multimodal, HCI, Machine  Learning, Hyperinstrument, eSitar   </keywords>
  </document>
  <document>
    <name>nime2010_238.pdf</name>
    <abstract>
In this paper, we present our research on the acquisition
of gesture information for the study of the expressiveness
in guitar performances. For that purpose, we design a sen-
sor system which is able to gather the movements from left
hand fingers. Our effort is focused on a design that is (1)
non-intrusive to the performer and (2) able to detect from
strong movements of the left hand to subtle movements of
the fingers. The proposed system is based on capacitive sen-
sors mounted on the fingerboard of the guitar. We present
the setup of the sensor system and analyze its response to
several finger movements.

</abstract>
    <keywords> Guitar; Gesture acquisition; Capacitive sensors  </keywords>
  </document>
  <document>
    <name>nime2010_244.pdf</name>
    <abstract>
The design of an unusually simple fabric-based touch
location and pressure sensor is introduced. An analysis
of the raw sensor data is shown to have significant non-
linearities and non-uniform noise. Using support vector
machine learning and a state-dependent adaptive filter it
is demonstrated that these problems can be overcome.
The method is evaluated quantitatively using a statistical
estimate of the instantaneous rate of information transfer.
The SVM regression alone is shown to improve the gesture
signal information rate by up to 20% with zero added
latency, and in combination with filtering by 40% subject
to a constant latency bound of 10 milliseconds.

</abstract>
  </document>
  <document>
    <name>nime2010_250.pdf</name>
    <abstract> 
Mapping in interactive dance performance poses a number of 
questions related to the perception and expression of gestures in 
contrast to pure motion-detection and analysis. A specific 
interactive dance project is discussed, in which two 
complementary sensing modes are integrated to obtain higher-
level expressive gestures. These are applied to a modular non-
linear composition, in which the exploratory dance performance 
assumes the role of instrumentalist and conductor. The 
development strategies and methods for each of the involved 
artists are discussed and the software tools and wearable 
devices that were developed for this project are presented. 

</abstract>
    <keywords>  Mapping, motion sensing, computer vision, artistic strategies,  wearable sensors, mapping tools, splines, delaunay tessellation.   </keywords>
  </document>
  <document>
    <name>nime2010_255.pdf</name>
    <abstract> 
GIIMP addresses the criticism that in many interactive music 
systems the machine simply reacts. Interaction is addressed by 
extending Winkler's [18] model toward adapting Paine's [10] 
conversational model of interaction. Realized using commercial 
tools, GIIMP implements a machine/human generative 
improvisation system using human gesture input, machine 
gesture capture, and a gesture mutation module in conjunction 
with a flocking patch, mapped through microtonal/spectral 
techniques to sound.  The intention is to meld some established 
and current practices, and combine aspects of symbolic and 
sub-symbolic approaches, toward musical outcomes. 

</abstract>
    <keywords>  Interaction, gesture, genetic algorithm, flocking, improvisation.   </keywords>
  </document>
  <document>
    <name>nime2010_263.pdf</name>
    <abstract> 
"VirtualPhilharmony" (V.P.) is a conducting interface that 
enables users to perform expressive music with conducting 
action. Several previously developed conducting interfaces do 
not satisfy users who have conducting experience because the 
feedback from the conducting action does not always 
correspond with a natural performance. The tempo scheduler, 
which is the main engine of a conducting system, must be 
improved. V.P. solves this problem by introducing heuristics of 
conducting an orchestra in detecting beats, applying rules 
regarding the tempo expression in a bar, etc. We confirmed 
with users that the system realized a high "following" 
performance and had musical persuasiveness. 

</abstract>
    <keywords>  Conducting system, heuristics, sensor, template.   </keywords>
  </document>
  <document>
    <name>nime2010_271.pdf</name>
    <abstract>
Pressure, motion, and gesture are important parameters in
musical instrument playing. Pressure sensing allows to in-
terpret complex hidden forces, which appear during playing
a musical instrument. The combination of our new sensor
setup with pattern recognition techniques like the lately de-
veloped ordered means models allows fast and precise recog-
nition of highly skilled playing techniques. This includes left
and right hand analysis as well as a combination of both. In
this paper we show bow position recognition for string in-
struments by means of support vector regression machines
on the right hand finger pressure, as well as bowing recog-
nition and inaccurate playing detection with ordered means
models. We also introduce a new left hand and chin pressure
sensing method for coordination and position change anal-
ysis. Our methods in combination with our audio, video,
and gesture recording software can be used for teaching
and exercising. Especially studies of complex movements
and finger force distribution changes can benefit from such
an approach. Practical applications include the recognition
of inaccuracy, cramping, or malposition, and, last but not
least, the development of augmented instruments and new
playing techniques.

</abstract>
  </document>
  <document>
    <name>nime2010_277.pdf</name>
    <abstract>
As one of the main expressive feature in music, articulation
affects a wide range of tone attributes. Based on experimen-
tal recordings we analyzed human articulation in the late
Baroque style. The results are useful for both the under-
standing of historically informed performance practices and
further progress in synthetic performance generation. This
paper reports of our findings and the implementation in a
performance system. Because of its flexibility and univer-
sality the system allows more than Baroque articulation.

</abstract>
    <keywords> Expressive Performance, Articulation, Historically Informed Performance  </keywords>
  </document>
  <document>
    <name>nime2010_283.pdf</name>
    <abstract>
Generative music systems can be played by musicians who 
manipulate the values of algorithmic parameters, and their data-
centric nature provides an opportunity for coordinated 
interaction amongst a group of systems linked over IP 
networks; a practice we call Network Jamming. This paper 
outlines the characteristics of this networked performance 
practice and discusses the types of mediated musical 
relationships and ensemble configurations that can arise. We 
have developed and tested the jam2jam network jamming 
software over recent years. We describe this system, draw from 
our experiences  with it, and use it to illustrate some 
characteristics of Network Jamming.

</abstract>
  </document>
  <document>
    <name>nime2010_287.pdf</name>
    <abstract>
The paper reports on the development of prototypes of glass
instruments. The focus has been on developing acoustic
instruments specifically designed for electronic treatment,
and where timbral qualities have had priority over pitch.
The paper starts with a brief historical overview of glass
instruments and their artistic use. Then follows an overview
of the glass blowing process. Finally the musical use of the
instruments is discussed.

</abstract>
  </document>
  <document>
    <name>nime2010_291.pdf</name>
    <abstract>
Input devices for controlling music software can benefit from
exploiting the use of perceptual-motor skill in interaction.
The project described here is a new musical controller, de-
signed with the aim of enabling intuitive and nuanced in-
teraction through direct physical manipulation of malleable
material.

The controller is made from conductive foam. This foam
changes electrical resistance when deformed; the controller
works by measuring resistance at multiple points in a sin-
gle piece of foam in order to track its shape. These mea-
surements are complex and interdependent so an echo state
network, a form of recurrent neural network, is employed to
translate the sensor readings into usable control data.

A cube shaped controller was built and evaluated in the
context of the haptic exploration of sound synthesis param-
eter spaces. Eight participants experimented with the con-
troller and were interviewed about their experiences. The
controller achieves its aim of enabling intuitive interaction,
but in terms of nuanced interaction, accuracy and repeata-
bility were issues for some participants. It's not clear from
the short evaluation study whether these issues would im-
prove with practice, a longitudinal study that gives musi-
cians time to practice and find the creative limitations of
the controller would help to evaluate this fully.

The evaluation highlighted interesting issues concerning
the high level nature of malleable control and different ap-
proaches to sonic exploration.

</abstract>
    <keywords> Musical Controller, Reservoir Computing, Human Computer Interaction, Tangible User Interface, Evaluation  </keywords>
  </document>
  <document>
    <name>nime2010_297.pdf</name>
    <abstract>
The number of artists who express themselves through mu-
sic in an unconventional way is constantly growing. This
trend strongly depends on the high diffusion of laptops,
which proved to be powerful and flexible musical devices.
However laptops still lack in flexible interface, specifically
designed for music creation in live and studio performances.
To resolve this issue many controllers have been developed,
taking into account not only the performer's needs and
habits during music creation, but also the audience desire to
visually understand how performer's gestures are linked to
the way music is made. According to the common need of
adaptable visual interface to manipulate music, in this pa-
per we present a custom tridimensional controller, based on
Open Sound Control protocol and completely designed to
work inside Virtual Reality: simple geometrical shapes can
be created to directly control loop triggering and parameter
modification, just using free hand interaction.

</abstract>
    <keywords> Glove device, Music controller, Virtual Reality, OSC, con- trol mapping  </keywords>
  </document>
  <document>
    <name>nime2010_303.pdf</name>
    <abstract>
The sound card anno 2010, is an ubiquitous part of almost
any personal computing system; what was once considered
a high-end, CD-quality audio fidelity, is today found in most
common sound cards. The increased presence of multichan-
nel devices, along with the high sampling frequency, makes
the sound card desirable as a generic interface for acquisi-
tion of analog signals in prototyping of sensor-based music
interfaces. However, due to the need for coupling capacitors
at a sound card's inputs and outputs, the use as a generic
signal interface of a sound card is limited to signals not car-
rying information in a constant DC component. Through a
revisit of a card design for the (now defunct) ISA bus, this
paper proposes use of analog gates for bypassing the DC
filtering input sections, controllable from software - thereby
allowing for arbitrary choice by the user, if a soundcard
input channel is to be used as a generic analog-to-digital
sensor interface. Issues regarding use of obsolete technol-
ogy and educational aspects are discussed as well.

</abstract>
  </document>
  <document>
    <name>nime2010_309.pdf</name>
    <abstract>
Most new digital musical interfaces have evolved upon the
intuitive idea that there is a causality between sonic output
and physical actions. Nevertheless, the advent of brain-
computer interfaces (BCI) now allows us to directly access
subjective mental states and express these in the physical
world without bodily actions. In the context of an interac-
tive and collaborative live performance, we propose to ex-
ploit novel brain-computer technologies to achieve unmedi-
ated brain control over music generation and expression.
We introduce a general framework for the generation, syn-
chronization and modulation of musical material from brain
signal and describe its use in the realization of Xmotion, a
multimodal performance for a "brain quartet".

</abstract>
    <keywords> Brain-computer Interface, Biosignals, Interactive Music Sys- tem, Collaborative Musical Performance  </keywords>
  </document>
  <document>
    <name>nime2010_315.pdf</name>
    <abstract> 

This paper explores the evolution of collaborative, multi-user, 
musical interfaces developed for the Bricktable interactive 
surface.  Two key types of applications are addressed: user 
interfaces for artistic installation and interfaces for musical 
performance. In describing our software, we provide insight on 
the methodologies and practicalities of designing interactive 
musical systems for tabletop surfaces. Additionally, subtleties 
of working with custom-designed tabletop hardware are 
addressed.  

</abstract>
    <keywords> Bricktable, Multi-touch Interface, Tangible  Interface, Generative Music, Music Information Retrieval   </keywords>
  </document>
  <document>
    <name>nime2010_319.pdf</name>
    <abstract> 
This paper introduces the concept of composing expressive 
music using the principles of Fuzzy Logic.  The paper provides 
a conceptual model of a musical work which follows 
compositional decision making processes.  Significant features 
of this Fuzzy Logic framework are its inclusiveness through the 
consideration of all the many and varied musical details, while 
also incorporating the imprecision that characterises musical 
terminology and discourse.   

A significant attribute of my Fuzzy Logic method is that it 
traces the trajectory of all musical details, since it is both the 
individual elements and their combination over time which is 
significant to the effectiveness of a musical work in achieving 
its goals. 

The goal of this work is to find a set of elements and rules, 
which will ultimately enable the construction of a genralised 
algorithmic compositional system which can produce 
expressive music if so desired. 

</abstract>
  </document>
  <document>
    <name>nime2010_323.pdf</name>
    <abstract> 
In this paper we examine a wearable sonification and 

visualisation display that uses physical analogue visualisation 

and digital sonification to convey feedback about the wearer's 

activity and environment. Intended to bridge a gap between art 

aesthetics, fashionable technologies and informative physical 

computing, the user experience evaluation reveals the wearers' 

responses and understanding of a novel medium for wearable 

expression. The study reveals useful insights for wearable 

device design in general and future iterations of this sonification 

and visualisation display. 

</abstract>
    <keywords>  Wearable display, sonification, visualisation, design aesthetics,   physical computing, multimodal expression, bimodal display   </keywords>
  </document>
  <document>
    <name>nime2010_327.pdf</name>
    <abstract> 
In this paper, we describe the shaping factors, which simplify 
and  help us understand the multi-dimensional aspects of 
designing Wearable Expressions. These descriptive shaping 
factors contribute to both the design and user-experience 
evaluation of Wearable Expressions. 

</abstract>
    <keywords>  Wearable expressions, body, user-centered design.   </keywords>
  </document>
  <document>
    <name>nime2010_331.pdf</name>
    <abstract>
In this paper, we discuss the musical potential of COM-
Path - an online map based music-making tool - as a novel
and unique interface for interactive music composition and
performance. COMPath provides an intuitive environment
for creative music making by sonification of georeferenced
data. Users can generate musical events with simple and
familiar actions on an online map interface; a set of lo-
cal information is collected along the user-drawn route and
then interpreted as sounds of various musical instruments.
We discuss the musical interpretation of routes on a map,
review the design and implementation of COMPath, and
present selected sonification results with focus on mapping
strategies for map-based composition.

</abstract>
    <keywords> Musical sonification, map interface, online map service, geo- referenced data, composition, mashup  </keywords>
  </document>
  <document>
    <name>nime2010_339.pdf</name>
    <abstract>
This paper proposes a design concept for a tangible interface for
collaborative performances that incorporates two social factors
present during performance, the individual creation and
adaptation of technology and the sharing of it within a
community. These factors are identified using the example of a
laptop ensemble and then applied to three existing collaborative
performance paradigms. Finally relevant technology, challenges
and the current state of our implementation are discussed.

</abstract>
    <keywords> Tangible User Interfaces, collaborative performances, social factors  </keywords>
  </document>
  <document>
    <name>nime2010_343.pdf</name>
    <abstract>
We present two complementary approaches for the visual-
ization and interaction of dimensionally reduced data sets
using hybridization interfaces. Our implementations privi-
lege syncretic systems allowing one to explore combinations
(hybrids) of disparate elements of a data set through their
placement in a 2-D space. The first approach allows for the
placement of data points anywhere on the plane according
to an anticipated performance strategy. The contribution
(weight) of each data point varies according to a power func-
tion of the distance from the control cursor. The second
approach uses constrained vertex colored triangulations of
manifolds with labels placed at the vertices of triangular
tiles. Weights are computed by barycentric projection of
the control cursor position.

</abstract>
    <keywords> Interpolation, dimension reduction, radial basis functions, triangular mesh  </keywords>
  </document>
  <document>
    <name>nime2010_348.pdf</name>
    <keywords> Generative music, mobile interfaces, multitouch interaction   </keywords>
  </document>
  <document>
    <name>nime2010_352.pdf</name>
    <keywords>  Interactive music interface, calligraphy, graphical music  composing, sonification   </keywords>
  </document>
  <document>
    <name>nime2010_356.pdf</name>
    <abstract>
The sponge is an interface that allows a clear link to be
established between gesture and sound in electroacoustic
music. The goals in developing the sponge were to rein-
troduce the pleasure of playing and to improve the interac-
tion between the composer/performer and the audience. It
has been argued that expenditure of effort or energy is re-
quired to obtain expressive interfaces. The sponge favors an
energy-sound relationship in two ways : 1) it senses accel-
eration, which is closely related to energy; and 2) it is made
out of a flexible material (foam) that requires effort to be
squeezed or twisted. Some of the mapping strategies used
in a performance context with the sponge are discussed.

</abstract>
    <keywords> Interface, electroacoustic music, performance, expressivity, mapping  </keywords>
  </document>
  <document>
    <name>nime2010_360.pdf</name>
    <abstract>
In this paper we discuss SurfaceMusic, a tabletop music sys-
tem in which touch gestures are mapped to physical models
of instruments. With physical models, parametric control
over the sound allows for a more natural interaction be-
tween gesture and sound. We discuss the design and im-
plementation of a simple gestural interface for interacting
with virtual instruments and a messaging system that con-
veys gesture data to the audio system.

</abstract>
    <keywords> Tabletop, multi-touch, gesture, physical model, Open Sound Control.  </keywords>
  </document>
  <document>
    <name>nime2010_364.pdf</name>
    <abstract>
Many musical instruments have interfaces which emphasise
the pitch of the sound produced over other perceptual char-
acteristics, such as its timbre. This is at odds with the mu-
sical developments of the last century. In this paper, we
introduce a method for replacing the interface of musical
instruments (both conventional and unconventional) with
a more flexible interface which can present the intrument's
available sounds according to variety of different perceptual
characteristics, such as their brightness or roughness. We
apply this method to an instrument of our own design which
comprises an electro-mechanically controlled electric guitar
and amplifier configured to produce feedback tones.

</abstract>
    <keywords> Concatenative Synthesis, Feedback, Guitar  </keywords>
  </document>
  <document>
    <name>nime2010_368.pdf</name>
    <abstract>
This paper presents a comparison of different configurations
of a wireless sensor system for capturing human motion.
The systems consist of sensor elements which wirelessly
transfers motion data to a receiver element. The sensor
elements consist of a microcontroller, accelerometer(s) and
a radio transceiver. The receiver element consists of a radio
receiver connected through a microcontroller to a computer
for real time sound synthesis. The wireless transmission be-
tween the sensor elements and the receiver element is based
on the low rate IEEE 802.15.4/ZigBee standard.

A configuration with several accelerometers connected by
wire to a wireless sensor element is compared to using multi-
ple wireless sensor elements with only one accelerometer in
each. The study shows that it would be feasable to connect
5-6 accelerometers in the given setups.

Sensor data processing can be done in either the receiver
element or in the sensor element. For various reasons it can
be reasonable to implement some sensor data processing in
the sensor element. The paper also looks at how much time
that typically would be needed for a simple pre-processing
task.

</abstract>
    <keywords> wireless communication, ZigBee, microcontroller  </keywords>
  </document>
  <document>
    <name>nime2010_372.pdf</name>
    <abstract> 
The past decade has seen an increase of low-cost technology for 
sensor data acquisition, which has been utilized for the 
expanding field of research in gesture measurement for music 
performance. Unfortunately, these devices are still far from 
being compatible with the audiovisual recording platforms 
which have been used to record synchronized streams of data. 
In this paper, we describe a practical solution for simultaneous 
recording of heterogeneous multimodal signals. The recording 
system presented uses MIDI Time Code to time-stamp sensor 
data and to synchronize with standard video and audio 
recording systems. We also present a set of tools for recording 
sensor data, as well as a set of analysis tools to evaluate in real-
time the sample rate of different signals, and the overall 
synchronization status of the recording system. 

</abstract>
    <keywords>  Synchronization, Multimodal Signals, Sensor Data Acquisition,  Signal Recording.   </keywords>
  </document>
  <document>
    <name>nime2010_375.pdf</name>
    <abstract>
This paper describes the development of an interactive 3D
audio/visual and network installation entitled POLLEN.
Specifically designed for large computer Laboratories, the
artwork explores the regeneration of those spaces through
the creation of a fully immersive multimedia art experience.

The paper describes the technical, aesthetic and educa-
tional development of the piece.

</abstract>
    <keywords> Interactive, Installation, Network, 3D Physics Emulator, Educational Tools, Public Spaces, Computer Labs, Sound Design, Site-Specific Art  </keywords>
  </document>
  <document>
    <name>nime2010_377.pdf</name>
    <abstract> 
 
Irregular Incurve is a MIDI controllable robotic string instrument. 
The twelve independent string-units compose the complete 
musical scale of 12 units. Each string can be plucked by a motor 
control guitar pick. A MIDI keyboard is attached to the instrument 
and serves as an interface for real-time interactions between the 
instrument and the audience. Irregular Incurve can also play pre-
programmed music by itself. This paper presents the design 
concept and the technical solutions to realizing the functionality 
of Irregular Incurve. The future features are also discussed.   

</abstract>
    <keywords>    NIME, Robotics, Acoustic, Interactive, MIDI, Real time  Performance, String Instrument, Arduino, Servo, Motor Control      </keywords>
  </document>
  <document>
    <name>nime2010_380.pdf</name>
    <abstract> 
Peacock is a newly designed interface for improvisational 
performances. The interface is equipped with thirty-five 
proximity sensors arranged in five rows and seven columns. 
The sensors detect the movements of  a performer's hands and 
arms in a three-dimensional space above them. The interface 
digitizes the output of the sensors into sets of high precision 
digital packets, and sends them to a patch running in Pd-
extended with a sufficiently high bandwidth for performances 
with almost no computational resource consumption in Pd. 
The precision, speed, and efficiency of the system enable the 
sonification of hand gestures in realtime without the need to 
attach any physical devices to the performer's body. This 
paper traces the interface's evolution, discussing relevant 
technologies, hardware construction, system design, and input 
monitoring. 

</abstract>
    <keywords>  Musical interface, Sensor technologies, Computer music,  Hardware and software design   </keywords>
  </document>
  <document>
    <name>nime2010_383.pdf</name>
    <abstract> 
Music recommendation systems can observe user's personal 
preferences and suggest new tracks from a large online catalog. 
In the case of context-aware recommenders, user's current 
emotional state plays an important role. One simple way to 
visualize emotions and moods is graphical emoticons. In this 
study, we researched a high-level mapping between genres, as 
descriptions of music, and emoticons, as descriptions of 
emotions and moods. An online questionnaire with 87 
participants was arranged. Based on the results, we present a list 
of genres that could be used as a starting point for making 
recommendations fitting the current mood of the user.  

</abstract>
    <keywords>  Music, music recommendation, context, facial expression,  mood, emotion, emoticon, and musical genre.   </keywords>
  </document>
  <document>
    <name>nime2010_387.pdf</name>
    <abstract>
This paper is a report on the development of a new musical 
instrument in which the main concept is "Untouchable". The 
key concept of this instrument is "sound generation by body 
gesture (both hands)" and "sound generation by kneading with 
hands". The new composition project had completed as  the 
premiere of a new work "controllable untouchableness" with 
this new instrument in December 2009.

</abstract>
    <keywords> Theremin, untouchable, distance sensor, Propeller processor  </keywords>
  </document>
  <document>
    <name>nime2010_391.pdf</name>
    <abstract> 
This paper describes the design, implementation and outcome 
of Ground Me!, an interactive sound installation set up in the 
Sonic Lab of the Sonic Arts Research Centre. The site-specific 
interactive installation consists of multiple copper poles 
hanging from the Sonic Lab's ceiling panels, which trigger 
samples of electricity sounds when grounded through the 
visitor's' body to the space's metallic floor. 

</abstract>
    <keywords>  Interactive sound installation, body impedance, skin  conductivity, site-specific sound installation, human network,  Sonic Lab, Arduino.   </keywords>
  </document>
  <document>
    <name>nime2010_395.pdf</name>
    <abstract>
This paper presents Mmmmm; a Multimodal Mobile Music
Mixer that provides DJs a new interface for mixing music
on the Nokia N900 phones. Mmmmm presents a novel way
for DJ to become more interactive with their audience and
vise versa. The software developed for the N900 mobile
phone utilizes the phones built-in accelerometer sensor and
Bluetooth audio streaming capabilities to mix and apply ef-
fects to music using hand gestures and have the mixed audio
stream to Bluetooth speakers, which allows the DJ to move
about the environment and get familiarized with their au-
dience, turning the experience of DJing into an interactive
and audience engaging process.

Mmmmm is designed so that the DJ can utilize hand
gestures and haptic feedback to help them perform the var-
ious tasks involved in DJing (mixing, applying effects, and
etc). This allows the DJ to focus on the crowd, thus pro-
viding the DJ a better intuition of what kind of music or
musical mixing style the audience is more likely to enjoy
and engage with. Additionally, Mmmmm has an "Ambi-
ent Tempo Detection mode in which the phones camera is
utilized to detect the amount of movement in the environ-
ment and suggest to the DJ the tempo of music that should
be played. This mode utilizes frame differencing and pixel
change overtime to get a sense of how fast the environment
is changing, loosely correlating to how fast the audience is
dancing or the lights are flashing in the scene. By deter-
mining the ambient tempo of the environment the DJ can
get a better sense for the type of music that would fit best
for their venue.

Mmmmm helps novice DJs achieve a better music reper-
toire by allowing them to interact with their audience and
receive direct feedback on their performance. The DJ can
choose to utilize these modes of interaction and perfor-
mance or utilize traditional DJ controls using Mmmmms
N900 touch screen based graphics user interface.

</abstract>
    <keywords> Multi-modal, interaction, music, mixer, mobile, interactive, DJ, smart phones, Nokia, n900, touch screen, accelerome- ter, phone, audience  </keywords>
  </document>
  <document>
    <name>nime2010_399.pdf</name>
    <abstract> 
With the decreasing audience of classical music performance, 
this research aims to develop a performance-enhancement 
system, called AIDA, to help classical performers better 
communicating with their audiences. With three procedures 
Input-Processing-Output, AIDA system can sense and analyze 
the body information of performers and further reflect it onto 
the responsive skin. Thus abstract and intangible emotional 
expressions of performers are transformed into tangible and 
concrete visual elements, which clearly facilitating the 
audiences' threshold for music appreciation. 

</abstract>
    <keywords>  Interactive Performance, Ambient Environment, Responsive  Skin, Music performance.   </keywords>
  </document>
  <document>
    <name>nime2010_403.pdf</name>
    <abstract> 
In this paper we outline the emerging field of Interactional 
Sound and Music which concerns itself with multi-person 
technologically mediated interactions primarily using audio. We 
present several examples of interactive systems in our group, 
and reflect on how they were designed and evaluated. 
Evaluation techniques for collective, performative, and task 
oriented activities are outlined and compared. We emphasise 
the importance of designing for awareness in these systems, and 
provide examples of different awareness mechanisms. 

</abstract>
    <keywords>  Interactional, sound, music, mutual engagement, improvisation,  composition, collaboration, awareness.   </keywords>
  </document>
  <document>
    <name>nime2010_411.pdf</name>
    <abstract>
In this study artistic human-robot interaction design is in-
troduced as a means for scientific research and artistic inves-
tigations. It serves as a methodology for situated cognition
integrating empirical methodology and computational mod-
eling, and is exemplified by the installation playing robot.
Its artistic purpose is to aid to create and explore robots as a
new medium for art and entertainment. We discuss the use
of finite state machines to organize robots' behavioral reac-
tions to sensor data, and give a brief outlook on structured
observation as a potential method for data collection.

</abstract>
  </document>
  <document>
    <name>nime2010_415.pdf</name>
    <abstract>
This project aims at studying how recent interactive and in-
teractions technologies would help extend how we play the
guitar, thus defining the "multimodal guitar". Our contri-
butions target three main axes: audio analysis, gestural con-
trol and audio synthesis. For this purpose, we designed and
developed a freely-available toolbox for augmented guitar
performances, compliant with the PureData and Max/MSP
environments, gathering tools for: polyphonic pitch estima-
tion, fretboard visualization and grouping, pressure sensing,
modal synthesis, infinite sustain, rearranging looping and
"smart" harmonizing.

</abstract>
    <keywords> Augmented guitar, audio synthesis, digital audio effects, multimodal interaction, gestural sensing, polyphonic tran- scription, hexaphonic guitar  </keywords>
  </document>
  <document>
    <name>nime2010_419.pdf</name>
    <abstract> 
This paper introduces my research in physical interactive design 
with my "GRIP MAESTRO" electroacoustic performance 
interface. It then discusses the considerations involved in 
creating intuitive software mappings of emotive performative 
gestures such that they are idiomatic not only of the sounds they 
create but also of the physical nature of the interface itself.   

</abstract>
  </document>
  <document>
    <name>nime2010_423.pdf</name>
    <abstract> 
In this paper, we present an interactive system that uses the 
body as a generative tool for creating music. We explore 
innovative ways to make music, create self-awareness, and 
provide the opportunity for unique, interactive social 
experiences. The system uses a multi-player game paradigm, 
where players work together to add layers to a soundscape of 
three distinct environments. Various sensors and hardware are 
attached to the body and transmit signals to a workstation, 
where they are processed using Max/MSP. The game is divided 
into three levels, each of a different soundscape. The underlying 
purpose of our system is to move the player's focus away from 
complexities of the modern urban world toward a more 
internalized meditative state. The system is currently viewed as 
an interactive installation piece, but future iterations have 
potential applications in music therapy, bio games, extended 
performance art, and as a prototype for new interfaces for 
musical expression. 

</abstract>
  </document>
  <document>
    <name>nime2010_427.pdf</name>
    <abstract>
Maintaining a sense of personal connection between 
increasingly synthetic performers and increasingly diffuse 
audiences is vital  to  storytelling and entertainment. Sonic 
intimacy is important, because voice is one of the highest-
bandwidth channels for expressing our real and imagined 
selves.

New tools for highly focused spatialization could help improve 
acoustical clarity, encourage audience engagement, reduce 
noise pollution and inspire creative expression.  We have a 
particular interest in embodied, embedded systems for vocal 
performance enhancement and transformation. 

This short  paper describes work in progress on a toolkit  for 
high-quality wearable sound suits. Design goals  include tailored 
directionality and resonance, full bandwidth, and sensible 
ergonomics. Engineering details  to accompany a demonstration 
of recent prototypes are presented, highlighting a novel 
magnetostrictive flextensional transducer. Based on initial 
observations we suggest that vocal acoustic output from the 
torso, and spatial perception of situated low frequency sources, 
are two areas deserving greater attention and further study.

</abstract>
  </document>
  <document>
    <name>nime2010_431.pdf</name>
    <abstract>
The Ghost has been developed to create a merger between 
the standard MIDI keyboard controller, MIDI/digital guitars 
and alternative desktop controllers.  Using a custom software 
editor, The Ghost's controls can be mapped to suit the users 
performative needs.  The interface takes its interaction and 
gestural cues  from the guitar but  it is not a MIDI guitar.  The 
Ghost's hardware, firmware and software will be open 
sourced with the hopes of creating a community of users that 
are invested in creating music with controller.

</abstract>
    <keywords> Controller, MIDI, Live Performance, Programmable, Open- Source  </keywords>
  </document>
  <document>
    <name>nime2010_436.pdf</name>
    <abstract> 
This paper presents a discussion regarding organology 
classification and taxonomies for digital musical instruments 
(DMI), arising from the TIEM (Taxonomy of Interfaces for 
Electronic Music performance) survey (http://tiem.emf.org/), 
conducted as part of an Australian Research Council Linkage 
project titled "Performance Practice in New Interfaces for 
Realtime Electronic Music Performance". This research is 
being carried out at the VIPRe Lab at, the University of 
Western Sydney in partnership with the Electronic Music 
Foundation (EMF), Infusion Systems1 and The Input Devices 
and Music Interaction Laboratory (IDMIL) at McGill 
University. The project seeks to develop a schema of new 
interfaces for realtime electronic music performance. 

</abstract>
    <keywords>   Instrument, Interface, Organology, Taxonomy.   </keywords>
  </document>
  <document>
    <name>nime2010_440.pdf</name>
    <abstract> 
humanaquarium is a self-contained, transportable performance 
environment that is used to stage technology-mediated 
interactive performances in public spaces. Drawing upon the 
creative practices of busking and street performance, 
humanaquarium incorporates live musicians, real-time 
audiovisual content generation, and frustrated total internal 
reflection (FTIR) technology to facilitate participatory 
interaction by members of the public.  

</abstract>
  </document>
  <document>
    <name>nime2010_444.pdf</name>
    <keywords>  Mobile device, music composer, pattern composing, MIDI   </keywords>
  </document>
  <document>
    <name>nime2010_451.pdf</name>
    <abstract>
Drawing on a model of spectator understanding of error in
performance in the literature, we document a qualitative
experiment that explores the relationships between domain
knowledge, mental models, intention and error recognition
by spectators of performances with electronic instruments.
Participants saw two performances with contrasting instru-
ments, with controls on their mental model and understand-
ing of intention. Based on data from a subsequent struc-
tured interview, we identify themes in participants' judge-
ments and understanding of performance and explanations
of their spectator experience. These reveal both elements
of similarity and difference between the two performances,
instruments and between domain knowledge groups. From
these, we suggest and discuss implications for the design of
novel performative interactions with technology.

</abstract>
  </document>
  <document>
    <name>nime2010_455.pdf</name>
    <abstract>
Gaining access to a prototype motion capture suit designed
by the Animazoo company, the Interactive Systems group
at the University of Sussex have been investigating appli-
cation areas. This paper describes our initial experiments
in mapping the suit control data to sonic attributes for mu-
sical purposes. Given the lab conditions under which we
worked, an agile design cycle methodology was employed,
with live coding of audio software incorporating fast feed-
back, and more reflective preparations between sessions, ex-
ploiting both individual and pair programming. As the suit
provides up to 66 channels of information, we confront a
challenging mapping problem, and techniques are described
for automatic calibration, and the use of echo state networks
for dimensionality reduction.

</abstract>
    <keywords> Motion Capture, Musical Controller, Mapping, Agile De- sign  </keywords>
  </document>
  <document>
    <name>nime2010_459.pdf</name>
    <abstract> 
This paper describes a study of membrane potentiometers and 
long force sensing resistors as tools to enable greater interaction 
between performers and audiences. This is accomplished through 
the building of a new interface called the Helio. In preparation for 
the Helio's construction, a variety of brands of membrane 
potentiometers and long force sensing resistors were analyzed for 
their suitability for use in a performance interface. Analog and 
digital circuit design considerations are discussed. We discuss in 
detail the design process and performance scenarios explored 
with the Helio.  

</abstract>
    <keywords> Force Sensing Resistors, Membrane  Potentiometers, Force Sensing Resistors, Haptic Feedback, Helio  </keywords>
  </document>
  <document>
    <name>nime2010_463.pdf</name>
    <abstract> 
We present a novel user interface device based around 
ferromagnetic sensing. The physical form of the interface can 
easily be reconfigured by simply adding and removing a variety 
of ferromagnetic objects to the device's sensing surface. This 
allows the user to change the physical form of the interface 
resulting in a variety of different interaction modes. When used 
in a musical context, the performer can leverage the physical 
reconfiguration of the device to affect the method of playing 
and ultimately the sound produced. We describe the 
implementation of the sensing system, along with a range of 
mapping techniques used to transform the sensor data into 
musical output, including both the direct synthesis of sound and 
also the generation of MIDI data for use with Ableton Live. We 
conclude with a discussion of future directions for the device. 

</abstract>
    <keywords>  Ferromagnetic sensing, ferrofluid, reconfigurable user interface,  wave terrain synthesis, MIDI controller.   </keywords>
  </document>
  <document>
    <name>nime2010_467.pdf</name>
  </document>
  <document>
    <name>nime2010_469.pdf</name>
    <abstract>
We propose an online generative algorithm to enhance musi-
cal expression via intelligent improvisation accompaniment.
Our framework called the ImprovGenerator, takes a live
stream of percussion patterns and generates an improvised
accompaniment track in real-time to stimulate new expres-
sions in the improvisation. We use a mixture model to
generate an accompaniment pattern, that takes into ac-
count both the hierarchical temporal structure of the live
input patterns and the current musical context of the per-
formance. The hierarchical structure is represented as a
stochastic context-free grammar, which is used to generate
accompaniment patterns based on the history of temporal
patterns. We use a transition probability model to augment
the grammar generated pattern to take into account the
current context of the performance. In our experiments we
show how basic beat patterns performed by a percussionist
on a cajon can be used to automatically generate on-the-fly
improvisation accompaniment for live performance.

</abstract>
    <keywords> Machine Improvisation, Grammatical Induction, Stochastic Context-Free Grammars, Algorithmic Composition  </keywords>
  </document>
  <document>
    <name>nime2010_473.pdf</name>
    <abstract>
This paper presents the development of rapid and reusable
gestural interface prototypes for navigation by similarity in
an audio database and for sound manipulation, using the
AudioCycle application. For this purpose, we propose and
follow guidelines for rapid prototyping that we apply using
the PureData visual programming environment. We have
mainly developed three prototypes of manual control: one
combining a 3D mouse and a jog wheel, a second featur-
ing a force-feedback 3D mouse, and a third taking advan-
tage of the multitouch trackpad. We discuss benefits and
shortcomings we experienced while prototyping using this
approach.

</abstract>
    <keywords> Human-computer interaction, gestural interfaces, rapid pro- totyping, browsing by similarity, audio database  </keywords>
  </document>
  <document>
    <name>nime2010_477.pdf</name>
    <abstract> 
In this paper we present a novel system for tactile actuation in 
stylus-based musical interactions. The proposed controller aims 
to support rhythmical musical performance. The system builds 
on resistive force feedback, which is achieved through a brake-
augmented ball pen stylus on a sticky touch-sensitive surface. 
Along the device itself, we present musical interaction 
principles that are enabled through the aforementioned tactile 
response. Further variations of the device and perspectives of 
the friction-based feedback are outlined.  

</abstract>
  </document>
  <document>
    <name>nime2010_479.pdf</name>
    <keywords> Multi-touch Interfaces, Computer-Assisted Composition  </keywords>
  </document>
  <document>
    <name>nime2010_481.pdf</name>
    <abstract> 
In this paper, we describe a comparison between parameters 
drawn from 3-dimensional measurement of a dance 
performance, and continuous emotional response data 
recorded from an audience present during this performance. A 
continuous time series representing the mean movement as the 
dance unfolds is extracted from the 3-dimensional data. The 
audiences' continuous emotional response data are also 
represented as a time series, and the series are compared. We 
concluded that movement in the dance performance directly 
influences the emotional arousal response of the audience.  

</abstract>
    <keywords>  Dance, Emotion, Motion Capture, Continuous Response.   </keywords>
  </document>
  <document>
    <name>nime2010_485.pdf</name>
    <abstract> 
This paper investigates whether a dynamic vibrotactile feedback 
improves the playability of a gesture controlled virtual 
instrument. The instrument described in this study is based on a 
virtual control surface that player strikes with a hand held 
sensor-actuator device. We designed two tactile cues to 
augment the stroke across the control surface: a static and 
dynamic cue. The static cue was a simple burst of vibration 
triggered when crossing the control surface. The dynamic cue 
was continuous vibration increasing in amplitude when 
approaching the surface. We arranged an experiment to study 
the influence of the tactile cues in performance. In a tempo 
follow task, the dynamic cue yielded significantly the best 
temporal and periodic accuracy and control of movement 
velocity and amplitude. The static cue did not significantly 
improve the rhythmic accuracy but assisted the control of 
movement velocity compared to the condition without tactile 
feedback at all. The findings of the study indicate that careful 
design of dynamic vibrotactile feedback can improve the 
controllability of gesture based virtual instrument. 

</abstract>
    <keywords>  Virtual instrument, Gesture, Tactile feedback, Motor control   </keywords>
  </document>
  <document>
    <name>nime2010_489.pdf</name>
    <abstract> 
In his demonstration, the author discusses the sequential 
progress of his technical and aesthetic decisions as composer 
and videographer for four large-scale works for dance through 
annotated video examples of live performances and PowerPoint 
slides.  In addition, he discusses his current real-time dance 
work with wireless sensor interfaces using sewable LilyPad 
Arduino modules and Xbee radio hardware. 

Keywords 
</abstract>
    <keywords> dance, video processing, video tracking, LilyPad  Arduino.   </keywords>
  </document>
  <document>
    <name>nime2010_493.pdf</name>
    <abstract> 
This paper describes a series of mathematical functions 
implemented by the author in the commercial algorithmic 
software language ArtWonk, written by John Dunn, which are 
offered with that language as resources for composers. It gives a 
history of the development of the functions, with an emphasis 
on how I developed them for use in my compositions.  

</abstract>
    <keywords>  Algorithmic composition, mathematical composition,  probability distributions, fractals, additive sequences   </keywords>
  </document>
  <document>
    <name>nime2010_497.pdf</name>
    <abstract> 
The console gaming industry is experiencing a revolution in 
terms of user control, and a large part to Nintendo's 
introduction of the Wii remote.  The online open source 
development community has embraced the Wii remote, 
integrating the inexpensive technology into numerous 
applications.  Some of the more interesting applications 
demonstrate how the remote hardware can be leveraged for 
nonstandard uses.  In this paper we describe a new way of 
interacting with the Wii remote and sensor bar to produce 
music.  The Wiiolin is a virtual instrument which can mimic a 
violin or cello.  Sensor bar motion relative to the Wii remote 
and button presses are analyzed in real-time to generate notes.  
Our design is novel in that it involves the remote's infrared 
camera and sensor bar as an integral part of music production, 
allowing users to change notes by simply altering the angle of 
their wrist, and henceforth, bow.  The Wiiolin introduces a 
more realistic way of instrument interaction than other attempts 
that rely on button presses and accelerometer data alone.  

</abstract>
    <keywords>  Wii remote, virtual instrument, violin, cello, motion  recognition, human computer interaction, gesture recognition.   </keywords>
  </document>
  <document>
    <name>nime2010_501.pdf</name>
  </document>
  <document>
    <name>nime2011_004.pdf</name>
    <abstract> 
The Overtone Fiddle is a new violin-family instrument that 
incorporates electronic sensors, integrated DSP, and physical 
actuation of the acoustic body. An embedded tactile sound 
transducer creates extra vibrations in the body of the Overtone 
Fiddle, allowing performer control and sensation via both 
traditional violin techniques, as well as extended playing 
techniques that incorporate shared man/machine control of the 
resulting sound. A magnetic pickup system is mounted to the 
end of the fiddle's fingerboard in order to detect the signals 
from the vibrating strings, deliberately not capturing vibrations 
from the full body of the instrument. This focused sensing 
approach allows less restrained use of DSP-generated feedback 
signals, as there is very little direct leakage from the actuator 
embedded in the body of the instrument back to the pickup. 
 
</abstract>
    <keywords>  Actuated Musical Instruments, Hybrid Instruments, Active  Acoustics, Electronic Violin    </keywords>
  </document>
  <document>
    <name>nime2011_008.pdf</name>
    <keywords> multi-touch, haptics, frustrated total internal reflection, mu- sic performance, music composition, latency, DIY  </keywords>
  </document>
  <document>
    <name>nime2011_014.pdf</name>
    <abstract>
The Electromagnetically Sustained Rhodes Piano is an aug-
mentation of the original instrument with additional con-
trol over the amplitude envelope of individual notes. This
includes slow attacks and infinite sustain while preserving
the familiar spectral qualities of this classic electromechan-
ical piano. These additional parameters are controlled with
aftertouch on the existing keyboard, extending standard
piano technique. Two sustain methods were investigated,
driving the actuator first with a pure sine wave, and second
with the output signal of the sensor. A special isolation
method effectively decouples the sensors from the actuators
and tames unruly feedback in the high-gain signal path.

</abstract>
    <keywords> Rhodes, keyboard, electromagnetic, sustain, augmented in- strument, feedback, aftertouch  </keywords>
  </document>
  <document>
    <name>nime2011_018.pdf</name>
    <abstract>
This paper describes the motivation and construction of
Gamelan Elektrika, a new electronic gamelan modeled after
a Balinese Gong Kebyar. The first of its kind, Elektrika con-
sists of seven instruments acting as MIDI controllers accom-
panied by traditional percussion and played by 11 or more
performers following Balinese performance practice. Three
main percussive instrument designs were executed using a
combination of force sensitive resistors, piezos, and capaci-
tive sensing. While the instrument interfaces are designed
to play interchangeably with the original, the sound and
travel possiblilities they enable are tremendous. MIDI en-
ables a massive new sound palette with new scales beyond
the quirky traditional tuning and non-traditional sounds.
It also allows simplified transcription for an aurally taught
tradition. Significantly, it reduces the transportation chal-
lenges of a previously large and heavy ensemble, creating
opportunities for wider audiences to experience Gong Ke-
byar's enchanting sound. True to the spirit of oneness in
Balinese music, as one of the first large all-MIDI ensembles,
Elek Trika challenges performers to trust silent instruments
and develop an understanding of highly intricate and inter-
locking music not through the sound of the individual, but
through the sound of the whole.

</abstract>
    <keywords> bali, gamelan, musical instrument design, MIDI ensemble  </keywords>
  </document>
  <document>
    <name>nime2011_024.pdf</name>
    <keywords>  Stereotypical transducers, audible sound, Doppler effect, hand-  free interface, musical instrument, interactive performance   </keywords>
  </document>
  <document>
    <name>nime2011_028.pdf</name>
    <abstract> 
This paper describes recent developments in the creation of 
sound-making instruments and devices powered by 
photovoltaic (PV) technologies.  With the rise of more efficient 
PV products in diverse packages, the possibilities for creating 
solar-powered musical instruments, sound installations, and 
loudspeakers are becoming increasingly realizable.  This paper 
surveys past and recent developments in this area, including 
several projects by the author, and demonstrates how the use of 
PV technologies can influence the creative process in unique 
ways.  In addition, this paper discusses how solar sound arts 
can enhance the aesthetic direction taken by recent work in 
soundscape studies and acoustic ecology.  Finally, this paper 
will point towards future directions and possibilities as PV 
technologies continue to evolve and improve in terms of 
performance, and become more affordable. 
 
</abstract>
    <keywords>  Solar Sound Arts, Circuit Bending, Hardware Hacking,  Human-Computer Interface Design, Acoustic Ecology, Sound  Art, Electroacoustics, Laptop Orchestra, PV Technology   </keywords>
  </document>
  <document>
    <name>nime2011_032.pdf</name>
    <abstract>
This paper provides a discussion of how the electronic, solely IT
based composition and performance of electronic music can be
supported in realtime with a collaborative application on a tabletop
interface, mediating between single-user style music composition
tools and co-located collaborative music improvisation. After hav-
ing elaborated on the theoretical backgrounds of prerequisites of
co-located collaborative tabletop applications as well as the com-
mon paradigms in music composition/notation, we will review re-
lated work on novel IT approaches to music composition and im-
provisation. Subsequently, we will present our prototypical imple-
mentation and the results.

</abstract>
    <keywords> Tabletop Interface, Collaborative Music Composition, Creativity Support  </keywords>
  </document>
  <document>
    <name>nime2011_036.pdf</name>
    <abstract> 
Popular music (characterized by improvised instrumental parts, 
beat and measure-level organization, and steady tempo) poses 
challenges for human-computer music performance (HCMP).  
Pieces of music are typically rearrangeable on-the-fly and 
involve a high degree of variation from ensemble to ensemble, 
and even between rehearsal and performance.  Computer 
systems aiming to participate in such ensembles must therefore 
cope with a dynamic high-level structure in addition to the 
more traditional problems of beat-tracking, score-following, 
and machine improvisation.  There are many approaches to 
integrating the components required to implement dynamic 
human-computer music performance systems.  This paper 
presents a reference architecture designed to allow the typical 
sub-components (e.g. beat-tracking, tempo prediction, 
improvisation) to be integrated in a consistent way, allowing 
them to be combined and/or compared systematically. In 
addition, the paper presents a dynamic score representation 
particularly suited to the demands of popular music 
performance by computer. 
 
</abstract>
  </document>
  <document>
    <name>nime2011_040.pdf</name>
    <abstract> 
V'OCT(Ritual) is a work for solo vocalist/performer and 

Bodycoder System, composed in residency at Dartington 

College of Arts (UK) Easter 2010. 

 This paper looks at the technical and compositional 

methodologies used in the realization of the work, in particular, 

the choices made with regard to the mapping of sensor 

elements to various spatialization functions. Kinaesonics will 

be discussed in relation to the coding of real-time one-to-one 

mapping of sound to gesture and its expression in terms of 

hardware and software design.  Four forms of expressivity 

arising out of interactive work with the Bodycoder system will 

be identified. How sonic (electro-acoustic), programmed, 

gestural (kinaesonic) and in terms of the V'Oct(Ritual) vocal 

expressivities are constructed as pragmatic and tangible 

elements within the compositional practice will be discussed 

and the subsequent importance of collaboration with a 

performer will be exposed. 

 

</abstract>
    <keywords>  Bodycoder, Kinaesonics, Expressivity, Gestural Control,   Interactive Performance Mechanisms, Collaboration.      </keywords>
  </document>
  <document>
    <name>nime2011_044.pdf</name>
    <abstract>
First Person Shooters are among the most played computer video
games. They combine navigation, interaction and collaboration in
3D virtual environments using simple input devices, i.e. mouse
and keyboard. In this paper, we study the possibilities brought
by these games for musical interaction. We present the Couacs, a
collaborative multiprocess instrument which relies on interaction
techniques used in FPS together with new techniques adding the
expressiveness required for musical interaction. In particular, the
Faders For All game mode allows musicians to perform pattern-
based electronic compositions.

</abstract>
    <keywords> the couacs, fps, first person shooters, collaborative, 3D interaction, multiprocess instrument  </keywords>
  </document>
  <document>
    <name>nime2011_048.pdf</name>
  </document>
  <document>
    <name>nime2011_052.pdf</name>
    <keywords>  Sound installation, robotic music, interactive systems   </keywords>
  </document>
  <document>
    <name>nime2011_056.pdf</name>
    <abstract>
Many performers of novel musical instruments find it diffi-
cult to engage audiences beyond those in the field. Previous
research points to a failure to balance complexity with us-
ability, and a loss of transparency due to the detachment
of the controller and sound generator. The issue is often
exacerbated by an audience's lack of prior exposure to the
instrument and its workings.

However, we argue that there is a conflict underlying
many novel musical instruments in that they are intended
to be both a tool for creative expression and a creative work
of art in themselves, resulting in incompatible requirements.
By considering the instrument, the composition and the
performance together as a whole with careful consideration
of the rate of learning demanded of the audience, we pro-
pose that a lack of transparency can become an asset rather
than a hindrance. Our approach calls for not only controller
and sound generator to be designed in sympathy with each
other, but composition, performance and physical form too.

Identifying three design principles, we illustrate this ap-
proach with the Serendiptichord, a wearable instrument for
dancers created by the authors.

</abstract>
    <keywords> Performance, composed instrument, transparency, constraint.  </keywords>
  </document>
  <document>
    <name>nime2011_060.pdf</name>
    <abstract>
In this paper, we discuss the use of the clothesline as a
metaphor for designing a musical interface called Airer Choi-
r. This interactive installation is based on the function of
an ordinary object that is not a traditional instrument, and
hanging articles of clothing is literally the gesture to use the
interface. Based on this metaphor, a musical interface with
high transparency was designed. Using the metaphor, we
explored the possibilities for recognizing of input gestures
and creating sonic events by mapping data to sound. Thus,
four different types of Airer Choir were developed. By clas-
sifying the interfaces, we concluded that various musical
expressions are possible by using the same metaphor.

</abstract>
    <keywords> musical interface, metaphor, clothesline installation  </keywords>
  </document>
  <document>
    <name>nime2011_064.pdf</name>
    <abstract> 
In this paper, we discuss the results obtained by means of the 

EGGS (Elementary Gestalts for Gesture Sonification) system in 

terms of artistic realizations. EGGS was introduced in a previous 

edition of this conference. The works presented include 

interactive installations in the form of public art and interactive 

onstage performances. In all of the works, the EGGS principles of 

simplicity based on the correspondence between elementary sonic 

and movement units, and of organicity between sound and gesture 

are applied. Indeed, we study both sound as a means for gesture 

representation and gesture as embodiment of sound. These 

principles constitute our guidelines for the investigation of the 

bidirectional relationship between sound and body expression 

with various strategies involving both educated and non-educated 

executors.  

</abstract>
    <keywords>  Gesture sonification, Interactive performance, Public art.    </keywords>
  </document>
  <document>
    <name>nime2011_068.pdf</name>
    <abstract>
The present article describes a reverberation instrument
which is based on cognitive categorization of reverberat-
ing spaces. Different techniques for artificial reverberation
will be covered. A multidimensional scaling experiment
was conducted on impulse responses in order to determine
how humans acoustically perceive spatiality. This research
seems to indicate that the perceptual dimensions are re-
lated to early energy decay and timbral qualities. These
results are applied to a reverberation instrument based on
delay lines. It can be contended that such an instrument
can be controlled more intuitively than other delay line re-
verberation tools which often provide a confusing range of
parameters which have a physical rather than perceptual
meaning.

</abstract>
    <keywords> Reverberation, perception, multidimensional scaling, map- ping  </keywords>
  </document>
  <document>
    <name>nime2011_072.pdf</name>
    <keywords> Vibrotactile feedback, human-computer interfaces, digital composition, real-time performance, augmented instruments.  </keywords>
  </document>
  <document>
    <name>nime2011_076.pdf</name>
    <abstract>
The use of Interactive Evolutionary Computation(IEC) is
suitable to the development of art-creation aid system for
beginners. This is because of important features of IEC,
like the ability of optimizing with ambiguous evaluation
measures, and not requiring special knowledge about art-
creation. With the popularity of Consumer Generated Me-
dia, many beginners in term of art-creation are interested
in creating their own original art works. Thus developing of
useful IEC system for musical creation is an urgent task.
However, user-assist functions for IEC proposed in past
works decrease the possibility of getting good unexpected
results, which is an important feature of art-creation with
IEC. In this paper, The author proposes a new IEC eval-
uation process named "Shopping Basket" procedure IEC.
In the procedure, an user-assist function called Similarity-
Based Reasoning allows for natural evaluation by the user.
The function reduces user's burden without reducing the
possibility of unexpected results. The author performs an
experiment where subjects use the new interface to validate
it. As a result of the experiment, the author concludes the
the new interface is better to motivate users to compose
with IEC system than the old interface.

</abstract>
    <keywords> Interactive Evolutionary Computation, User-Interface, Com- position Aid  </keywords>
  </document>
  <document>
    <name>nime2011_080.pdf</name>
    <abstract>
BioRhythm is an interactive bio-feedback installation con-
trolled by the cardiovascular system. Data from a photo-
plethysmograph (PPG) sensor controls sonification and vi-
sualization parameters in real-time. Biological signals are
obtained using the techniques of Resonance Theory in Hemo-
dynamics and mapped to audiovisual cues via the Five Ele-
ment Philosophy. The result is a new media interface utiliz-
ing sound synthesis and spatialization with advanced graph-
ics rendering. BioRhythm serves as an artistic exploration
of the harmonic spectra of pulse waves.

</abstract>
  </document>
  <document>
    <name>nime2011_084.pdf</name>
    <keywords>  Electromechanical sonic art, kinetic sound art, prepared  speakers, Infinite Spring.   </keywords>
  </document>
  <document>
    <name>nime2011_088.pdf</name>
    <abstract> 
We introduce a novel algorithm for automatically generating 

rhythms in real time in a certain meter. The generated rhythms 

are "generic" in the sense that they are characteristic of each 

time signature without belonging to a specific musical style. 

The algorithm is based on a stochastic model in which various 

aspects and qualities of the generated rhythm can be controlled 

intuitively and in real time. Such qualities are the density of the 

generated events per bar, the amount of variation in generation, 

the amount of syncopation, the metrical strength, and of course 

the meter itself. The kin.rhythmicator software application was 

developed to implement this algorithm. During a performance 

with the kin.rhythmicator the user can control all aspects of the 

performance through descriptive and intuitive graphic controls. 

 

</abstract>
    <keywords>  automatic music generation, generative, stochastic, metric   indispensability, syncopation, Max/MSP, Max4Live    </keywords>
  </document>
  <document>
    <name>nime2011_092.pdf</name>
    <abstract>
The importance of embedded devices as new devices to the
field of Voltage-Controlled Synthesizers is realized. Empha-
sis is directed towards understanding the importance of such
devices in Voltage-Controlled Synthesizers. Introducing the
Voltage-Controlled Computer as a new paradigm. Specifica-
tions for hardware interfacing and programming techniques
are described based on real prototypes. Implementations
and successful results are reported.

</abstract>
    <keywords> Voltage-controlled synthesizer, embedded systems, voltage- controlled computer, computer driven control voltage gen- eration  </keywords>
  </document>
  <document>
    <name>nime2011_096.pdf</name>
    <abstract>
We developed an automatic piano performance system called
Polyhymnia that is able to generate expressive polyphonic
piano performances with music scores so that it can be used
as a computer-based tool for an expressive performance.
The system automatically renders expressive piano music
by means of automatic musical symbol interpretation and
statistical models of structure-expression relations regard-
ing polyphonic features of piano performance. Experimen-
tal results indicate that the generated performances of vari-
ous piano pieces with diverse trained models had polyphonic
expression and sounded expressively. In addition, the mod-
els trained with different performance styles reflected the
styles observed in the training performances, and they were
well distinguishable by human listeners. Polyhymnia won
the first prize in the autonomous section of the Performance
Rendering Contest for Computer Systems (Rencon) 2010.

</abstract>
    <keywords> performance rendering, polyphonic expression, statistical modeling, conditional random fields  </keywords>
  </document>
  <document>
    <name>nime2011_100.pdf</name>
    <abstract>
Audio mixing is the adjustment of relative volumes, pan-
ning and other parameters corresponding to different sound
sources, in order to create a technically and aesthetically
adequate sound sum. To do this, audio engineers employ
"panpots" and faders, the standard controls in audio mix-
ers. The design of such devices has remained practically un-
changed for decades since their introduction. At the time,
no usability studies seem to have been conducted on such
devices, so one could question if they are really optimized
for the task they are meant for.

This paper proposes a new set of controls that might be
used to simplify and/or improve the performance of audio
mixing tasks, taking into account the spatial characteristics
of modern mixing technologies such as surround and 3D
audio and making use of multitouch interface technologies.
A preliminary usability test has shown promising results.

</abstract>
  </document>
  <document>
    <name>nime2011_104.pdf</name>
    <abstract>
This paper explores how a general cognitive architecture can
pragmatically facilitate the development and exploration of
interactive music interfaces on a mobile platform. To this
end we integrated the Soar cognitive architecture into the
mobile music meta-environment urMus. We develop and
demonstrate four artificial agents which use diverse learning
mechanisms within two mobile music interfaces. We also
include details of the computational performance of these
agents, evincing that the architecture can support real-time
interactivity on modern commodity hardware.

</abstract>
  </document>
  <document>
    <name>nime2011_108.pdf</name>
    <abstract>
Supervised machine learning enables complex many-to-many
mappings and control schemes needed in interactive perfor-
mance systems. One of the persistent problems in these
applications is generating, identifying and choosing input
output pairings for training. This poses problems of scope
(limiting the realm of potential control inputs), effort (re-
quiring significant pre-performance training time), and cog-
nitive load (forcing the performer to learn and remember the
control areas). We discuss the creation and implementation
of an automatic "supervisor," using unsupervised machine
learning algorithms to train a supervised neural network
on the fly. This hierarchical arrangement enables network
training in real time based on the musical or gestural con-
trol inputs employed in a performance, aiming at freeing the
performer to operate in a creative, intuitive realm, making
the machine control transparent and automatic. Three im-
plementations of this self supervised model driven by iPod,
iPad, and acoustic violin are described.

</abstract>
    <keywords> NIME, machine learning, interactive computer music, ma- chine listening, improvisation, adaptive resonance theory  </keywords>
  </document>
  <document>
    <name>nime2011_112.pdf</name>
    <abstract>
A mixed media tool was created that promotes ensemble
virtuosity through tight coordination and interdepence in
musical performance. Two different types of performers in-
teract with a virtual space using Wii remote and tangible
interfaces using the reacTIVision toolkit [11]. One group of
performers uses a tangible tabletop interface to place and
move sound objects in a virtual environment. The sound
objects are represented by visual avatars and have audio
samples associated with them. A second set of performers
make use of Wii remotes to create triggering waves that
can collide with those sound objects. Sound is only pro-
duced upon collision of the waves with the sound objects.
What results is a performance in which users must negoti-
ate through a physical and virtual space and are positioned
to work together to create musical pieces.

</abstract>
    <keywords> reacTIVision, processing, ensemble, mixed media, virtual- ization, tangible, sample  </keywords>
  </document>
  <document>
    <name>nime2011_116.pdf</name>
    <abstract>
This paper presents MoodifierLive, a mobile phone appli-
cation for interactive control of rule-based automatic music
performance. Five different interaction modes are available,
of which one allows for collaborative performances with up
to four participants, and two let the user control the ex-
pressive performance using expressive hand gestures. Eval-
uations indicate that the application is interesting, fun to
use, and that the gesture modes, especially the one based on
data from free expressive gestures, allow for performances
whose emotional content matches that of the gesture that
produced them.

</abstract>
    <keywords> Expressive performance, gesture, collaborative performance, mobile phone  </keywords>
  </document>
  <document>
    <name>nime2011_120.pdf</name>
  </document>
  <document>
    <name>nime2011_124.pdf</name>
    <abstract>
This paper presents a study of blowing pressure profiles
acquired from recorder playing. Blowing pressure signals
are captured from real performance by means of a a low-
intrusiveness acquisition system constructed around com-
mercial pressure sensors based on piezoelectric transducers.
An alto recorder was mechanically modified by a luthier
to allow the measurement and connection of sensors while
respecting playability and intrusiveness. A multi-modal
database including aligned blowing pressure and sound sig-
nals is constructed from real practice, covering the perfor-
mance space by considering different fundamental frequen-
cies, dynamics, articulations and note durations. Once sig-
nals were pre-processed and segmented, a set of temporal
envelope features were defined as a basis for studying and
constructing a simplified model of blowing pressure profiles
in different performance contexts.

</abstract>
  </document>
  <document>
    <name>nime2011_128.pdf</name>
    <abstract> 
This is an overview of the three installations Hoppsa 
Universum, CLOSE and Flying Carpet. They were all designed 
as choreographed sound and music installations controlled by 
the visitors movements. The perspective is from an artistic 
goal/vision intention in combination with the technical 
challenges and possibilities. All three installations were 
realized with video cameras in the ceiling registering the users' 
position or movement.  The video analysis was then controlling 
different types of interactive software audio players. Different 
aspects like narrativity, user control, and technical limitations 
are discussed.  
</abstract>
    <keywords>  Gestures, dance, choreography, music installation, interactive  music.   </keywords>
  </document>
  <document>
    <name>nime2011_136.pdf</name>
    <abstract> 
We developed a kinetic particles synthesizer for mobile devices 
having a multi-touch screen such as a tablet PC and a smart 
phone. This synthesizer generates music based on the kinetics 
of particles under a two-dimensional physics engine. The 
particles move in the screen to synthesize sounds according to 
their own physical properties, which are shape, size, mass, 
linear and angular velocity, friction, restitution, etc. If a particle 
collides with others, a percussive sound is generated. A player 
can play music by the simple operation of touching or dragging 
on the screen of the device. Using a three-axis acceleration 
sensor, a player can perform music by shuffling or tilting the 
device. Each particle sounds just a simple tone. However, a 
large amount of various particles play attractive music by 
aggregating their sounds. This concept has been inspired by 
natural sounds made from an assembly of simple components, 
for example, rustling leaves or falling rain. For a novice who 
has no experience of playing a musical instrument, it is easy to 
learn how to play instantly and enjoy performing music with 
intuitive operation. Our system is used for musical instruments 
for interactive music entertainment.  

 
</abstract>
    <keywords>  Particle, Tablet PC, iPhone, iPod touch, iPad, Smart phone,  Kinetics, Touch screen, Physics engine.   </keywords>
  </document>
  <document>
    <name>nime2011_140.pdf</name>
    <abstract> 
Daft Datum is an autonomous new media artefact that takes 
input from movement of the feet (i.e. 
tapping/stomping/stamping) on a wooden surface, underneath 
which is a sensor sheet. The sensors in the sheet are mapped to 
various sound samples and synthesized sounds. Attributes of 
the synthesized sound, such as pitch and octave, can be 
controlled using the Nintendo Wii Remote. It also facilitates 
switching between modes of sound and recording/playing back 
a segment of audio. The result is music generated by dancing 
on the device that is further modulated by a hand-held 
controller. 
 
</abstract>
    <keywords>  Daft Datum, Wii, Dance Pad, Feet, Controller, Bluetooth,  Musical Interface, Dance, Sensor Sheet   </keywords>
  </document>
  <document>
    <name>nime2011_144.pdf</name>
    <abstract>
In this paper we present an experimental study concerning
gestural embodiment of environmental sounds in a listening
context. The presented work is part of a project aiming at
modeling movement-sound relationships, with the end goal
of proposing novel approaches for designing musical instru-
ments and sounding objects. The experiment is based on
sound stimuli corresponding to "causal" and "non-causal"
sounds. It is divided into a performance phase and an in-
terview. The experiment is designed to investigate possible
correlation between the perception of the "causality" of en-
vironmental sounds and different gesture strategies for the
sound embodiment. In analogy with the perception of the
sounds' causality, we propose to distinguish gestures that
"mimic" a sound's cause and gestures that "trace" a sound's
morphology following temporal sound characteristics. Re-
sults from the interviews show that, first, our causal sounds
database lead to consistent descriptions of the action at the
origin of the sound and participants mimic this action. Sec-
ond, non-causal sounds lead to inconsistent metaphoric de-
scriptions of the sound and participants make gestures fol-
lowing sound "contours". Quantitatively, the results show
that gesture variability is higher for causal sounds that non-
causal sounds.

</abstract>
    <keywords> Embodiment, Environmental Sound Perception, Listening, Gesture Sound Interaction  </keywords>
  </document>
  <document>
    <name>nime2011_149.pdf</name>
    <abstract>
The use of physiological signals in Human Computer Inter-
action (HCI) is becoming popular and widespread, mostly
due to sensors miniaturization and advances in real-time
processing. However, most of the studies that use physiology-
based interaction focus on single-user paradigms, and its
usage in collaborative scenarios is still in its beginning. In
this paper we explore how interactive sonification of brain
and heart signals, and its representation through physical
objects (physiopucks) in a tabletop interface may enhance
motivational and controlling aspects of music collaboration.

A multimodal system is presented, based on an electro-
physiology sensor system and the Reactable, a musical table-
top interface. Performance and motivation variables were
assessed in an experiment involving a test "Physio" group
(N=22) and a control "Placebo" group (N=10). Pairs of
participants used two methods for sound creation: implicit
interaction through physiological signals, and explicit in-
teraction by means of gestural manipulation. The results
showed that pairs in the Physio Group declared less diffi-
culty, higher confidence and more symmetric control than
the Placebo Group, where no real-time sonification was pro-
vided as subjects were using pre-recorded physiological sig-
nal being unaware of it. These results support the feasibility
of introducing physiology-based interaction in multimodal
interfaces for collaborative music generation.

</abstract>
  </document>
  <document>
    <name>nime2011_155.pdf</name>
    <abstract>
This paper examines the creation of augmented musical
instruments by a number of musicians. Equipped with a
system called the Augmentalist, 10 musicians created new
augmented instruments based on their traditional acoustic
or electric instruments. This paper discusses the ways in
which the musicians augmented their instruments, exam-
ines the similarities and differences between the resulting
instruments and presents a number of interesting findings
resulting from this process.

</abstract>
    <keywords> Augmented Instruments, Instrument Design, Digital Musi- cal Instruments, Performance  </keywords>
  </document>
  <document>
    <name>nime2011_161.pdf</name>
    <abstract> 
We present "Tahakum", an open source, extensible collection 
of software tools designed to enhance workflow on multi-
channel audio systems within complex multi-functional 
research and development environments. Tahakum aims to 
provide critical functionality required across a broad spectrum 
of audio systems usage scenarios, while at the same time 
remaining sufficiently open as to easily support modifications 
and extensions via 3rd party hardware and software.  Features 
provided in the framework include software for custom 
mixing/routing and audio system preset automation, software 
for network message routing/redirection and protocol 
conversion, and software for dynamic audio asset 
management and control. 
 
</abstract>
    <keywords>  Audio Control Systems, Audio for VR, Max/MSP, Spatial  Audio     </keywords>
  </document>
  <document>
    <name>nime2011_167.pdf</name>
    <abstract> 
Computer music systems that coordinate or interact with 
human musicians exist in many forms. Often, coordination is 
at the level of gestures and phrases without synchronization at 
the beat level (or perhaps the notion of "beat" does not even 
exist). In music with beats, fine-grain synchronization can be 
achieved by having humans adapt to the computer (e.g. 
following a click track), or by computer accompaniment in 
which the computer follows a predetermined score. We 
consider an alternative scenario in which improvisation 
prevents traditional score following, but where 
synchronization is achieved at the level of beats, measures, and 
cues. To explore this new type of human-computer interaction, 
we have created new software abstractions for synchronization 
and coordination of music and interfaces in different 
modalities. We describe these new software structures, present 
examples, and introduce the idea of music notation as an 
interactive musical interface rather than a static document. 
 
</abstract>
  </document>
  <document>
    <name>nime2011_173.pdf</name>
    <abstract>
This paper describes a new Beagle Board-based platform for
teaching and practicing interaction design for musical appli-
cations. The migration from desktop and laptop computer-
based sound synthesis to a compact and integrated con-
trol, computation and sound generation platform has enor-
mous potential to widen the range of computer music instru-
ments and installations that can be designed, and improves
the portability, autonomy, extensibility and longevity of de-
signed systems. We describe the technical features of the
Satellite CCRMA platform and contrast it with personal
computer-based systems used in the past as well as emerging
smart phone-based platforms. The advantages and trade-
offs of the new platform are considered, and some project
work is described.

</abstract>
  </document>
  <document>
    <name>nime2011_179.pdf</name>
    <keywords> Digital scratching, mobile music, digital DJ, smartphone, turntable, turntablism, record player, accelerometer, gyro- scope, vinyl emulation software  </keywords>
  </document>
  <document>
    <name>nime2011_185.pdf</name>
    <abstract>
MadPad is a networked audiovisual sample station for mo-
bile devices. Twelve short video clips are loaded onto the
screen in a grid and playback is triggered by tapping any-
where on the clip. This is similar to tapping the pads of an
audio sample station, but extends that interaction to add
visual sampling. Clips can be shot on-the-fly with a camera-
enabled mobile device and loaded into the player instantly,
giving the performer an ability to quickly transform his or
her surroundings into a sample-based, audiovisual instru-
ment. Samples can also be sourced from an online commu-
nity in which users can post or download content. The re-
cent ubiquity of multitouch mobile devices and advances in
pervasive computing have made this system possible, pro-
viding for a vast amount of content only limited by the
imagination of the performer and the community. This pa-
per presents the core features of MadPad and the design
explorations that inspired them.

</abstract>
    <keywords> mobile music, networked music, social music, audiovisual, sampling, user-generated content, crowdsourcing, sample station, iPad, iPhone  </keywords>
  </document>
  <document>
    <name>nime2011_191.pdf</name>
    <abstract>
Visual information integration in mobile music performance
is an area that has not been thoroughly explored and current
applications are often individually designed. From camera
input to flexible output rendering, we discuss visual perfor-
mance support in the context of urMus, a meta-environment
for mobile interaction and performance development. The
use of cameras, a set of image primitives, interactive visual
content, projectors, and camera flashes can lead to visually
intriguing performance possibilities.

</abstract>
    <keywords> Mobile performance, visual interaction, camera phone, mo- bile collaboration  </keywords>
  </document>
  <document>
    <name>nime2011_197.pdf</name>
    <abstract> 
This paper describes the origin, design, and implementation of 
Smule's Magic Fiddle, an expressive musical instrument for the 
iPad.  Magic Fiddle takes advantage of the physical aspects of 
the device to integrate game-like and pedagogical elements.  
We describe the origin of Magic Fiddle, chronicle its design 
process, discuss its integrated music education system, and 
evaluate the overall experience. 
 
</abstract>
    <keywords>  Magic Fiddle, iPad, physical interaction design, experiential  design, music education.   </keywords>
  </document>
  <document>
    <name>nime2011_203.pdf</name>
  </document>
  <document>
    <name>nime2011_207.pdf</name>
    <abstract>
Laptop Orchestras (LOs) have recently become a very pop-
ular mode of musical expression. They engage groups of
performers to use ordinary laptop computers as instruments
and sound sources in the performance of specially created
music software. Perhaps the biggest challenge for LOs is
the distribution, management and control of software across
heterogeneous collections of networked computers. Soft-
ware must be stored and distributed from a central reposi-
tory, but launched on individual laptops immediately before
performance. The GRENDL project leverages proven grid
computing frameworks and approaches the Laptop Orches-
tra as a distributed computing platform for interactive com-
puter music. This allows us to readily distribute software
to each laptop in the orchestra depending on the laptop's
internal configuration, its role in the composition, and the
player assigned to that computer. Using the SAGA frame-
work, GRENDL is able to distribute software and manage
system and application environments for each composition.
Our latest version includes tangible control of the GRENDL
environment for a more natural and familiar user experi-
ence.

</abstract>
    <keywords> laptop orchestra, tangible interaction, grid computing  </keywords>
  </document>
  <document>
    <name>nime2011_211.pdf</name>
    <abstract>
A contemporary PC user, typically expects a sound card
to be a piece of hardware, that: can be manipulated by
'audio' software (most typically exemplified by 'media play-
ers'); and allows interfacing of the PC to audio reproduc-
tion and/or recording equipment. As such, a 'sound card'
can be considered to be a system, that encompasses design
decisions on both hardware and software levels - that also
demand a certain understanding of the architecture of the
target PC operating system.
This project outlines how an Arduino Duemillanove

board (containing a USB interface chip, manufactured by
Future Technology Devices International Ltd [FTDI]
company) can be demonstrated to behave as a full-duplex,
mono, 8-bit 44.1 kHz soundcard, through an implemen-
tation of: a PC audio driver for ALSA (Advanced Linux
Sound Architecture); a matching program for theArduino's
ATmega microcontroller - and nothing more than head-
phones (and a couple of capacitors). The main contribution
of this paper is to bring a holistic aspect to the discussion
on the topic of implementation of soundcards - also by re-
ferring to open-source driver, microcontroller code and test
methods; and outline a complete implementation of an open
- yet functional - soundcard system.

</abstract>
  </document>
  <document>
    <name>nime2011_217.pdf</name>
    <abstract>
In this paper, we introduce a pipe interface that recognizes
touch on tone holes by the resonances in the pipe instead of
a touch sensor. This work was based on the acoustic princi-
ples of woodwind instruments without complex sensors and
electronic circuits to develop a simple and durable interface.
The measured signals were analyzed to show that different
fingerings generate various sounds. The audible resonance
signal in the pipe interface can be used as a sonic event for
musical expression by itself and also as an input parameter
for mapping different sounds.

</abstract>
    <keywords> resonance, mapping, pipe  </keywords>
  </document>
  <document>
    <name>nime2011_220.pdf</name>
    <abstract> 
In this paper a collaborative music game for two pen tablets is 
studied in order to see how two people with no professional 
music background negotiated musical improvisation. In an 
initial study of what it is that constitutes play fluency in 
improvisation, a music game has been designed and evaluated 
through video analysis: A qualitative view of mutual action 
describes the social context of music improvisation: how two 
people with speech, laughter, gestures, postures and pauses 
negotiate individual and joint action. The objective behind the 
design of the game application was to support players in some 
aspects of their mutual play. Results show that even though 
players activated additional sound feedback as a result of their 
mutual play, players also engaged in forms of mutual play that 
the game engine did not account for. These ways of mutual 
play are descibed further along with some suggestions for how 
to direct future designs of collaborative music improvisation 
games towards ways of mutual play.   
 
</abstract>
    <keywords>  Collaborative interfaces, improvisation, interactive music  games, social interaction, play, novice.   </keywords>
  </document>
  <document>
    <name>nime2011_224.pdf</name>
    <abstract> 
The Bass Sleeve uses an Arduino board with a combination of 
buttons, switches, flex sensors, force sensing resistors, and an 
accelerometer to map the ancillary movements of a performer 
to sampling, real-time audio and video processing including 
pitch shifting, delay, low pass filtering, and onscreen video 
movement.  The device was created to augment the existing 
functions of the electric bass and explore the use of ancillary 
gestures to control the laptop in a live performance.  In this 
research it was found that incorporating ancillary gestures into 
a live performance could be useful when controlling the 
parameters of audio processing, sound synthesis and video 
manipulation.   These ancillary motions can be a practical 
solution to gestural multitasking allowing independent control 
of computer music parameters while performing with the 
electric bass.  The process of performing with the Bass Sleeve 
resulted in a greater amount of laptop control, an increase in the 
amount of expressiveness using the electric bass in combination 
with the laptop, and an improvement in the interactivity on both 
the electric bass and laptop during a live performance.  The 
design uses various gesture-to-sound mapping strategies to 
accomplish a compositional task during an electro acoustic 
multimedia musical performance piece. 
 
</abstract>
    <keywords>  Interactive Music, Interactive Performance Systems, Gesture  Controllers, Augmented Instruments, Electric Bass, Video  Tracking   </keywords>
  </document>
  <document>
    <name>nime2011_228.pdf</name>
    <abstract> 
This paper describes the KarmetiK NotomotoN, a new musical 
robotic system for performance and education. A long time 
goal of the authors has been to provide users with plug-and-
play, highly expressive musical robot system with a high 
degree of portability. This paper describes the technical details 
of the NotomotoN, and discusses its use in performance and 
educational scenarios. Detailed tests performed to optimize 
technical aspects of the NotomotoN are described to highlight 
usability and performance specifications for electronic 
musicians and educators. 
 
</abstract>
  </document>
  <document>
    <name>nime2011_232.pdf</name>
    <abstract> 
The Manipuller is a novel Gestural Controller based on strings 

manipulation and multi-dimensional force sensing technology. 

This paper describes its motivation, design and operational 
principles along with some of its musical applications. Finally 

the results of a preliminary usability test are presented and 

discussed. 

</abstract>
  </document>
  <document>
    <name>nime2011_236.pdf</name>
    <abstract> 
The Surface Editor is a software tool for creating control 
interfaces and mapping input actions to OSC or MIDI actions 
very easily and intuitively. Originally conceived to be used 
with a tactile interface, the Surface Editor has been extended to 
support the creation of graspable interfaces as well. This paper 
presents a new framework for the generic mapping of user 
actions with graspable objects on a surface. We also present a 
system for detecting touch on thin objects, allowing for 
extended interactive possibilities. The Surface Editor is not 
limited to a particular tracking system though, and the generic 
mapping approach for objects can have a broader use with 
various input interfaces supporting touch and/or objects.  
 
</abstract>
    <keywords>  NIME, mapping, interaction, user-defined interfaces, tangibles,  graspable interfaces.   </keywords>
  </document>
  <document>
    <name>nime2011_240.pdf</name>
    <abstract> 
This paper presents the SmartFiducial, a wireless tangible object 
that facilitates additional modes of expressivity for vision-based 
tabletop surfaces. Using infrared proximity sensing and resistive 
based force-sensors, the SmartFiducial affords users unique, and 
highly gestural inputs. Furthermore, the SmartFiducial incorpo-
rates additional customizable pushbutton switches. Using XBee 
radio frequency (RF) wireless transmission, the SmartFiducial 
establishes bipolar communication with a host computer. This 
paper describes the design and implementation of the SmartFidu-
cial, as well as an exploratory use in a musical context. 
 
</abstract>
    <keywords>  Fiducial, Tangible Interface, Multi-touch, Sensors, Gesture, Hap- tics, Bricktable, Proximity Sensing   </keywords>
  </document>
  <document>
    <name>nime2011_244.pdf</name>
    <keywords> generalized keyboard, isomorphic layout, multi-touch sur- face, tablet, musical interface design, iPad, microtonality  </keywords>
  </document>
  <document>
    <name>nime2011_248.pdf</name>
  </document>
  <document>
    <name>nime2011_252.pdf</name>
    <abstract>
This research presents a 3D gestural interface for collabo-
rative concatenative sound synthesis and audio mosaicing.
Our goal is to improve the communication between the au-
dience and performers by means of an enhanced correlation
between gestures and musical outcome. Nuvolet consists of
a 3D motion controller coupled to a concatenative synthe-
sis engine. The interface detects and tracks the perform-
ers hands in four dimensions (x,y,z,t) and allows them to
concurrently explore two or three-dimensional sound cloud
representations of the units from the sound corpus, as well
as to perform collaborative target-based audio mosaicing.
Nuvolet is included in the Esmuc Laptop Orchestra catalog
for forthcoming performances.

</abstract>
    <keywords> concatenative synthesis, audio mosaicing, open-air inter- face, gestural controller, musical instrument, 3D  </keywords>
  </document>
  <document>
    <name>nime2011_256.pdf</name>
    <abstract>
We report on a performance study of a French-Canadian fid-
dler. The fiddling tradition forms an interesting contrast to
classical violin performance in several ways. Distinguishing
features include special elements in the bowing technique
and the presence of an accompanying foot clogging pattern.
These two characteristics are described, visualized and an-
alyzed using video and motion capture recordings as source
material.

</abstract>
    <keywords> fiddler, violin, French-Canadian, bowing, feet, clogging, mo- tion capture, video, motiongram, kinematics, sonification  </keywords>
  </document>
  <document>
    <name>nime2011_260.pdf</name>
    <abstract>
In this  paper  an audio-visual  installation is  discussed,  which 
combines interactive, immersive and generative elements. After 
introducing some of the challenges in the field of Generative 
Art and placing the work within its research context, conceptual 
reflections are made about the spatial, behavioural, perceptual 
and social issues that are raised within the entire installation. A 
discussion about the artistic content follows, focussing on the 
scenography  and  on  working  with  flocking  algorithms  in 
general, before addressing three specific pieces realised for the 
exhibition.  Next  the technical  implementation  for  both  hard- 
and software are detailed before the idea of a hybrid ecosystem 
gets discussed and further developments outlined.

</abstract>
    <keywords> Generative  Art,  Interactive  Environment,  Immersive  Installation, Swarm Simulation, Hybrid Ecosystem   </keywords>
  </document>
  <document>
    <name>nime2011_264.pdf</name>
    <abstract> 
In this paper, we describe an implementation of a real-time 
sound synthesizer using Finite Difference-based simulation of a 
two-dimensional membrane. Finite Difference (FD) methods 
can be the basis for physics-based music instrument models that 
generate realistic audio output. However, such methods are 
compute-intensive; large simulations cannot run in real time on 
current CPUs. Many current systems now include powerful 
Graphics Processing Units (GPUs), which are a good fit for FD 
methods. We demonstrate that it is possible to use this method 
to create a usable real-time audio synthesizer. 
 
</abstract>
    <keywords>  Finite Difference, GPU, CUDA, Synthesis   </keywords>
  </document>
  <document>
    <name>nime2011_268.pdf</name>
    <abstract>
Interacting with musical avatars have been increasingly pop-
ular over the years, with the introduction of games like
Guitar Hero and Rock Band. These games provide MIDI-
equipped controllers that look like their real-world counter-
parts (e.g. MIDI guitar, MIDI drumkit) that the users play
to control their designated avatar in the game. The perfor-
mance of the user is measured against a score that needs to
be followed. However, the avatar does not move in response
to how the user plays, it follows some predefined movement
pattern. If the user plays badly, the game ends with the
avatar ending the performance (i.e. throwing the guitar on
the floor). The gaming experience would increase if the
avatar would move in accordance with user input. This pa-
per presents an architecture that couples musical input with
body movement. Using imitation learning, a simulated hu-
man robot learns to play the drums like human drummers
do, both visually and auditory. Learning data is recorded
using MIDI and motion tracking. The system uses an arti-
ficial intelligence approach to implement imitation learning,
employing artificial neural networks.

</abstract>
  </document>
  <document>
    <name>nime2011_272.pdf</name>
    <abstract>
TweetDreams is an instrument and musical composition
which creates real-time sonification and visualization of
tweets. Tweet data containing specified search terms is re-
trieved from Twitter and used to build networks of associ-
ated tweets. These networks govern the creation of melodies
associated with each tweet and are displayed graphically.
Audience members participate in the piece by tweeting,
and their tweets are given special musical and visual promi-
nence.

</abstract>
    <keywords> Twitter, audience participation, sonification, data visual- ization, text processing, interaction, multi-user instrument.  </keywords>
  </document>
  <document>
    <name>nime2011_276.pdf</name>
    <abstract>
JunctionBox is a new software toolkit for creating multi-
touch interfaces for controlling sound and music. More
specifically, the toolkit has special features which make it
easy to create TUIO-based touch interfaces for controlling
sound engines via Open Sound Control. Programmers us-
ing the toolkit have a great deal of freedom to create highly
customized interfaces that work on a variety of hardware.

</abstract>
    <keywords> Multi-touch, Open Sound Control, Toolkit, TUIO  </keywords>
  </document>
  <document>
    <name>nime2011_280.pdf</name>
    <abstract>
This paper presents an approach to practice-based research
in new musical instrument design. At a high level, the pro-
cess involves drawing on relevant theories and aesthetic ap-
proaches to design new instruments, attempting to iden-
tify relevant applied design criteria, and then examining
the experiences of performers who use the instruments with
particular reference to these criteria. Outcomes of this pro-
cess include new instruments, theories relating to musician-
instrument interaction and a set of design criteria informed
by practice and research.

</abstract>
    <keywords> practice-based research, evaluation, Human-Computer In- teraction, research methods, user studies  </keywords>
  </document>
  <document>
    <name>nime2011_284.pdf</name>
    <keywords> Spectral Model Synthesis, Gesture Recognition, Synthesis Control, Wacom Tablet, Machine Learning  </keywords>
  </document>
  <document>
    <name>nime2011_288.pdf</name>
    <abstract>
We present BeatJockey, a prototype interface which makes
use of Audio Mosaicing (AM), beat-tracking and machine
learning techniques, for supporting Diskjockeys (DJs) by
proposing them new ways of interaction with the songs on
the DJ's playlist. This prototype introduces a new paradigm
to DJing in which the user has the capability to mix songs
interacting with beat-units that accompany the DJ's mix.
For this type of interaction, the system suggests song slices
taken from songs selected from a playlist, which could go
well with the beats of whatever master song is being played.
In addition the system allows the synchronization of multi-
ple songs, thus permitting flexible, coherent and rapid pro-
gressions in the DJ's mix. BeatJockey uses the Reactable,
a musical tangible user interface (TUI), and it has been
designed to be used by all DJs regardless of their level of
expertise, as the system helps the novice while bringing new
creative opportunities to the expert.

</abstract>
    <keywords> DJ, music information retrieval, audio mosaicing, percus- sion, turntable, beat-mash, interactive music interfaces, real- time, tabletop interaction, reactable.  </keywords>
  </document>
  <document>
    <name>nime2011_292.pdf</name>
    <abstract> 
In this paper the relationship between body, motion and sound 
is addressed. The comparison with traditional instruments and 
dance is shown with regards to basic types of motion. The 
difference between gesture and movement is outlined and some 
of the models used in dance for structuring motion sequences 
are described. In order to identify expressive aspects of motion 
sequences a test scenario is devised. After the description of the 
methods and tools used in a series of measurements, two types 
of data-display are shown and the applied in the interpretation. 
One salient feature is recognized and put into perspective with 
regards to movement and gestalt perception. Finally the merits 
of the technical means that were applied are compared and a 
model-based approach to motion-sound mapping is proposed. 

</abstract>
    <keywords>  Interactive Dance, Motion and Gesture, Sonification, Motion  Perception, Mapping     </keywords>
  </document>
  <document>
    <name>nime2011_296.pdf</name>
    <abstract>
MoodMixer is an interactive installation in which partici-
pants collaboratively navigate a two-dimensional music space
by manipulating their cognitive state and conveying this
state via wearable Electroencephalography (EEG) technol-
ogy. The participants can choose to actively manipulate
or passively convey their cognitive state depending on their
desired approach and experience level. A four-channel elec-
tronic music mixture continuously conveys the participants'
expressed cognitive states while a colored visualization of
their locations on a two-dimensional projection of cogni-
tive state attributes aids their navigation through the space.
MoodMixer is a collaborative experience that incorporates
aspects of both passive and active EEG sonification and
performance art. We discuss the technical design of the in-
stallation and place its collaborative sonification aesthetic
design within the context of existing EEG-based music and
art.

</abstract>
    <keywords> EEG, BCMI, collaboration, sonification, visualization  </keywords>
  </document>
  <document>
    <name>nime2011_304.pdf</name>
    <abstract> 
The goal of our research is to find ways of supporting and 
encouraging musical behavior by non-musicians in shared 
public performance environments. Previous studies indicated 
simultaneous music listening and performance is difficult for 
non-musicians, and that visual support for the task might be 
helpful. This paper presents results from a preliminary user 
study conducted to evaluate the effect of visual feedback on a 
musical tracking task. Participants generated a musical signal 
by manipulating a hand-held device with two dimensions of 
control over two parameters, pitch and density of note events, 
and were given the task of following a target pattern as closely 
as possible. The target pattern was a machine-generated 
musical signal comprising of variation over the same two 
parameters. Visual feedback provided participants with 
information about the control parameters of the musical signal 
generated by the machine. We measured the task performance 
under different visual feedback strategies. Results show that 
single parameter visualizations tend to improve the tracking 
performance with respect to the visualized parameter, but not 
the non-visualized parameter. Visualizing two independent 
parameters simultaneously decreases performance in both 
dimensions.  
  
</abstract>
    <keywords>  Mobile phone, Interactive music performance, Listening, Group  music play, Visual support     </keywords>
  </document>
  <document>
    <name>nime2011_308.pdf</name>
    <abstract> 
An effective programming style for gesture signal processing is 
described using a new library that brings efficient run-time 
polymorphism, functional and instance-based object-oriented 
programming to Max/MSP. By introducing better support for 
generic programming and composability Max/MSP becomes a 
more productive environment for managing the growing scale 
and complexity of gesture sensing systems for musical 
instruments and interactive installations. 
</abstract>
  </document>
  <document>
    <name>nime2011_316.pdf</name>
    <abstract> 
The article describes a flexible mapping technique realized as a 
many-to-many dynamic mapping matrix. Digital sound 
generation is typically controlled by a large number of 
parameters and efficient and flexible mapping is necessary to 
provide expressive control over the instrument. The proposed 
modulation matrix technique may be seen as a generic and self-
modifying mapping mechanism integrated in a dynamic 
interpolation scheme. It is implemented efficiently by taking 
advantage of its inherent sparse matrix structure. The 
modulation matrix is used within the Hadron Particle 
Synthesizer, a complex granular module with 200 synthesis 
parameters and a simplified performance control structure with 
4 expression parameters. 
</abstract>
    <keywords>  Mapping, granular synthesis, modulation, live performance      </keywords>
  </document>
  <document>
    <name>nime2011_322.pdf</name>
    <abstract>
The purpose of this brief paper is to revisit the question of
longevity in present experimental practice and coin the term
autonomous new media artefacts (AutoNMA), which are
complete and independent of external computer systems,
so they can be operable for a longer period of time and
can be demonstrated at a moment's notice. We argue that
platforms for prototyping should promote the creation of
AutoNMA to make extant the devices which will be a part
of the future history of new media.

</abstract>
    <keywords> autonomous, standalone, Satellite CCRMA, Arduino  </keywords>
  </document>
  <document>
    <name>nime2011_324.pdf</name>
    <abstract> 
Recently, Microsoft introduced a game interface called Kinect 
for the Xbox 360 video game platform. This interface enables 
users to control and interact with the game console without the 
need to touch a controller. It largely increases the users' degree 
of freedom to express their emotion. In this paper, we first 
describe the system we developed to use this interface for 
sound generation and controlling musical expression. The 
skeleton data are extracted from users' motions and the data are 
translated to pre-defined MIDI data. We then use the MIDI data 
to control several applications. To allow the translation between 
the data, we implemented a simple Kinect-to-MIDI data 
convertor, which is introduced in this paper. We describe two 
applications to make music with Kinect: we first generate 
sound with Max/MSP, and then control the adlib with our own 
adlib generating system by the body movements of the users. 
 
</abstract>
    <keywords>  Kinect, gaming interface, sound generation, adlib generation     </keywords>
  </document>
  <document>
    <name>nime2011_326.pdf</name>
    <abstract>
This  paper  proposes  a  new research  direction  for  the  large 
family  of  instrumental  musical  interfaces  where  sound  is 
generated  using  digital  granular  synthesis,  and  where 
interaction and control involve the (fine) operation of stiff, flat 
contact surfaces. 
First,  within a historical  context, a general  absence of, and 

clear need for, tangible output that is dynamically instantiated 
by the grain-generating process  itself is identified. Second, to 
fill this gap, a concrete general approach is proposed based on 
the careful  construction of  non-vibratory and vibratory force 
pulses, in a one-to-one relationship with sonic grains.
An  informal  pilot  psychophysics  experiment  initiating  the 

approach was conducted, which took into account the two main 
cases for applying forces to the human skin: perpendicular, and 
lateral. Initial results indicate that the force pulse approach can 
enable  perceivably multidimensional,  tangible  display of  the 
ongoing grain-generating process. Moreover, it was found that 
this can be made to meaningfully happen (in real time) in the 
same timescale of basic sonic grain generation. This is not a 
trivial  property,  and  provides  an  important  and  positive 
fundament for further developing this type of enhanced display. 
It also leads to the exciting prospect of making arbitrary sonic 
grains actual physical manipulanda.
 

</abstract>
  </document>
  <document>
    <name>nime2011_329.pdf</name>
    <abstract>
This paper presents a prototypical tool for sound selec-
tion driven by users' gestures. Sound selection by gestures
is a particular case of "query by content" in multimedia
databases. Gesture-to-Sound matching is based on com-
puting the similarity between both gesture and sound pa-
rameters' temporal evolution. The tool presents three al-
gorithms for matching gesture query to sound target. The
system leads to several applications in sound design, virtual
instrument design and interactive installation.

</abstract>
    <keywords> Query by Gesture, Time Series Analysis, Sonic Interaction  </keywords>
  </document>
  <document>
    <name>nime2011_331.pdf</name>
    <abstract>
We propose and discuss an open source real-time inter-
face that focuses in the vast potential for interactive sound
art creation emerging from biological neural networks, as
paradigmatic complex systems for musical exploration. In
particular, we focus on networks that are responsible for the
generation of rhythmic patterns.The interface relies upon
the idea of relating metaphorically neural behaviors to elec-
tronic and acoustic instruments notes, by means of flexible
mapping strategies. The user can intuitively design net-
work configurations by dynamically creating neurons and
configuring their inter-connectivity. The core of the system
is based in events emerging from his network design, which
functions in a similar way to what happens in real small
neural networks. Having multiple signal and data inputs
and outputs, as well as standard communications protocols
such as MIDI, OSC and TCP/IP, it becomes and unique
tool for composers and performers, suitable for different per-
formance scenarios, like live electronics, sound installations
and telematic concerts.

</abstract>
    <keywords> rhythm generation, biological neural networks, complex pat- terns, musical interface, network performance  </keywords>
  </document>
  <document>
    <name>nime2011_337.pdf</name>
    <abstract>
This paper presents a novel algorithm that has been specif-
ically designed for the recognition of multivariate tempo-
ral musical gestures. The algorithm is based on Dynamic
Time Warping and has been extended to classify any N -
dimensional signal, automatically compute a classification
threshold to reject any data that is not a valid gesture and
be quickly trained with a low number of training examples.
The algorithm is evaluated using a database of 10 temporal
gestures performed by 10 participants achieving an average
cross-validation result of 99%.

</abstract>
    <keywords> Dynamic Time Warping, Gesture Recognition, Musician- Computer Interaction, Multivariate Temporal Gestures  </keywords>
  </document>
  <document>
    <name>nime2011_343.pdf</name>
    <abstract>
This paper presents the SARC EyesWeb Catalog, (SEC),
a machine learning toolbox that has been specifically devel-
oped for musician-computer interaction. The SEC features
a large number of machine learning algorithms that can be
used in real-time to recognise static postures, perform re-
gression and classify multivariate temporal gestures. The
algorithms within the toolbox have been designed to work
with any N -dimensional signal and can be quickly trained
with a small number of training examples. We also provide
the motivation for the algorithms used for the recognition
of musical gestures to achieve a low intra-personal gener-
alisation error, as opposed to the inter-personal generalisa-
tion error that is more common in other areas of human-
computer interaction.

</abstract>
    <keywords> Machine learning, gesture recognition, musician-computer interaction, SEC  </keywords>
  </document>
  <document>
    <name>nime2011_349.pdf</name>
    <abstract>
In composer Tod Machover's new opera Death and the Pow-
ers, the main character uploads his consciousness into an
elaborate computer system to preserve his essence and agency
after his corporeal death. Consequently, for much of the
opera, the stage and the environment itself come alive as
the main character. This creative need brings with it a host
of technical challenges and opportunities. In order to satisfy
the needs of this storyline, Machover's Opera of the Future
group at the MIT Media Lab has developed a suite of new
performance technologies, including robot characters, inter-
active performance capture systems, mapping systems for
authoring interactive multimedia performances, new mu-
sical instruments, unique spatialized sound controls, and
a unified control system for all these technological com-
ponents. While developed for a particular theatrical pro-
duction, many of the concepts and design procedures re-
main relevant to broader contexts including performance,
robotics, and interaction design.

</abstract>
    <keywords> opera, Death and the Powers, Tod Machover, gestural in- terfaces, Disembodied Performance, ambisonics  </keywords>
  </document>
  <document>
    <name>nime2011_355.pdf</name>
    <abstract>
In this paper we introduce a multimodal platform for Hy-
brid Reality live performances: by means of non-invasive
Virtual Reality technology, we developed a system to present
artists and interactive virtual objects in audio/visual chore-
ographies on the same real stage. These choreographies
could include spectators too, providing them with the pos-
sibility to directly modify the scene and its audio/visual fea-
tures. We also introduce the first interactive performance
staged with this technology, in which an electronic musi-
cian played live five tracks manipulating the 3D projected
visuals. As questionnaires have been distributed after the
show, in the last part of this work we discuss the analysis
of collected data, underlining positive and negative aspects
of the proposed experience.

This paper belongs together with a performance proposal
called Dissonance, in which two performers exploit the plat-
form to create a progressive soundtrack along with the ex-
ploration of an interactive virtual environment.

</abstract>
    <keywords> Interactive Performance, Hybrid Choreographies, Virtual Reality, Music Control  </keywords>
  </document>
  <document>
    <name>nime2011_361.pdf</name>
    <abstract> 
We conducted three studies with contemporary music compos-
ers at IRCAM. We found that even highly computer-literate 
composers use an iterative process that begins with expressing 
musical ideas on paper, followed by active parallel exploration 
on paper and in software, prior to final execution of their ideas 
as an original score. We conducted a participatory design study 
that focused on the creative exploration phase, to design tools 
that help composers better integrate their paper-based and elec-
tronic activities. We then developed InkSplorer as a technology 
probe that connects users' hand-written gestures on paper to 
Max/MSP and OpenMusic. Composers appropriated InkSplorer 
according to their preferred composition styles, emphasizing its 
ability to help them quickly explore musical ideas on paper as 
they interact with the computer. We conclude with recommen-
dations for designing interactive paper tools that support the 
creative process, letting users explore musical ideas both on 
paper and electronically.  

</abstract>
    <keywords>  Composer, Creativity, Design Exploration, InkSplorer, Interac- tive Paper, OpenMusic, Technology Probes.   </keywords>
  </document>
  <document>
    <name>nime2011_367.pdf</name>
    <abstract>
The DJ culture uses a gesture lexicon strongly rooted in the
traditional setup of turntables and a mixer. As novel tools
are introduced in the DJ community, this lexicon is adapted
to the features they provide. In particular, multitouch tech-
nologies can offer a new syntax while still supporting the old
lexicon, which is desired by DJs.

We present a classification of DJ tools, from an interac-
tion point of view, that divides the previous work into Tra-
ditional, Virtual and Hybrid setups. Moreover, we present
a multitouch tabletop application, developed with a group
of DJ consultants to ensure an adequate implementation of
the traditional gesture lexicon.

To conclude, we conduct an expert evaluation, with ten
DJ users in which we compare the three DJ setups with our
prototype. The study revealed that our proposal suits ex-
pectations of Club/Radio-DJs, but fails against the mental
model of Scratch-DJs, due to the lack of haptic feedback to
represent the record's physical rotation. Furthermore, tests
show that our multitouch DJ setup, reduces task duration
when compared with Virtual setups.

</abstract>
    <keywords> DJing, Multitouch Interaction, Expert User evaluation, HCI  </keywords>
  </document>
  <document>
    <name>nime2011_373.pdf</name>
    <abstract>
As NIME's focus has expanded beyond the design reports
which were pervasive in the early days to include studies and
experiments involving music control devices, we report on a
particular area of activity that has been overlooked: designs
of music devices in experimental contexts. We demonstrate
this is distinct from designing for artistic performances, with
a unique set of novel challenges. A survey of methodological
approaches to experiments in NIME reveals a tendency to
rely on existing instruments or evaluations of new devices
designed for broader creative application. We present two
examples from our own studies that reveal the merits of
designing purpose-built devices for experimental contexts.

</abstract>
    <keywords> Experiment, Methodology, Instrument Design, DMIs  </keywords>
  </document>
  <document>
    <name>nime2011_377.pdf</name>
    <abstract>
This paper describes the design of Crackle, a interactive
sound and touch experience inspired by the CrackleBox.
We begin by describing a ruleset for Crackle's interaction
derived from the salient interactive qualities of the Crack-
leBox. An implementation strategy is then described for
realizing the ruleset as an application for the iPhone. The
paper goes on to consider the potential of using Crackle
as an encapsulated interaction paradigm for exploring arbi-
trary sound spaces, and concludes with lessons learned on
designing for multitouch surfaces as expressive input sen-
sors.

</abstract>
    <keywords> touchscreen, interface topology, mobile music, interaction paradigm, dynamic mapping, CrackleBox, iPhone  </keywords>
  </document>
  <document>
    <name>nime2011_381.pdf</name>
    <abstract>
This paper introduces Improcess, a novel cross-disciplinary
collaborative project focussed on the design and develop-
ment of tools to structure the communication between per-
former and musical process. We describe a 3-tiered archi-
tecture centering around the notion of a Common Music
Runtime, a shared platform on top of which inter-operating
client interfaces may be combined to form new musical in-
struments. This approach allows hardware devices such as
the monome to act as an extended hardware interface with
the same power to initiate and control musical processes
as a bespoke programming language. Finally, we reflect on
the structure of the collaborative project itself, which of-
fers an opportunity to discuss general research strategy for
conducting highly sophisticated technical research within a
performing arts environment such as the development of a
personal regime of preparation for performance.

</abstract>
    <keywords> Improvisation, live coding, controllers, monome, collabora- tion, concurrency, abstractions  </keywords>
  </document>
  <document>
    <name>nime2011_387.pdf</name>
  </document>
  <document>
    <name>nime2011_393.pdf</name>
    <abstract> 
The design space of fabric multitouch surface interaction is 
explored with emphasis on novel materials and construction 
techniques aimed towards reliable, repairable pressure sensing 
surfaces for musical applications.  

</abstract>
    <keywords>  Multitouch, surface interaction, piezoresistive, fabric sensor, e- textiles, tangible computing, drum controller   </keywords>
  </document>
  <document>
    <name>nime2011_399.pdf</name>
    <abstract>
This paper deals with the effects of integrated vibrotac-
tile feedback on the "feel" of a digital musical instrument
(DMI). Building on previous work developing a DMI with
integrated vibrotactile feedback actuators, we discuss how
to produce instrument-like vibrations, compare these sim-
ulated vibrations with those produced by an acoustic in-
strument and examine how the integration of this feedback
effects performer ratings of the instrument. We found that
integrated vibrotactile feedback resulted in an increase in
performer engagement with the instrument, but resulted in
a reduction in the perceived control of the instrument. We
discuss these results and their implications for the design of
new digital musical instruments.

</abstract>
    <keywords> Vibrotactile Feedback, Digital Musical Instruments, Feel, Loudspeakers  </keywords>
  </document>
  <document>
    <name>nime2011_405.pdf</name>
    <abstract> 
This paper presents a series of open-source firmwares for the 
latest iteration of the popular Arduino microcontroller 
platform. A portmanteau of Human Interface Device and 
Arduino, the HIDUINO project tackles a major problem in 
designing NIMEs: easily and reliably communicating with a 
host computer using standard MIDI over USB. HIDUINO 
was developed in conjunction with a class at the California 
Institute of the Arts intended to teach introductory-level 
human-computer and human-robot interaction within the 
context of musical controllers. We describe our frustration 
with existing microcontroller platforms and our experiences 
using the new firmware to facilitate the development and 
prototyping of new music controllers. 
 
</abstract>
    <keywords>  Arduino, USB, HID, MIDI, HCI, controllers, microcontrollers   </keywords>
  </document>
  <document>
    <name>nime2011_409.pdf</name>
    <abstract> 
We present a strategy for the improvement of wireless sensor 
data transmission latency, implemented in two current projects 
involving gesture/control sound interaction. Our platform was 
designed to be capable of accepting accessories using a digital 
bus. The receiver features a IEEE 802.15.4 microcontroller 
associated to a TCP/IP stack integrated circuit that transmits the 
received wireless data to a host computer using the Open Sound 
Control protocol. This paper details how we improved the 
latency and sample rate of the said technology while keeping 
the device small and scalable. 

</abstract>
    <keywords>  Embedded sensors, gesture recognition, wireless, sound and  music computing, interaction, 802.15.4, Zigbee.         </keywords>
  </document>
  <document>
    <name>nime2011_413.pdf</name>
    <abstract> 
The Snyderphonics Manta controller is a USB touch controller 
for music and video. It features 48 capacitive touch sensors, 
arranged in a hexagonal grid, with bi-color LEDs that are 
programmable from the computer. The sensors send continuous 
data proportional to surface area touched, and a velocity-
detection algorithm has been implemented to estimate attack 
velocity based on this touch data.  In addition to these 
hexagonal sensors, the Manta has two high-dimension touch 
sliders (giving 12-bit values), and four assignable function 
buttons. In this paper, I outline the features of the controller, the 
available methods for communicating between the device and a 
computer, and some current uses for the controller. 
 
</abstract>
    <keywords>  Snyderphonics, Manta, controller, USB, capacitive, touch,  sensor, decoupled LED, hexagon, grid, touch slider, HID,  portable, wood, live music, live video   </keywords>
  </document>
  <document>
    <name>nime2011_421.pdf</name>
    <abstract> 
This paper deals with the usage of bio-data from performers to 

create interactive multimedia performances or installations. It 

presents this type of research in some art works produced in the 

last fifty years (such as Lucier's Music for a Solo Performance, 

from 1965), including two interactive performances of my 

authorship, which use two different types of bio-interfaces: on the 

one hand, an EMG (Electromyography) and on the other hand, an 

EEG (electroencephalography). The paper explores the interaction 

between the human body and real-time media (audio and visual) 

by the usage of bio-interfaces.  This research is based on 

biofeedback investigations pursued by the psychologist Neal E. 

Miller in the 1960s, mainly based on finding new methods to 

reduce stress. However, this article explains and shows examples 
in which biofeedback research is used for artistic purposes only. 

</abstract>
    <keywords>  Live electronics, Butoh, performance, biofeedback, interactive  sound and video.    </keywords>
  </document>
  <document>
    <name>nime2011_425.pdf</name>
    <abstract>
TresnaNet explores the potential of Telematics as a generator of

musical expressions. I pretend to sound the silent flow of
information from the network.

This is realized through the fabrication of a prototype

following the intention of giving substance to the intangible

parameters of our communication. The result may have

educational, commercial and artistic applications because it is a

physical and perceptible representation of the transfer of

information over the network. This paper describes the design,
implementation and conclusions about TresnaNet.

</abstract>
    <keywords> Interface, musical generation, telematics, network, musical  instrument, network sniffer.  </keywords>
  </document>
  <document>
    <name>nime2011_429.pdf</name>
    <keywords>  Music interfaces, music therapy, modifiable interfaces, design  tools, Human-Technology Interaction (HTI), User-Centred  Design (UCD), design for all (DfA), prototyping, performance.     </keywords>
  </document>
  <document>
    <name>nime2011_433.pdf</name>
    <abstract>
Motion-based interactive systems have long been utilized
in contemporary dance performances. These performances
bring new insight to sound-action experiences in multidisci-
plinary art forms. This paper discusses the related technol-
ogy within the framework of the dance piece, Raja. The per-
formance set up of Raja gives a possibility to use two com-
plementary tracking systems and two alternative choices for
motion sensors in real-time audio-visual synthesis.

</abstract>
    <keywords> raja, performance, dance, motion sensor, accelerometer, gyro, positioning, sonification, pure data, visualization, Qt  </keywords>
  </document>
  <document>
    <name>nime2011_437.pdf</name>
    <keywords>  Controller, Sensor, MIDI, USB, Computer Music, USB,  OSC, CV, MIDI, DMX, A/D Converter, Interface.    </keywords>
  </document>
  <document>
    <name>nime2011_441.pdf</name>
    <abstract>
The aim of this study was to investigate how well subjects
beat out a rhythm using eye movements and to establish
the most accurate method of doing this. Eighteen subjects
participated in an experiment were five different methods
were evaluated. A fixation based method was found to be
the most accurate. All subjects were able to synchronize
their eye movements with a given beat but the accuracy
was much lower than usually found in finger tapping stud-
ies. Many parts of the body are used to make music but so
far, with a few exceptions, the eyes have been silent. The re-
search presented here provides guidelines for implementing
eye controlled musical interfaces. Such interfaces would en-
able performers and artists to use eye movement for musical
expression and would open up new, exiting possibilities.

</abstract>
    <keywords> Rhythm, Eye tracking, Sensorimotor synchronization, Eye tapping  </keywords>
  </document>
  <document>
    <name>nime2011_445.pdf</name>
    <abstract> 
I present MelodyMorph, a reconfigurable musical instrument 
designed with a focus on melodic improvisation. It is designed 
for a touch-screen interface, and allows the user to create 
"bells" which can be tapped to play a note, and dragged around 
on a pannable and zoomable canvas. Colors, textures and 
shapes of the bells represent pitch and timbre properties. 
"Recorder bells" can store and play back performances. Users 
can construct instruments that are modifiable as they play, and 
build up complex melodies hierarchically from simple parts. 
</abstract>
    <keywords>  Melody, improvisation, representation, multi-touch, iPad   </keywords>
  </document>
  <document>
    <name>nime2011_453.pdf</name>
    <abstract> 
In this paper we discuss how the band 000000Swan uses 
machine learning to parse complex sensor data and create 
intricate artistic systems for live performance. Using the 
Wekinator software for interactive machine learning, we have 
created discrete and continuous models for controlling audio 
and visual environments using human gestures sensed by a 
commercially-available sensor bow and the Microsoft Kinect. 
In particular, we have employed machine learning to quickly 
and easily prototype complex relationships between performer 
gesture and performative outcome. 
 
</abstract>
    <keywords>  Wekinator, K-Bow, Machine Learning, Interactive,  Multimedia, Kinect, Motion-Tracking, Bow Articulation,  Animation    </keywords>
  </document>
  <document>
    <name>nime2011_457.pdf</name>
    <abstract>
In the past decade we have seen a growing presence of table-
top systems applied to music, lately with even some prod-
ucts becoming commercially available and being used by
professional musicians in concerts. The development of this
type of applications requires several demanding technical
expertises such as input processing, graphical design, real
time sound generation or interaction design, and because of
this complexity they are usually developed by a multidisci-
plinary group.

In this paper we present the Musical Tabletop Coding
Framework (MTCF) a framework for designing and coding
musical tabletop applications by using the graphical pro-
gramming language for digital sound processing Pure Data
(Pd). With this framework we try to simplify the creation
process of such type of interfaces, by removing the need of
any programming skills other than those of Pd.

</abstract>
    <keywords> Pure Data, tabletop, tangible, framework  </keywords>
  </document>
  <document>
    <name>nime2011_465.pdf</name>
    <abstract>
This paper documents the first developmental phase of an
interface that enables the performance of live music using
gestures and body movements. The work included focuses
on the first step of this project: the composition and per-
formance of live music using hand gestures captured using a
single data glove. The paper provides a background to the
field, the aim of the project and a technical description of
the work completed so far. This includes the development
of a robust posture vocabulary, an artificial neural network-
based posture identification process and a state-based sys-
tem to map identified postures onto a set of performance
processes. The paper is closed with qualitative usage obser-
vations and a projection of future plans.

</abstract>
    <keywords> Music Controller, Gestural Music, Data Glove, Neural Net- work, Live Music Composition, Looping, Imogen Heap  </keywords>
  </document>
  <document>
    <name>nime2011_469.pdf</name>
    <abstract> 
The use of non-invasive electroencephalography (EEG) in the 
experimental arts is not a novel concept. Since 1965, EEG has 
been used in a large number of, sometimes highly sophisticated, 
systems for musical and artistic expression. However, since the 
advent of the synthesizer, most such systems have utilized 
digital and/or synthesized media in sonifying the EEG signals. 
There have been relatively few attempts to create interfaces for 
musical expression that allow one to mechanically manipulate 
acoustic instruments by modulating one's mental state. 
Secondly, few such systems afford a distributed performance 
medium, with data transfer and audience participation 
occurring over the Internet. The use of acoustic instruments and 
Internet-enabled communication expands the realm of 
possibilities for musical expression in Brain-Computer Music 
Interfaces (BCMI), while also introducing additional 
challenges. In this paper we report and examine a first 
demonstration (Music for Online Performer) of a novel system 
for Internet-enabled manipulation of robotic acoustic 
instruments, with feedback, using a non-invasive EEG-based 
BCI and low-cost, commercially available robotics hardware. 
 
</abstract>
    <keywords>  EEG, Brain-Computer Music Interface, Internet, Arduino.   </keywords>
  </document>
  <document>
    <name>nime2011_473.pdf</name>
    <abstract>
A shoe-based interface is presented, which enables users to
play percussive virtual instruments by tapping their feet.
The wearable interface consists of a pair of sandals equipped
with four force sensors and four actuators affording audio-
tactile feedback. The sensors provide data via wireless trans-
mission to a host computer, where they are processed and
mapped to a physics-based sound synthesis engine. Since
the system provides OSC and MIDI compatibility, alterna-
tive electronic instruments can be used as well. The audio
signals are then sent back wirelessly to audio-tactile exciters
embedded in the sandals' sole, and optionally to headphones
and external loudspeakers. The round-trip wireless commu-
nication only introduces very small latency, thus guaran-
teeing coherence and unity in the multimodal percept and
allowing tight timing while playing.

</abstract>
    <keywords> interface, audio, tactile, foot tapping, embodiment, footwear, wireless, wearable, mobile  </keywords>
  </document>
  <document>
    <name>nime2011_477.pdf</name>
    <abstract>
We present a generic, structured model for design and eval-
uation of musical interfaces. This model is development
oriented, and it is based on the fundamental function of the
musical interfaces, i.e., to coordinate the human action and
perception for musical expression, subject to human capa-
bilities and skills. To illustrate the particulars of this model
and present it in operation, we consider the previous design
and evaluation phase of iPalmas, our testbed for exploring
rhythmic interaction. Our findings inform the current de-
sign phase of iPalmas visual and auditory displays, where
we build on what has resonated with the test users, and ex-
plore further possibilities based on the evaluation results.

</abstract>
  </document>
  <document>
    <name>nime2011_481.pdf</name>
    <abstract>
This paper introduces and evaluates a novel methodology
for the estimation of bow pressing force in violin perfor-
mance, aiming at a reduced intrusiveness while maintaining
high accuracy. The technique is based on using a simplified
physical model of the hair ribbon deflection, and feeding this
model solely with position and orientation measurements of
the bow and violin spatial coordinates. The physical model
is both calibrated and evaluated using real force data ac-
quired by means of a load cell.

</abstract>
    <keywords> bow pressing force, bow force, pressing force, force, violin playing, bow simplified physical model, 6DOF, hair ribbon ends, string ends  </keywords>
  </document>
  <document>
    <name>nime2011_487.pdf</name>
    <abstract>
Remixing audio samples is a common technique for the cre-
ation of electronic music, and there are a wide variety of
tools available to edit, process, and recombine pre-recorded
audio into new compositions. However, all of these tools
conceive of the timeline of the pre-recorded audio and the
playback timeline as identical. In this paper, we introduce
a dual time axis representation in which these two time-
lines are described explicitly. We also discuss the random
access remix application for the iPad, an audio sample ed-
itor based on this representation. We describe an initial
user study with 15 high school students that indicates that
the random access remix application has the potential to
develop into a useful and interesting tool for composers and
performers of electronic music.

</abstract>
    <keywords> interactive systems, sample editor, remix, iPad, multi-touch  </keywords>
  </document>
  <document>
    <name>nime2011_491.pdf</name>
    <abstract>
This paper outlines the formation of the Expanded Perfor-
mance (EP) trio, a chamber ensemble comprised of electric
cello with sensor bow, augmented digital percussion, and
digital turntable with mixer. Decisions relating to phys-
ical set-ups and control capabilities, sonic identities, and
mappings of each instrument, as well as their roles within
the ensemble, are explored. The contributions of these fac-
tors to the design of a coherent, expressive ensemble and
its emerging performance practice are considered. The trio
proposes solutions to creation, rehearsal and performance
issues in ensemble live electronics.

</abstract>
    <keywords> Live electronics, digital performance, mapping, chamber music, ensemble, instrument identity  </keywords>
  </document>
  <document>
    <name>nime2011_495.pdf</name>
    <abstract>
We present observations from two separate studies of spec-
tators' perceptions of musical performances, one involving
two acoustic instruments, the other two electronic instru-
ments. Both studies followed the same qualitative method,
using structured interviews to ascertain and compare spec-
tators' experiences. In this paper, we focus on outcomes
pertaining to perceptions of the performers' skill, relating
to concepts of embodiment and communities of practice.

</abstract>
    <keywords> skill, embodiment, perception, effort, control, spectator  </keywords>
  </document>
  <document>
    <name>nime2011_499.pdf</name>
    <keywords>  Computer music, programming language, the psychology of  programming, usability   </keywords>
  </document>
  <document>
    <name>nime2011_503.pdf</name>
    <abstract>
This paper introduces the Seaboard, a new tangible musical
instrument which aims to provide musicians with significant
capability to manipulate sound in real-time in a musically
intuitive way. It introduces the core design features which
make the Seaboard unique, and describes the motivation
and rationale behind the design. The fundamental approach
to dealing with problems associated with discrete and con-
tinuous inputs is summarized.

</abstract>
    <keywords> Piano keyboard-related interface, continuous and discrete control, haptic feedback, Human-Computer Interaction (HCI)  </keywords>
  </document>
  <document>
    <name>nime2011_507.pdf</name>
    <keywords>  Interactive music, public displays, user experience, out-of- home media, algorithmic composition, soft constraints   </keywords>
  </document>
  <document>
    <name>nime2011_511.pdf</name>
    <abstract> 
The traditional role of the musical instrument is to be the 
working tool of the professional musician. On the instrument 
the musician performs music for the audience to listen to. In 
this paper we present an interactive installation, where we 
expand the role of the instrument to motivate musicking and co-
creation between diverse users. We have made an open 
installation, where users can perform a variety of actions in 
several situations. By using the abilities of the computer, we 
have made an installation, which can be interpreted to have 
many roles. It can both be an instrument, a co-musician, a 
communication partner, a toy, a meeting place and an ambient 
musical landscape. The users can dynamically shift between 
roles, based on their abilities, knowledge and motivation. 
 
</abstract>
  </document>
  <document>
    <name>nime2011_515.pdf</name>
    <abstract>
We developed very small and light sensors, each equipped
with 3-axes accelerometers, magnetometers and gyroscopes.
Those MARG (Magnetic, Angular Rate, and Gravity) sen-
sors allow for a drift-free attitude computation which in turn
leads to the possibility of recovering the skeleton of body
parts that are of interest for the performance, improving
the results of gesture recognition and allowing to get rela-
tive position between the extremities of the limbs and the
torso of the performer. This opens new possibilities in terms
of mapping. We kept our previous approach developed at
ARTeM [2]: wireless from the body to the host computer,
but wired through a 4-wire digital bus on the body. By
relieving the need for a transmitter on each sensing node,
we could built very light and flat sensor nodes that can be
made invisible under the clothes. Smaller sensors, coupled
with flexible wires on the body, give more freedom of move-
ment to dancers despite the need for cables on the body.
And as the weight of each sensor node, box included, is
only 5 grams (Figure 1), they can also be put on the up-
per and lower arm and hand of a violin or viola player, to
retrieve the skeleton from the torso to the hand, without
adding any weight that would disturb the performer. We
used those sensors in several performances with a dancing
viola player and in one where she was simultaneously con-
trolling gas flames interactively. We are currently applying
them to other types of musical performances.

</abstract>
    <keywords> wireless MARG sensors  </keywords>
  </document>
  <document>
    <name>nime2011_519.pdf</name>
    <abstract> 
This paper covers and also describes an ongoing research 
project focusing on new artistic possibilities by exchanging 
music technological methods and techniques between two 
distinct musical genres. 
 Through my background as a guitarist and composer in an 
experimental metal band I have experienced a vast 
development in music technology during the last 20 years. This 
development has made a great impact in changing the 
procedures for composing and producing music within my 
genre without necessarily changing the strategies of how the 
technology is used. The transition from analogue to digital 
sound technology not only opened up new ways of 
manipulating and manoeuvring sound, it also opened up 
challenges in how to integrate and control the digital sound 
technology as a seamless part of my musical genre. By using 
techniques and methods known from electro-acoustic/computer 
music, and adapting them for use within my tradition, this 
research aims to find new strategies for composing and 
producing music within my genre. 
 
</abstract>
    <keywords>  Artistic research, strategies for composition and production,  convolution, environmental sounds, real time control   </keywords>
  </document>
  <document>
    <name>nime2011_523.pdf</name>
    <keywords>  LPC, software instrument, analysis, modeling, csound   </keywords>
  </document>
  <document>
    <name>nime2011_527.pdf</name>
    <abstract> 
Gliss is an application for iOS that lets the user sequence five 
separate instruments and play them back in various ways. 
Sequences can be created by drawing onto the screen while the 
sequencer is running. The playhead of the sequencer can be set 
to randomly deviate from the drawings or can be controlled via 
the accelerometer of the device. This makes Gliss a hybrid of a 
sequencer, an instrument and a generative music system. 
 
</abstract>
    <keywords>  Gliss, iOS, iPhone, iPad, interface, UPIC, music, sequencer,  accelerometer, drawing   </keywords>
  </document>
  <document>
    <name>nime2011_529.pdf</name>
    <abstract>
This paper describes a new musical instrument inspired by the 
pedal-steel  guitar,  along  with  its  motivations  and  other 
considerations.   Creating  a  multi-dimensional,  expressive 
instrument was the primary driving force.  For these criteria the 
pedal steel guitar proved an apt model as it allows control over 
several instrument parameters simultaneously and continuously. 
The parameters we wanted  control over were volume, timbre,  
release time and pitch.
The  Quadrofeelia  is  played  with  two  hands  on a  horizontal 
surface.  Single notes and melodies are easily played as well as 
chordal accompaniment with a variety of timbres and release 
times  enabling  a  range  of  legato  and  staccato  notes  in  an 
intuitive manner with a new yet familiar interface.

</abstract>
    <keywords> NIME, pedal-steel, electronic, slide, demonstration, membrane,  continuous, ribbon, instrument, polyphony, lead  </keywords>
  </document>
  <document>
    <name>nime2011_533.pdf</name>
    <abstract> 
In this paper, we suggest a conceptual model of a Web 
application framework for the composition and documentation 
of soundscape and introduce corresponding prototype projects, 
SeoulSoundMap and SoundScape Composer. We also survey 
the current Web-based sound projects in terms of soundscape 
documentation. 
 
</abstract>
    <keywords>  soundscape, web application framework, sound archive, sound  map, soundscape composition, soundscape documentation.     </keywords>
  </document>
  <document>
    <name>nime2011_535.pdf</name>
    <abstract>
We are presenting a set of applications that have been real-
ized with the MO modular wireless motion capture device
and a set of software components integrated into Max/MSP.
These applications, created in the context of artistic projects,
music pedagogy, and research, allow for the gestural re-
embodiment of recorded sound and music. They demon-
strate a large variety of different "playing techniques" in
musical performance using wireless motion sensor modules
in conjunction with gesture analysis and real-time audio
processing components.

</abstract>
    <keywords> Music, Gesture, Interface, Wireless Sensors, Gesture Recog- nition, Audio Processing, Design, Interaction  </keywords>
  </document>
  <document>
    <name>nime2011_537.pdf</name>
    <abstract>
(land)moves is an interactive installation: the user's ges-
tures control the multimedia processing with a total synergy
between audio and video synthesis and treatment.

</abstract>
    <keywords> mapping gesture-audio-video, gesture recognition, landscape, soundscape  </keywords>
  </document>
  <document>
    <name>nime2011_539.pdf</name>
    <abstract>
Haptic interfaces using active force-feedback have mostly been 
used for emulating  existing  instruments and making 
conventional music.  With the right speed, force, precision and 
software they can also be used to  make new sounds and 
perhaps new music.

 The requirements are local  microprocessors (for low-latency 
and high update rates), strategic sensors (for force as well as 
position), and non-linear dynamics (that make for rich 
overtones and chaotic music).

</abstract>
    <keywords> NIME, Haptics, Music Controllers, Microprocessors.   </keywords>
  </document>
  <document>
    <name>nime2012_009.pdf</name>
    <abstract> 
This paper describes three hardware devices for integrating 
modular synthesizers with computers, each with a different 
approach to the relationship between hardware and software. 
The devices discussed are the USB-Octomod, an 8-channel 
OSC-compatible computer-controlled control-voltage 
generator, the tabulaRasa, a hardware table-lookup oscillator 
synthesis module with corresponding waveform design 
software, and the pucktronix.snake.corral, a dual 8x8 
computer-controlled analog signal routing matrix. The devices 
make use of open-source hardware and software, and are 
designed around affordable micro-controllers and integrated 
circuits. 

</abstract>
  </document>
  <document>
    <name>nime2012_020.pdf</name>
    <abstract> 
The Sonik Spring is a portable and wireless digital instrument, 
created for real-time synthesis and control of sound. It brings 
together different types of sensory input, linking gestural motion 
and kinesthetic feedback to the production of sound. 
 The interface consists of a 15-inch spring with unique 
flexibility, which allows multiple degrees of variation in its 
shape and length. The design of the instrument is described and 
its features discussed. Three performance modes are detailed 
highlighting the instrument's expressive potential and wide 
range of functionality. 
 
</abstract>
    <keywords>  Interface for sound and music, Gestural control of sound,  Kinesthetic and visual feedback   </keywords>
  </document>
  <document>
    <name>nime2012_025.pdf</name>
    <keywords>  Antheil, Stravinsky, player piano, pianola, mechanical  instruments, synchronization   </keywords>
  </document>
  <document>
    <name>nime2012_030.pdf</name>
    <abstract>
This paper presents ongoing work on methods dedicated to
relations between composers and performers in the context
of experimental music. The computer music community has
over the last decade paid a strong interest on various kinds
of gestural interfaces to control sound synthesis processes.
The mapping between gesture and sound parameters has
specially been investigated in order to design the most rel-
evant schemes of sonic interaction. In fact, this relevance
results in an aesthetic choice that encroaches on the pro-
cess of composition. This work proposes to examine the
relations between composers and performers in the context
of the new interfaces for musical expression. It aims to de-
fine a theoretical and methodological framework clarifying
these relations. In this project, this paper is the first exper-
imental study about the use of physical models as gestural
maps for the production of textural sounds.

</abstract>
    <keywords> Simulation, Interaction, Sonic textures  </keywords>
  </document>
  <document>
    <name>nime2012_036.pdf</name>
    <abstract> 
Musician Maker is a system to allow novice players the 
opportunity to create expressive improvisational music.  While 
the system plays an accompaniment background chord 
progression, each participant plays some kind of controller to 
make music through the system.  The program takes the signals 
from the controllers and adjusts the pitches somewhat so that 
the players are limited to notes which fit the chord progression.  
The various controllers are designed to be very easy and 
intuitive so anyone can pick one up and quickly be able to play 
it.  Since the computer is making sure that wrong notes are 
avoided, even inexperienced players can immediately make 
music and enjoy focusing on some of the more expressive 
elements and thus become musicians. 

</abstract>
    <keywords>  Musical Instrument, Electronic, Computer Music, Novice,  Controller    </keywords>
  </document>
  <document>
    <name>nime2012_037.pdf</name>
    <abstract>
Force-feedback devices can provide haptic feedback during
interaction with physical models for sound synthesis. How-
ever, low-end devices may not always provide high-fidelity
display of the acoustic characteristics of the model. This ar-
ticle describes an enhanced handle for the Phantom Omni
containing a vibration actuator intended to display the high-
frequency portion of the synthesized forces. Measurements
are provided to show that this approach achieves a more
faithful representation of the acoustic signal, overcoming
limitations in the device control and dynamics.

</abstract>
    <keywords> Haptics, force feedback, bowing, audio, interaction  </keywords>
  </document>
  <document>
    <name>nime2012_047.pdf</name>
    <abstract> 
With the advent of high resolution digital video projection and 
high quality spatial sound systems in modern planetariums, the 
planetarium can become the basis for a unique set of virtual 
musical instrument capabilities that go well beyond packaged 
multimedia shows. The dome, circular speaker and circular 
seating arrangements provide means for skilled composers and 
performers to create a virtual reality in which attendees are 
immersed in the composite instrument. 
 This initial foray into designing an audio-visual computer-
based instrument for improvisational performance in a 
planetarium builds on prior, successful work in mapping the 
rules and state of two-dimensional computer board games to 
improvised computer music. The unique visual and audio 
geometries of the planetarium present challenges and 
opportunities. The game tessellates the dome in mobile, colored 
hexagons that emulate both atoms and musical scale intervals in 
an expanding universe. Spatial activity in the game maps to 
spatial locale and instrument voices in the speakers, in essence 
creating a virtual orchestra with a string section, percussion 
section, etc. on the dome. Future work includes distribution of 
game play via mobile devices to permit attendees to participate 
in a performance. This environment is open-ended, with great 
educational and aesthetic potential. 
 
</abstract>
    <keywords>  aleatory music, algorithmic improvisation, computer game,  planetarium   </keywords>
  </document>
  <document>
    <name>nime2012_048.pdf</name>
    <abstract> 
This paper is an in depth exploration of the fashion object and 
device, the Play-A-Grill. It details inspirations, socio-cultural 
implications, technical function and operation, and potential 
applications for the Play-A-Grill system. 

</abstract>
    <keywords>  Digital Music Players, Hip Hop, Rap, Music Fashion, Grills,  Mouth Jewelry, Mouth Controllers, and Bone Conduction  Hearing.   </keywords>
  </document>
  <document>
    <name>nime2012_057.pdf</name>
    <abstract> 
Sound generators and synthesis engines expose a large set of 
parameters, allowing run-time timbre morphing and exploration 
of sonic space. However, control over these high-dimensional 
interfaces is constrained by the physical limitations of 
performers. In this paper we propose the exploitation of vocal 
gesture as an extension or alternative to traditional physical 
controllers. The approach uses dynamic aspects of vocal sound 
to control variations in the timbre of the synthesized sound. The 
mapping from vocal to synthesis parameters is automatically 
adapted to information extracted from vocal examples as well 
as to the relationship between parameters and timbre within the 
synthesizer. The mapping strategy aims to maximize the 
breadth of the explorable perceptual sonic space over a set of 
the synthesizer's real-valued parameters, indirectly driven by 
the voice-controlled interface. 

 
</abstract>
  </document>
  <document>
    <name>nime2012_060.pdf</name>
    <abstract> 
In this work, a comprehensive study is performed on the 

relationship between audio, visual and emotion by applying the 

principles of cognitive emotion theory into digital creation. The 

study is driven by an audiovisual emotion library project that is 

named AVIEM, which provides an interactive interface for 

experimentation and evaluation of the perception and creation 

processes of audiovisuals. AVIEM primarily consists of 

separate audio and visual libraries and grows with user 

contribution as users explore different combinations between 

them. The library provides a wide range of experimentation 

possibilities by allowing users to create audiovisual relations 

and logging their emotional responses through its interface. 

Besides being a resourceful tool of experimentation, AVIEM 

aims to become a source of inspiration, where digitally created 

abstract virtual environments and soundscapes can elicit target 

emotions at a preconscious level, by building genuine 

audiovisual relations that would engage the viewer on a strong 

emotional stage. Lastly, various schemes are proposed to 

visualize information extracted through AVIEM, to improve 

the navigation and designate the trends and dependencies 

among audiovisual relations.  

</abstract>
    <keywords>  Designing emotive audiovisuals, cognitive emotion theory,   audiovisual perception and interaction, synaesthesia   </keywords>
  </document>
  <document>
    <name>nime2012_061.pdf</name>
    <abstract> 
Tok! is a collaborative acoustic instrument application for iOS 

devices aimed at real time percussive music making in a co-

located setup. It utilizes the mobility of hand-held devices and 

transforms them into drumsticks to tap on flat surfaces and 

produce acoustic music. Tok! is also networked and consists of 

a shared interactive music score to which the players tap their 

phones, creating a percussion ensemble. Through their social 

interaction and real-time modifications to the music score, and 

through their creative selection of tapping surfaces, the players 

can collaborate and dynamically create interesting rhythmic 

music with a variety of timbres.  

</abstract>
    <keywords>  Mobile Phones, Collaboration, Social Interaction, Acoustic   Musical Instrument   </keywords>
  </document>
  <document>
    <name>nime2012_062.pdf</name>
    <abstract> 
This paper describes recent extensions to LOLC, a text-based 
environment for collaborative improvisation for laptop 
ensembles, which integrate acoustic instrumental musicians 
into the environment. Laptop musicians author short 
commands to create, transform, and share pre-composed 
musical fragments, and the resulting notation is digitally 
displayed, in real time, to instrumental musicians to sight-read 
in performance. The paper describes the background and 
motivations of the project, outlines the design of the original 
LOLC environment and describes its new real-time notation 
components in detail, and explains the use of these new 
components in a musical composition, SGLC, by one of the 
authors. 
</abstract>
  </document>
  <document>
    <name>nime2012_063.pdf</name>
    <abstract>
Platforms for mobile computing and gesture recognition
provide enticing interfaces for creative expression on virtual
musical instruments. However, sound synthesis on these
systems is often limited to sample-based synthesizers, which
limits their expressive capabilities. Source-filter models are
adept for such interfaces since they provide flexible, algo-
rithmic sound synthesis, especially in the case of the guitar.
In this paper, we present a data-driven approach for mod-
eling guitar excitation signals using principal components
derived from a corpus of excitation signals. Using these
components as features, we apply nonlinear principal com-
ponents analysis to derive a feature space that describes
the expressive attributes characteristic to our corpus. Fi-
nally, we propose using the reduced dimensionality space as
a control interface for an expressive guitar synthesizer.

</abstract>
    <keywords> Source-filter models, musical instrument synthesis, PCA, touch musical interfaces  </keywords>
  </document>
  <document>
    <name>nime2012_064.pdf</name>
    <abstract> 
This article proposes a wireless handheld multimedia digital 
instrument, which allows one to compose and perform digital 
music for films in real-time.  Not only does it allow the 
performer and the audience to follow the film images in 
question, but also the relationship between the gestures 
performed and the sound generated. Furthermore, it allows one 
to have an effective control over the sound, and consequently 
achieve great musical expression. In addition, a method for 
calibrating the multimedia digital instrument, devised to 
overcome the lack of a reliable reference point of the 
accelerometer and a process to obtain a video score are 
presented. This instrument has been used in a number of 
concerts (Portugal and Brazil) so as to test its robustness.  

   
</abstract>
  </document>
  <document>
    <name>nime2012_066.pdf</name>
    <abstract> 
Tweet Harp is a musical instrument using Twitter and a laser 
harp. This instrument features the use of the human voice 
speaking tweets in Twitter as sounds for music. It is played by 
touching the six harp strings of laser beams. Tweet Harp gets 
the latest tweets from Twitter in real-time, and it creates music 
like a song with unexpected words. It also creates animation 
displaying the texts at the same time. The audience can visually 
enjoy this performance by sounds synchronized with animation. 
If the audience has a Twitter account, they can participate in the 
performance by tweeting.  
 
</abstract>
    <keywords>  Twitter, laser harp, text, speech, voice, AppleScript, Quartz  Composer, Max/MSP, TTS, Arduino   </keywords>
  </document>
  <document>
    <name>nime2012_068.pdf</name>
    <abstract>
Machine learning models are useful and attractive tools for
the interactive computer musician, enabling a breadth of in-
terfaces and instruments. With current consumer hardware
it becomes possible to run advanced machine learning algo-
rithms in demanding performance situations, yet expertise
remains a prominent entry barrier for most would-be users.
Currently available implementations predominantly employ
supervised machine learning techniques, while the adaptive,
self-organizing capabilities of unsupervised models are not
generally available. We present a free, new toolbox of unsu-
pervised machine learning algorithms implemented in Max
5 to support real-time interactive music and video, aimed
at the non-expert computer artist.

</abstract>
    <keywords> NIME, unsupervised machine learning, adaptive resonance theory, self-organizing maps, Max 5  </keywords>
  </document>
  <document>
    <name>nime2012_070.pdf</name>
    <abstract>
We introduce a prototype of a new tangible step sequencer
that transforms everyday objects into percussive musical
instruments. DrumTop adapts our everyday task-oriented
hand gestures with everyday objects as the basis of musical
interaction, resulting in an easily graspable musical interface
for musical novices. The sound, tactile, and visual feedback
comes directly from everyday objects as the players program
drum patterns and rearrange the objects on the tabletop
interface. DrumTop encourages the players to explore the
musical potentiality of their surroundings and be musically
creative through rhythmic interactions with everyday ob-
jects. The interface consists of transducers that trigger a
hit, causing the objects themselves to produce sound when
they are in close contact with the transducers. We discuss
how we designed and implemented our current DrumTop
prototype and describe how players interact with the inter-
face. We then highlight the players' experience with Drum-
top and our plans for future work in the fields of music
education and performance.

</abstract>
    <keywords> Tangible User Interfaces, Playful Experience, Percussion, Step Sequencer, Transducers, Everyday Objects  </keywords>
  </document>
  <document>
    <name>nime2012_073.pdf</name>
    <abstract> 
This paper demonstrates the practical benefits and performance 
opportunities of using the dual-analog gamepad as a controller 
for real-time live electronics. Numerous diverse instruments 
and interfaces, as well as detailed control mappings, are 
described. Approaches to instrument and preset switching are 
also presented. While all of the instrument implementations 
presented are made available through the Martingale Pd library, 
resources for other synthesis languages are also described. 
 
</abstract>
    <keywords>  Controllers, live electronics, dual-analog, gamepad, joystick,  computer music, instrument, interface   </keywords>
  </document>
  <document>
    <name>nime2012_074.pdf</name>
    <abstract> 
Potential users of audio production software, such as parametric 
audio equalizers, may be discouraged by the complexity of the 
interface. A new approach creates a personalized on-screen 
slider that lets the user manipulate the audio in terms of a 
descriptive term (e.g. "warm"), without the user needing to 
learn or use the interface of an equalizer. This system learns 
mappings by presenting a sequence of sounds to the user and 
correlating the gain in each frequency band with the user's 
preference rating. The system speeds learning through transfer 
learning. Results on a study of 35 participants show how an 
effective, personalized audio manipulation tool can be 
automatically built after only three ratings from the user.   
 
</abstract>
    <keywords>  Human computer interaction, music, multimedia production,  transfer learning   </keywords>
  </document>
  <document>
    <name>nime2012_077.pdf</name>
    <abstract>
We describe a system that allows non-programmers to spec-
ify the grammar for a novel graphic score notation of their
own design, defining performance notations suitable for draw-
ing in live situations on a surface such as a whiteboard. The
score can be interpreted via the camera of a smartphone,
interactively scanned over the whiteboard to control the
parameters of synthesisers implemented in Overtone. The
visual grammar of the score, and its correspondence to the
sound parameters, can be defined by the user with a sim-
ple visual condition-action language. This language can be
edited on the touchscreen of an Android phone, allowing
the grammar to be modified live in performance situations.
Interactive scanning of the score is visible to the audience as
a performance interface, with a colour classifier and visual
feature recogniser causing the grammar-specified events to
be sent using OSC messages via Wi-Fi from the hand-held
smartphone to an audio workstation.

</abstract>
    <keywords> Graphic Notation, Disposable Notation, Live Coding, Com- puter Vision, Mobile Music  </keywords>
  </document>
  <document>
    <name>nime2012_082.pdf</name>
    <abstract> 
In this paper we present a multimodal system for analyzing 
drum performance. In the first example we perform automatic 
drum hand recognition utilizing a technique for automatic 
labeling of training data using direct sensors, and only indirect 
sensors (e.g. a microphone) for testing. Left/Right drum hand 
recognition is achieved with an average accuracy of 84.95% for 
two performers. Secondly we provide a study investigating 
multimodality dependent performance metrics analysis. 
</abstract>
    <keywords>  Multimodality, Drum stroke identification, surrogate sensors,  surrogate data training, machine learning, music information  retrieval, performance metrics   </keywords>
  </document>
  <document>
    <name>nime2012_094.pdf</name>
    <keywords> Multi-Touch, User Study, Relational-point interface  </keywords>
  </document>
  <document>
    <name>nime2012_096.pdf</name>
    <abstract>
TedStick is a new wireless musical instrument that processes
acoustic sounds resonating within its wooden body and ma-
nipulates them via gestural movements. The sounds are
transduced by a piezoelectric sensor inside the wooden body,
so any tactile contact with TedStick is transmitted as audio
and further processed by a computer. The main method
for performing with TedStick focuses on extracting diverse
sounds from within the resonant properties of TedStick it-
self. This is done by holding TedStick in one hand and
a standard drumstick in the opposite hand while tapping,
rubbing, or scraping the two against each other. Gestural
movements of TedStick are then mapped to parameters for
several sound effects including pitch shift, delay, reverb and
low/high pass filters. Using this technique the hand holding
the drumstick can control the acoustic sounds/interaction
between the sticks while the hand holding TedStick can fo-
cus purely on controlling the sound manipulation and effects
parameters.

</abstract>
    <keywords> tangible user interface, piezoelectric sensors, gestural per- formance, digital sound manipulation  </keywords>
  </document>
  <document>
    <name>nime2012_098.pdf</name>
    <abstract>
WIS platform is a wireless interactive sensor platform de-
signed to support dynamic and interactive applications. The
platform consists of a capture system which includes multi-
ple on-body Zigbee compatible motion sensors, a processing
unit and an audio-visual display control unit. It has a com-
plete open architecture and provides interfaces to interact
with other user-designed applications. Therefore, WIS plat-
form is highly extensible. Through gesture recognitions by
on-body sensor nodes and data processing, WIS platform
can offer real-time audio and visual experiences to the users.
Based on this platform, we set up a multimedia installation
that presents a new interaction model between the partic-
ipants and the audio-visual environment. Furthermore, we
are also trying to apply WIS platform to other installations
and performances.

</abstract>
    <keywords> Interactive, Audio-visual experience  </keywords>
  </document>
  <document>
    <name>nime2012_099.pdf</name>
    <abstract>
In this paper, we introduce Kritaanjli, a robotic harmo-
nium. Details concerning the design, construction, and use
of Kritaanjli are discussed. After an examination of related
work, quantitative research concerning the hardware chosen
in the construction of the instrument is shown, as is a thor-
ough exposition of the design process and use of CAD/CAM
techniques in the design lifecycle of the instrument. Addi-
tionally, avenues for future work and compositional prac-
tices are focused upon, with particular emphasis placed on
human/robot interaction, pedagogical techniques afforded
by the robotic instrument, and compositional avenues made
accessible through the use of Kritaanjli.

</abstract>
  </document>
  <document>
    <name>nime2012_100.pdf</name>
    <abstract>
A problem with many contemporary musical robotic percus-
sion systems lies in the fact that solenoids fail to respond lin-
early to linear increases in input velocity. This nonlinearity
forces performers to individually tailor their compositions
to specific robotic drummers. To address this problem, we
introduce a method of pre-performance calibration using
metaheuristic search techniques. A variety of such tech-
niques are introduced and evaluated and the results of the
optimized solenoid-based percussion systems are presented
and compared with output from non-calibrated systems.

</abstract>
  </document>
  <document>
    <name>nime2012_101.pdf</name>
    <abstract>
The EMvibe is an augmented vibraphone that allows for
continuous control over the amplitude and spectrum of in-
dividual notes. The system uses electromagnetic actuators
to induce vibrations in the vibraphone's aluminum tone
bars. The tone bars and the electromagnetic actuators are
coupled via neodymium magnets affixed to each bar. The
acoustic properties of the vibraphone allowed us to develop
a very simple, low-cost and powerful amplification solution
that requires no heat sinking. The physical design is meant
to be portable and robust, and the system can be easily in-
stalled on any vibraphone without interfering with normal
performance techniques. The system supports multiple in-
terfacing solutions, affording the performer and composer
the ability to interact with the EMvibe in different ways
depending on the musical context.

</abstract>
    <keywords> Vibraphone, augmented instrument, electromagnetic actu- ation  </keywords>
  </document>
  <document>
    <name>nime2012_102.pdf</name>
    <keywords> Augmented instruments, controllers, motion tracking, map- ping  </keywords>
  </document>
  <document>
    <name>nime2012_105.pdf</name>
    <abstract> 
The upper limit of frequency sensitivity for vibrotactile 
stimulation of the fingers and hand is commonly accepted as 1 
kHz. However, during the course of our research to develop a 
full-hand vibrotactile musical communication device for the 
hearing-impaired, we repeatedly found evidence suggesting 
sensitivity to higher frequencies.  Most of the studies on which 
vibrotactile sensitivity are based have been conducted using 
sine tones delivered by point-contact actuators. The current 
study was designed to investigate vibrotactile sensitivity using 
complex signals and full, open-hand contact with a flat 
vibrating surface representing more natural environmental 
conditions. Sensitivity to frequencies considerably higher than 
previously reported was demonstrated for all the signal types 
tested. Furthermore, complex signals seem to be more easily 
detected than sine tones, especially at low frequencies. Our 
findings are applicable to a general understanding of sensory 
physiology, and to the development of new vibrotactile display 
devices for music and other applications.  

 
</abstract>
    <keywords>  Haptic Sensitivity, Hearing-impaired, Vibrotactile Threshold   </keywords>
  </document>
  <document>
    <name>nime2012_108.pdf</name>
    <abstract>
In this paper strategies for augmenting the social dimension
of collaborative music making, in particular in the form
of bodily and situated interaction are presented. Mobile
instruments are extended by means of relational descrip-
tors democratically controlled by the group and mapped to
sound parameters. A qualitative evaluation approach is de-
scribed and a user test with participants playing in groups
of three conducted. The results of the analysis show core-
categories such as familiarity with instrument and situ-
ation , shift of focus in activity , family of interactions
and different categories of the experience emerging from
the interviews. Our evaluation shows the suitability of our
approach but also the need for iterating on our design on the
basis of the perspectives brought forth by the users. This
latter observation confirms the importance of conducting a
thorough interview session followed by data analysis on the
line of grounded theory.

</abstract>
    <keywords> Collaborative music making, evaluation methods, mobile music, human-human interaction.  </keywords>
  </document>
  <document>
    <name>nime2012_109.pdf</name>
    <keywords>  music, cochlear implants, perception, rehabilitation, auditory  training, interactive learning, client-centred software     </keywords>
  </document>
  <document>
    <name>nime2012_114.pdf</name>
    <abstract>
The Electric Slide Organistrum (Figure 1) is an acoustic
stringed instrument played through a video capture system.
The vibration of the instrument string is generated electro-
magnetically and the pitch variation is achieved by move-
ments carried out by the player in front of a video camera.
This instrument results from integrating an ancient tech-
nique for the production of sounds as it is the vibration of
a string on a soundbox and actual human-computer inter-
action technology such as motion detection.

Figure 1: Electric Slide Organistrum.

</abstract>
    <keywords> Gestural Interface, eBow, Pickup, Bowed string, Electro- magnetic actuation  </keywords>
  </document>
  <document>
    <name>nime2012_117.pdf</name>
    <abstract>
There is growing interest in the field of augmented musical
instruments, which extend traditional acoustic instruments
using new sensors and actuators. Several designs use elec-
tromagnetic actuation to induce vibrations in the acoustic
mechanism, manipulating the traditional sound of the in-
strument without external speakers. This paper presents
techniques and guidelines for the use of electromagnetic ac-
tuation in augmented instruments, including actuator de-
sign and selection, interfacing with the instrument, and cir-
cuits for driving the actuators. The material in this pa-
per forms the basis of the magnetic resonator piano, an
electromagnetically-augmented acoustic grand piano now in
its second design iteration. In addition to discussing appli-
cations to the piano, this paper aims to provide a toolbox
to accelerate the design of new hybrid acoustic-electronic
instruments.

</abstract>
    <keywords> augmented instruments, electromagnetic actuation, circuit design, hardware  </keywords>
  </document>
  <document>
    <name>nime2012_119.pdf</name>
    <abstract> 
This paper describes a recent addition to LOLC, a text-based 
environment for collaborative improvisation for laptop 
ensembles, incorporating a machine musician that plays along 
with human performers. The machine musician LOLbot 
analyses the patterns created by human performers and the 
composite music they create as they are layered in 
performance. Based on user specified settings, LOLbot 
chooses appropriate patterns to play with the ensemble, either 
to add contrast to the existing performance or to be coherent 
with the rhythmic structure of the performance. The paper 
describes the background and motivations of the project, 
outlines the design of the original LOLC environment and 
describes the architecture and implementation of LOLbot.  
 
</abstract>
    <keywords>  Machine Musicianship, Live Coding, Laptop Orchestra   </keywords>
  </document>
  <document>
    <name>nime2012_120.pdf</name>
    <abstract>
Corpus-based concatenative synthesis is a fairly recent
sound synthesis method, based on descriptor analysis of any
number of existing or live-recorded sounds, and synthesis
by selection of sound segments from the database matching
given sound characteristics. It is well described in the litera-
ture, but has been rarely examined for its capacity as a new
interface for musical expression. The interesting outcome
of such an examination is that the actual instrument is the
space of sound characteristics, through which the performer
navigates with gestures captured by various input devices.
We will take a look at different types of interaction modes
and controllers (positional, inertial, audio analysis) and the
gestures they afford, and provide a critical assessment of
their musical and expressive capabilities, based on several
years of musical experience, performing with the CataRT
system for real-time CBCS.

</abstract>
  </document>
  <document>
    <name>nime2012_123.pdf</name>
    <abstract> 
This paper presents the results of user interaction with two 
explorative music environments (sound system A and B) that 
were inspired from the Banda Linda music tradition in two 
different ways. The sound systems adapted to how a team of 
two players improvised and made a melody together in an 
interleaved fashion: Systems A and B used a fuzzy logic 
algorithm and pattern recognition to respond with modifications 
of a background rhythms. In an experiment with a pen tablet 
interface as the music instrument, users aged 10-13 were to tap 
tones and continue each other's melody. The sound systems 
rewarded users sonically, if they managed to add tones to their 
mutual melody in a rapid turn taking manner with rhythmical 
patterns. Videos of experiment sessions show that user teams 
contributed to a melody in ways that resemble conversation. 
Interaction data show that each sound system made player 
teams play in different ways, but players in general had a hard 
time adjusting to a non-Western music tradition. The paper 
concludes with a comparison and evaluation of the two sound 
systems. Finally it proposes a new approach to the design of 
collaborative and shared music environments that is based on 
"listening applications".  
 
</abstract>
    <keywords>  Music improvisation, novices, social learning, interaction  studies, interaction design.   </keywords>
  </document>
  <document>
    <name>nime2012_125.pdf</name>
    <abstract> 
SoundStrand is a tangible music composition tool. It 

demonstrates a paradigm developed to enable music 

composition through the use of tangible interfaces. This 

paradigm attempts to overcome the contrast between the 

relatively small of amount degrees of freedom usually 

demonstrated by tangible interfaces and the vast number of 

possibilities that musical composition presents. 

SoundStrand is comprised of a set of physical objects called 

cells, each representing a musical phrase. Cells can be 

sequentially connected to each other to create a musical theme. 

Cells can also be physically manipulated to access a wide range 

of melodic, rhythmic and harmonic variations. The 

SoundStrand software assures that as the cells are manipulated, 

the melodic flow, harmonic transitions and rhythmic patterns of 

the theme remain musically plausible while preserving the 

user's intentions. 

 

</abstract>
    <keywords>  Tangible, algorithmic, composition, computer assisted   </keywords>
  </document>
  <document>
    <name>nime2012_128.pdf</name>
    <abstract> 
massMobile is a client-server system for mass audience 
participation in live performances using smartphones.  It was 
designed to flexibly adapt to a variety of participatory 
performance needs and to a variety of performance venues. It 
allows for real time bi-directional communication between 
performers and audiences utilizing existing wireless 3G, 4G, or 
WiFi networks. In this paper, we discuss the goals, design, and 
implementation of the framework, and we describe several 
projects realized with massMobile.  
 
</abstract>
    <keywords>  audience participation, network music, smartphone,  performance, mobile   </keywords>
  </document>
  <document>
    <name>nime2012_131.pdf</name>
    <abstract>
This paper introduces the concept of Kugelschwung, a dig-
ital musical instrument centrally based around the use of
pendulums and lasers to create unique and highly inter-
active electronic ambient soundscapes. Here, we explore
the underlying design and physical construction of the in-
strument, as well as its implementation and feasibility as
an instrument in the real world. To conclude, we outline
potential expansions to the instrument, describing how its
range of applications can be extended to accommodate a
variety of musical styles.

</abstract>
    <keywords> laser, pendulums, instrument design, electronic, sampler, soundscape, expressive performance  </keywords>
  </document>
  <document>
    <name>nime2012_132.pdf</name>
  </document>
  <document>
    <name>nime2012_133.pdf</name>
    <abstract>
Performing music with a computer and loudspeakers repre-
sents always a challenge. The lack of a traditional instru-
ment requires the performer to study idiomatic strategies
by which musicianship becomes apparent. On the other
hand, the audience needs to decode those strategies, so to
achieve an understanding and appreciation of the music be-
ing played. The issue is particularly relevant to the per-
formance of music that results from the mediation between
biological signals of the human body and physical perfor-
mance.

The present article tackles this concern by demonstrating
a new model of musical performance; what I define biophys-
ical music. This is music generated and played in real time
by amplifying and processing the acoustic sound of a per-
former's muscle contractions. The model relies on an origi-
nal and open source technology made of custom biosensors
and a related software framework. The succesfull applica-
tion of these tools is discussed in the practical context of a
solo piece for sensors, laptop and loudspeakers. Eventually,
the compositional strategies that characterize the piece are
discussed along with a systematic description of the relevant
mapping techniques and their sonic outcome.

</abstract>
    <keywords> Muscle sounds, biophysical music, augmented body, real- time performance, human-computer interaction, embodi- ment.  </keywords>
  </document>
  <document>
    <name>nime2012_136.pdf</name>
    <abstract> 
 

In this paper, we argue that the design of New Interfaces for 
Musical Expression has much to gain from the study of 
interaction in ensemble laptop performance contexts using 
ethnographic techniques. Inspired by recent third-stream 
research in the field of human computer interaction, we 
describe a recent ethnomethodologically-informed study of the 
Birmingham Laptop Ensemble (BiLE), and detail our approach 
to thick description of the group's working practices. Initial 
formal analysis of this material sheds light on the fluidity of 
composer, performer and designer roles within the ensemble 
and shows how confluences of these roles constitute member's 
differing viewpoints. We go on to draw out a number of strands 
of interaction that highlight the essentially complex, socially 
constructed and value driven nature of the group's practice and 
conclude by reviewing the implications of these factors on the 
design of software tools for laptop ensembles. 
</abstract>
    <keywords>  Laptop Performance, Ethnography, Ethnomethodology, Human  Computer Interaction.   </keywords>
  </document>
  <document>
    <name>nime2012_142.pdf</name>
    <keywords>  Network music, mobile music, distributed music, interactivity,  sound art installation, collaborative instrument, site-specific,  electromagnetic signals, WiFi, trilateration, traceroute,  echolocation, SuperCollider, Pure Data, RjDj, mapping   </keywords>
  </document>
  <document>
    <name>nime2012_150.pdf</name>
    <abstract>
The configurability and networking abilities of digital musi-
cal instruments increases the possibilities for collaboration
in musical performances. Computer music ensembles such
as laptop orchestras are becoming increasingly common and
provide laboratories for the exploration of these possibil-
ities. However, much of the literature regarding the cre-
ation of DMIs has been focused on individual expressivity,
and their potential for collaborative performance has been
under-utilized. This paper makes the case for the benefits
of an approach to digital musical instrument design that
begins with their collaborative potential, examines several
frameworks and sets of principles for the creation of digital
musical instruments, and proposes a dimension space repre-
sentation of collaborative approaches which can be used to
evaluate and guide future DMI creation. Several examples
of DMIs and compositions are then evaluated and discussed
in the context of this dimension space.

</abstract>
    <keywords> dimension space, collaborative, digital musical instrument, dmi, digital music ensemble, dme  </keywords>
  </document>
  <document>
    <name>nime2012_152.pdf</name>
    <abstract> 
Borderlands is a new interface for composing and performing 
with granular synthesis. The software enables flexible, real-
time improvisation and is designed to allow users to engage 
with sonic material on a fundamental level, breaking free of 
traditional paradigms for interaction with this technique. The 
user is envisioned as an organizer of sound, simultaneously 
assuming the roles of curator, performer, and listener. This 
paper places the software within the context of painterly 
interfaces and describes the user interaction design and 
synthesis methodology.  
 
</abstract>
  </document>
  <document>
    <name>nime2012_153.pdf</name>
    <abstract>
The Physical Computing Ensemble was created in order to
determine the viability of an approach to musical perfor-
mance which focuses on the relationships and interactions
of the performers. Three performance systems utilizing ges-
tural controllers were designed and implemented, each with
a different strategy for performer interaction.

These strategies took advantage of the opportunities for
collaborative performance inherent in digital musical instru-
ments due to their networking abilities and reconfigurable
software. These characteristics allow for the easy implemen-
tation of varying approaches to collaborative performance.
Ensembles who utilize digital musical instruments provide
a fertile environment for the design, testing, and utilization
of collaborative performance systems.

The three strategies discussed in this paper are the pa-
rameterization of musical elements, turn-based collabora-
tive control of sound, and the interaction of musical sys-
tems created by multiple performers. Design principles,
implementation, and a performance using these strategies
are discussed, and the conclusion is drawn that performer
interaction and collaboration as a primary focus for system
design, composition, and performance is viable.

</abstract>
    <keywords> Collaborative performance, interaction, digital musical in- struments, gestural controller, digital music ensemble, Wii  </keywords>
  </document>
  <document>
    <name>nime2012_155.pdf</name>
    <abstract>
This paper presents a comparison of three-dimensional (3D)
position tracking systems in terms of some of their perfor-
mance parameters such as static accuracy and precision,
update rate, and shape of the space they sense. The under-
lying concepts and characteristics of position tracking tech-
nologies are reviewed, and four position tracking systems
(Vicon, Polhemus, Kinect, and Gametrak), based on dif-
ferent technologies, are empirically compared according to
their performance parameters and technical specifications.
Our results show that, overall, the Vicon was the position
tracker with the best performance.

</abstract>
    <keywords> Position tracker, comparison, touch-less, gestural control  </keywords>
  </document>
  <document>
    <name>nime2012_159.pdf</name>
    <abstract>
A new method for interpolating between presets is described.
The interpolation algorithm called Intersecting N-Spheres
Interpolation is simple to compute and its generalization to
higher dimensions is straightforward. The current imple-
mentation in the SuperCollider environment is presented
as a tool that eases the design of many-to-many mappings
for musical interfaces. Examples of its uses, including such
mappings in conjunction with a musical interface called the
sponge, are given and discussed.

</abstract>
    <keywords> Mapping, Preset, Interpolation, Sponge, SuperCollider  </keywords>
  </document>
  <document>
    <name>nime2012_161.pdf</name>
    <keywords> music balls, instruments, controllers, inexpensive  </keywords>
  </document>
  <document>
    <name>nime2012_162.pdf</name>
  </document>
  <document>
    <name>nime2012_164.pdf</name>
    <abstract>
In this paper, we describe our pioneering work in developing
speech synthesis beyond the Text-To-Speech paradigm. We
introduce tangible speech synthesis as an alternate way of
envisioning how artificial speech content can be produced.
Tangible speech synthesis refers to the ability, for a given
system, to provide some physicality and interactivity to im-
portant speech production parameters. We present MAGE,
our new software platform for high-quality reactive speech
synthesis, based on statistical parametric modeling and more
particularly hidden Markov models. We also introduce a
new HandSketch-based musical instrument. This instru-
ment brings pen and posture based interaction on the top
of MAGE, and demonstrates a first proof of concept.

</abstract>
  </document>
  <document>
    <name>nime2012_167.pdf</name>
    <abstract>  
This paper describes the development of the Emotion Light, an  
interactive biofeedback artwork where the user listens to a  
piece of electronic music whilst holding a semi-transparent  
sculpture that tracks his/her bodily responses and translates 
these into changing light patterns that emerge from the 
sculpture. The context of this work is briefly described and the  
questions it poses are derived from interviews held with  
audience members.  
</abstract>
    <keywords>   Interactive biofeedback artwork, music and emotion, novel  interfaces, practice based research, bodily response, heart rate,   biosignals, affective computing, aesthetic interaction, mediating   body, biology inspired system       </keywords>
  </document>
  <document>
    <name>nime2012_168.pdf</name>
    <abstract> 
As a part of the research project Voice Meetings, a solo live-
electronic vocal performance was presented for 63 students. 
Through a mixed method approach applying both written and 
oral response, feedback from one blindfolded and one seeing 
audience group was collected and analyzed.  
 There were marked differences between the groups regarding 
focus, in that the participants in blindfolded group tended to 
focus on fewer aspects, have a heightened focus and be less 
distracted than the seeing group. The seeing group, on its part, 
focused more on the technological instruments applied in the 
performance, the performer herself and her actions. This study 
also shows that there were only minor differences between the 
groups regarding the experience of skill and control, and argues 
that this observation can be explained by earlier research on 
skill in NIMEs. 
 
</abstract>
    <keywords>  Performance, audience reception, acousmatic listening, live- electronics, voice, qualitative research   </keywords>
  </document>
  <document>
    <name>nime2012_169.pdf</name>
    <abstract>
Through a series of collaborative research projects using
Orient, a wireless, inertial sensor-based motion capture system,
I have studied the requirements of musicians, dancers,
performers and choreographers and identified various design
strategies for the realization of Whole Body Interactive (WBI)
performance systems. The acquired experience and knowledge
led to the design and development of EnActor, prototype
Whole Body Interaction Design software. The software has
been realized as a collection of modules that were proved
valuable for the design of interactive performance systems that
are directly controlled by the body.

This paper presents EnActor's layout as a blueprint for the
design and development of more sophisticated descendants.
Complete video archive of my research projects in WBI
performance systems at: http://www.inter-axions.com

</abstract>
    <keywords> Whole Body Interaction, Motion Capture, Interactive Performance Systems, Interaction Design, Software Prototype,  </keywords>
  </document>
  <document>
    <name>nime2012_170.pdf</name>
    <abstract> 
In this paper we introduce an interactive mobile music 

performance system using the digital compass of mobile 

phones. Compass-based interface can detect the aiming 

orientation of performers on stage, allowing us to obtain 

information on interactions between performers and use it for 

both musical mappings and visualizations on screen for the 

audience. We document and discuss the result of a compass-

based mobile music performance, Where Are You Standing, and 

present an algorithm for a new app to track down the 

performers' positions in real-time.  

 

</abstract>
  </document>
  <document>
    <name>nime2012_171.pdf</name>
    <keywords> Many person musical instruments, cooperative music, asym- metric interfaces, transmodal feedback  </keywords>
  </document>
  <document>
    <name>nime2012_174.pdf</name>
    <keywords>  Empirical methods, quantitative, usability testing and evaluation,   digital musical instruments, evaluation methodology, Illusio   </keywords>
  </document>
  <document>
    <name>nime2012_175.pdf</name>
  </document>
  <document>
    <name>nime2012_177.pdf</name>
    <keywords>  Skin-based instruments, skin conductivity, collaborative  interfaces, embodiment, intimacy, multi-player performance   </keywords>
  </document>
  <document>
    <name>nime2012_179.pdf</name>
    <abstract> 
Empatheater is a video playing system that is controlled by 

multimodal interaction.  As the video is played, the user must 

interact and emulate predefined "events" for the video to 
continue on.  The user is given the illusion of playing an active 

role in the unraveling video content and can empathize with the 

performer.  In this paper, we report about user experiences with 

Empatheater when applied to musical video contents.  

</abstract>
    <keywords>  Music video, Empathy, Interactive video, Musical event,   Multimodal interaction.   </keywords>
  </document>
  <document>
    <name>nime2012_180.pdf</name>
    <abstract> 
The augmented ballet project aims at gathering research from 
several fields and directing them towards a same application 
case: adding virtual elements (visual and acoustic) to a dance 
live performance, and allowing the dancer to interact with 
them. In this paper, we describe a novel interaction that we 
used in the frame of this project: using the dancer's movements 
to recognize the emotions he expresses, and use these emotions 
to generate musical audio flows evolving in real-time. The 
originality of this interaction is threefold. First, it covers the 
whole interaction cycle from the input (the dancer's 
movements) to the output (the generated music). Second, this 
interaction isn't direct but goes through a high level of 
abstraction: dancer's emotional expression is recognized and is 
the source of music generation. Third, this interaction has been 
designed and validated through constant collaboration with a 
choreographer, culminating in an augmented ballet 
performance in front of a live audience.  
</abstract>
    <keywords>   Interactive sonification, motion, gesture and music, interaction,  live performance, musical human-computer interaction   </keywords>
  </document>
  <document>
    <name>nime2012_181.pdf</name>
    <abstract>
In this paper we present our project to make sound synthesis
and music controller construction accessible to children in
a technology design workshop. We present the work we
have carried out to develop a graphical user interface, and
give account of the workshop we conducted in collaboration
with a local primary school. Our results indicate that the
production of audio events by means of digital synthesis
and algorithmic composition provides a rich and interesting
field to be discovered for pedagogical workshops taking a
Constructionist approach.

</abstract>
    <keywords> Child Computer Interaction, Constructionism, Sound and Music Computing, Human-Computer Interface Design, Mu- sic Composition and Generation, Interactive Audio Sys- tems, Technology Design Activities.  </keywords>
  </document>
  <document>
    <name>nime2012_185.pdf</name>
    <abstract>
Meaning crossword of sound, Crossole is a musical meta-
instrument where the music is visualized as a set of virtual
blocks that resemble a crossword puzzle. In Crossole, the
chord progressions are visually presented as a set of virtual
blocks. With the aid of the Kinect sensing technology, a per-
former controls music by manipulating the crossword blocks
using hand movements. The performer can build chords in
the high level, traverse over the blocks, step into the low
level to control the chord arpeggiations note by note, loop
a chord progression or map gestures to various processing
algorithms to enhance the timbral scenery.

</abstract>
    <keywords> Kinect, meta-instrument, chord progression, body gesture  </keywords>
  </document>
  <document>
    <name>nime2012_187.pdf</name>
    <abstract> 
This paper presents the JD-1, a digital controller for analog 
modular synthesizers. The JD-1 features a capacitive touch-
sensing keyboard that responds to continuous variations in 
finger contact, high-accuracy polyphonic control-voltage 
outputs, a built-in sequencer, and digital interfaces for 
connection to MIDI and OSC devices. Design goals include 
interoperability with a wide range of synthesizers, very high-
resolution pitch control, and intuitive control of the sequencer 
from the keyboard. 
 
</abstract>
  </document>
  <document>
    <name>nime2012_189.pdf</name>
    <abstract> 
Development of new musical interfaces often requires 
experimentation with the mapping of available controller inputs 
to output parameters. Useful mappings for a particular 
application may be complex in nature, with one or more inputs 
being linked to one or more outputs. Existing development 
environments are commonly used to program such mappings, 
while code libraries provide powerful data-stream 
manipulation. However, room exists for a standalone 
application with a simpler graphical user interface for 
dynamically patching between inputs and outputs. This paper 
presents an early prototype version of a software tool that 
allows the user to route control signals in real time, using 
various messaging formats. It is cross-platform and runs as a 
standalone application in desktop and Android OS versions. 
The latter allows the users of mobile devices to experiment 
with mapping signals to and from physical computing 
components using the inbuilt multi-touch screen. Potential uses 
therefore include real-time mapping during performance in a 
more expressive manner than facilitated by existing tools.  
 
</abstract>
    <keywords>  Mapping, Software Tools, Android.   </keywords>
  </document>
  <document>
    <name>nime2012_193.pdf</name>
    <abstract>
An augmented bass clarinet is developed in order to extend
the performance and composition potential of the instru-
ment. Four groups of sensors are added: key positions, in-
ertial movement, mouth pressure and trigger switches. The
instrument communicates wirelessly with a receiver setup
which produces an OSC data stream, usable by any appli-
cation on a host computer.

The SABRe projects intention is to be neither tied to its
inventors nor to one single player but to offer a reference
design for a larger community of bass clarinet players and
composers. For this purpose, several instruments are made
available and a number of composer residencies, workshops,
presentations and concerts are organized. These serve for
evaluation and improvement purposes in order to build a
robust and user friendly extended musical instrument, that
opens new playing modalities.

</abstract>
    <keywords> augmented instrument, bass clarinet, sensors, air pressure, gesture, OSC  </keywords>
  </document>
  <document>
    <name>nime2012_194.pdf</name>
    <keywords>  Musical Interaction Design, NIME education, Microcontroller,  Arduino language, StickOS BASIC, Open Sound Control,  Microchip PIC32, Wireless, Zigflea, Wifi, 802.11g, Bluetooth,  CUI32, CUI32Stem     </keywords>
  </document>
  <document>
    <name>nime2012_195.pdf</name>
    <abstract>
Capacitive touch sensing is increasingly used in musical con-
trollers, particularly those based on multi-touch screen in-
terfaces. However, in contrast to the venerable piano-style
keyboard, touch screen controllers lack the tactile feedback
many performers find crucial. This paper presents an aug-
mentation system for acoustic and electronic keyboards in
which multi-touch capacitive sensors are added to the sur-
face of each key. Each key records the position of fingers
on the surface, and by combining this data with MIDI note
onsets and aftertouch from the host keyboard, the system
functions as a multidimensional polyphonic controller for a
wide variety of synthesis software. The paper will discuss
general capacitive touch sensor design, keyboard-specific
implementation strategies, and the development of a flexible
mapping engine using OSC and MIDI.

</abstract>
    <keywords> augmented instruments, keyboard, capacitive sensing, multi- touch  </keywords>
  </document>
  <document>
    <name>nime2012_197.pdf</name>
    <abstract> 
This paper addresses the issue of engaging the audience with 
new musical instruments in live performance context. We 
introduce design concerns that we consider influential to 
enhance the communication flow between the audience and the 
performer. We also propose and put in practice a design 
approach that considers the use of performance space as a way 
to engage with the audience. A collaborative project, Sound 
Gloves, presented here exemplifies such a concept by 
dissolving the space between performers and audience. Our 
approach resulted in a continuous interaction between audience 
and performers, in which the social dynamics was changed in a 
positive way in a live performance context of NIMEs. Such an 
approach, we argue, may be considered as one way to further 
engage and interact with the audience. 
 
</abstract>
    <keywords>  NIME, wearable electronics, performance, design approach   </keywords>
  </document>
  <document>
    <name>nime2012_199.pdf</name>
    <abstract>
This paper describes an interactive gestural microphone for
vocal performance named Voicon. Voicon is a non-invasive
and gesture-sensitive microphone which allows vocal per-
formers to use natural gestures to create vocal augmenta-
tions and modifications by using embedded sensors in a mi-
crophone. Through vocal augmentation and modulation,
the performers can easily generate desired amount of the
vibrato and achieve wider vocal range. These vocal en-
hancements will deliberately enrich the vocal performance
both in its expressiveness and the dynamics. Using Voicon,
singers can generate additional vibrato, control the pitch
and activate customizable vocal effect by simple and intu-
itive gestures in live and recording context.

</abstract>
    <keywords> Gesture, Microphone, Vocal Performance, Performance In- terface  </keywords>
  </document>
  <document>
    <name>nime2012_200.pdf</name>
  </document>
  <document>
    <name>nime2012_201.pdf</name>
    <abstract> 
This paper describes a novel music control sensate surface, 
which enables integration between any musical instruments 
with a v ersatile, customizable, and essentially cost-effective 
user interface. This sensate surface is based on c onductive 
inkjet printing technology which allows capacitive sensor 
electrodes and connections between electronics components to 
be printed onto a large roll of flexible substrate that is 
unrestricted in length. The high dynamic range capacitive 
sensing electrodes can not only infer touch, but near-range, 
non-contact gestural nuance in a music performance. With this 
sensate surface, users can "cut" out their desired shapes, 
"paste" the number of inputs, and customize their controller 
interface, which can then send signals wirelessly to effects or 
software synthesizers. We seek to find a solution for integrating 
the form factor of traditional music controllers seamlessly on 
top of one's music instrument and meanwhile adding 
expressiveness to the music performance by sensing and 
incorporating movements and gestures to manipulate the 
musical output.  We present an example of implementation on 
an electric ukulele and provide several design examples to 
demonstrate the versatile capabilities of this system.    

 
</abstract>
    <keywords>  Sensate surface, music controller skin, customizable controller  surface, flexible electronics    </keywords>
  </document>
  <document>
    <name>nime2012_202.pdf</name>
    <abstract>
We have developed a prototype wireless microphone that
provides vocalists with control over their vocal effects di-
rectly from the body of the microphone. A wireless micro-
phone has been augmented with six momentary switches,
one fader, and three axes of motion and position sensors,
all of which provide MIDI output from the wireless receiver.
The MIDI data is used to control external vocal effects units
such as live loopers, reverbs, distortion pedals, etc. The goal
was to to provide dramatically increased expressive control
to vocal performances, and address some of the shortcom-
ings of pedal-controlled effects. The addition of gestural
controls from the motion sensors opens up new performance
possibilities such as panning the voice simply by pointing
the microphone in one direction or another. The result is a
hybrid microphone-musical instrument which has recieved
extremely positive results from vocalists in numerous infor-
mal workshops.

</abstract>
    <keywords> NIME, Sennheiser, Concept Tahoe, MIDI, control, micro- phone  </keywords>
  </document>
  <document>
    <name>nime2012_203.pdf</name>
    <abstract>
We augment the piano keyboard with a 3D gesture space
using Microsoft Kinect for sensing and top-down projec-
tion for visual feedback. This interface provides multi-axial
gesture controls to enable continuous adjustments to multi-
ple acoustic parameters such as those on the typical digital
synthesizers. We believe that using gesture control is more
visceral and aesthetically pleasing, especially during concert
performance where the visibility of the performer's action is
important. Our system can also be used for other types of
gesture interaction as well as for pedagogical applications.

</abstract>
    <keywords> NIME, piano, depth camera, musical instrument, gesture, tabletop projection  </keywords>
  </document>
  <document>
    <name>nime2012_205.pdf</name>
    <abstract>
We present a new wireless transceiver board for the CUI32
sensor interface, aimed at creating a solution that is flexible,
reliable, and with little power consumption. Communica-
tion with the board is based on the ZigFlea protocol and
it has been evaluated on a CUI32 using the StickOS oper-
ating system. Experiments show that the total sensor data
collection time is linearly increasing with the number of sen-
sor samples used. A data rate of 0.8 kbit/s is achieved for
wirelessly transmitting three axes of a 3D accelerometer.
Although this data rate is low compared to other systems,
our solution benefits from ease-of-use and stability, and is
useful for applications that are not time-critical.

</abstract>
    <keywords> wireless sensing, CUI32, StickOS, ZigBee, ZigFlea  </keywords>
  </document>
  <document>
    <name>nime2012_208.pdf</name>
    <abstract> 
"Perfect Take" is a public installation out of networked acoustic 
instruments that let composers from all over the world exhibit 
their MIDI-works by means of the Internet. The primary aim of 
this system is to offer composers a way to have works exhibited 
and recorded in venues and with technologies not accessible to 
him/her under normal circumstances. The Secondary aim of 
this research is to highlight experience design as a complement 
to interaction design, and a shift of focus from functionality of 
a specific gestural controller, towards the environments, events 
and processes that they are part of. 
 
</abstract>
    <keywords>  NIME, Networked Music, MIDI, Disklavier, music  collaboration, creativity   </keywords>
  </document>
  <document>
    <name>nime2012_209.pdf</name>
    <abstract> 
 
FutureGrab is a new wearable musical instrument for live 
performance that is highly intuitive while still generating an 
interesting sound by subtractive synthesis. Its sound effects 
resemble the human vowel pronunciation, which were mapped 
to hand gestures that are similar to the mouth shape of human 
to pronounce corresponding vowel. FutureGrab also provides 
all necessary features for a lead musical instrument such as 
pitch control, trigger, glissando and key adjustment. In addition, 
pitch indicator was added to give visual feedback to the 
performer, which can reduce the mistakes during live 
performances. This paper describes the motivation, system 
design, mapping strategy and implementation of FutureGrab, 
and evaluates the overall experience. 
 
</abstract>
    <keywords>  Wearable musical instrument, Pure Data, gestural synthesis,  formant synthesis, data-glove, visual feedback, subtractive  synthesis     </keywords>
  </document>
  <document>
    <name>nime2012_211.pdf</name>
    <abstract>
In an attempt to utilize the expert pianist's technique and spare 
bandwidth, a new keyboard-based instrument augmented by 
sensors suggested by the examination of existing acoustic 
instruments is introduced. The complete instrument includes a 
keyboard, various pedals and knee levers, several bowing 
controllers, and breath and embouchure sensors connected to an 
Arduino microcontroller that sends sensor data to a laptop 
running Max/MSP, where custom software maps the data to 
synthesis algorithms. The audio is output to a digital amplifier 
powering a transducer mounted on a resonator box to which 
several of the sensors are attached. Careful sensor selection and 
mapping help to facilitate performance mode. 

</abstract>
    <keywords> Gesture, controllers, Digital Musical Instrument, keyboard  </keywords>
  </document>
  <document>
    <name>nime2012_212.pdf</name>
    <abstract> 
Dirty Tangible Interfaces (DIRTI) are a new concept in 
interface design that forgoes the dogma of repeatability in favor 
of a richer and more complex experience, constantly evolving, 
never reversible, and infinitely modifiable. We built a prototype 
based on granular or liquid interaction material placed in a 
glass dish, that is analyzed by video tracking for its 3D relief.  
This relief, and the dynamic changes applied to it by the user, 
are interpreted as activation profiles to drive corpus-based 
concatenative sound synthesis, allowing one or more players to 
mold sonic landscapes and to plow through them in an 
inherently collaborative, expressive, and dynamic experience. 
   
</abstract>
    <keywords>  Tangible interface, Corpus-based concatenative synthesis, Non- standard interaction   </keywords>
  </document>
  <document>
    <name>nime2012_213.pdf</name>
    <abstract>
In this paper we present a series of algorithms developed
to detect the following guitar playing techniques : bend,
hammer-on, pull-off, slide, palm muting and harmonic. De-
tection of playing techniques can be used to control exter-
nal content (i.e audio loops and effects, videos, light events,
etc.), as well as to write real-time score or to assist gui-
tar novices in their learning process. The guitar used is a
Godin Multiac with an under-saddle RMC hexaphonic piezo
pickup (one pickup per string, i.e six mono signals).

</abstract>
    <keywords> Guitar audio analysis, playing techniques, hexaphonic pickup, controller, augmented guitar  </keywords>
  </document>
  <document>
    <name>nime2012_214.pdf</name>
    <abstract>
The Deckle Group1 is an ensemble that designs, builds and
performs on electroacoustic drawing boards. These draw-
ing surfaces are augmented with Satellite CCRMA Beagle-
Boards and Arduinos2.[1] Piezo microphones are used in
conjunction with other sensors to produce sounds that are
coupled tightly to mark-making gestures. Position tracking
is achieved with infra-red object tracking, conductive fabric
and a magnetometer.

</abstract>
    <keywords> Deckle, BeagleBoard, Drawing, Sonification, Performance, Audiovisual, Gestural Interface  </keywords>
  </document>
  <document>
    <name>nime2012_215.pdf</name>
    <abstract>
In this paper we describe the EyeHarp, a new gaze-controlled
musical instrument, and the new features we recently added
to its design. In particular, we report on the EyeHarp new
controls, the arpeggiator, the new remote eye-tracking de-
vice, and the EyeHarp capacity to act as a MIDI controller
for any VST plugin virtual instrument. We conducted an
evaluation of the EyeHarp Temporal accuracy by monitor-
ing 10 users while performing a melody task, and comparing
their gaze control accuracy with their accuracy using a com-
puter keyboard. We report on the results of the evaluation.

</abstract>
    <keywords> Eye-tracking systems, music interfaces, gaze interaction  </keywords>
  </document>
  <document>
    <name>nime2012_216.pdf</name>
    <abstract> 

Virtual Pottery is an interactive audiovisual piece that uses 
hand gesture to create 3D pottery objects and sound shape. 
Using the OptiTrack motion capture (Rigid Body) system at  
TransLab in UCSB, performers can take a glove with attached 
trackers, move the hand in x, y, and z axis and create their own 
sound pieces. Performers can also manipulate their pottery 
pieces in real time and change arrangement on the musical 
score interface in order to create a continuous musical 
composition. In this paper we address the relationship between 
body, sound and 3D shapes. We also describe the origin of 
Virtual Pottery, its design process, discuss its aesthetic value 
and musical sound synthesis system, and evaluate the overall 
experience. 
 
</abstract>
    <keywords>  Virtual Pottery, virtual musical instrument, sound synthesis,  motion and gesture, pottery, motion perception, interactive  sound installation.   </keywords>
  </document>
  <document>
    <name>nime2012_217.pdf</name>
    <abstract> 
This paper presents concepts, models, and empirical findings 

relating to liveness and flow in the user experience of systems 

mediated by notation. Results from an extensive two-year field 

study of over 1,000 sequencer and tracker users, combining 

interaction logging, user surveys, and a video study, are used  

to illustrate the properties of notations and interfaces that 

facilitate greater immersion in musical activities and domains, 

borrowing concepts from programming to illustrate the role  

of visual and musical feedback, from the notation and  

domain respectively. The Cognitive Dimensions of Notations 

framework and Csikszentmihalyi's flow theory are combined to 

demonstrate how non-realtime, notation-mediated interaction 

can support focused, immersive, energetic, and intrinsically-

rewarding musical experiences, and to what extent they are 

supported in the interfaces of music production software. Users 

are shown to maintain liveness through a rapid, iterative edit-

audition cycle that integrates audio and visual feedback. 
 

</abstract>
    <keywords>  notation, composition, liveness, flow, feedback, sequencers,   DAWs, soundtracking, performance, user studies, programming     </keywords>
  </document>
  <document>
    <name>nime2012_222.pdf</name>
    <abstract>
The Gyil is a pentatonic African wooden xylophone with
14-15 keys. The work described in this paper has been
motivated by three applications: computer analysis of Gyil
performance, live improvised electro-acoustic music incor-
porating the Gyil, and hybrid sampling and physical mod-
eling. In all three of these cases, detailed information about
what is played on the Gyil needs to be digitally captured
in real-time. We describe a direct sensing apparatus that
can be used to achieve this. It is based on contact micro-
phones and is informed by the specific characteristics of the
Gyil. An alternative approach based on indirect acquisition
is to apply polyphonic transcription on the signal acquired
by a microphone without requiring the instrument to be
modified. The direct sensing apparatus we have developed
can be used to acquire ground truth for evaluating different
approaches to polyphonic transcription and help create a
"surrogate" sensor. Some initial results comparing different
strategies to polyphonic transcription are presented.

</abstract>
    <keywords> hyperinstruments, indirect acquisition, surrogate sensors, computational ethnomusicology, physical modeling, perfor- mance analysis  </keywords>
  </document>
  <document>
    <name>nime2012_223.pdf</name>
    <abstract>
The Instant Instrument Anywhere (IIA) is a small device
which can be attached to any metal object to create an
electronic instrument. The device uses capacitive sensing
to detect proximity of the player's body to the metal ob-
ject, and sound is generated through a surface transducer
which can be attached to any flat surface. Because the ca-
pacitive sensor can be any shape or size, absolute capacitive
thresholding is not possible since the baseline capacitance
will change. Instead, we use a differential-based moving
sum threshold which can rapidly adjust to changes in the
environment or be re-calibrated to a new metal object. We
show that this dynamic threshold is effective in rejecting
environmental noise and rapidly adapting to new objects.
We also present details for constructing Instant Instruments
Anywhere, including using smartphone as the synthesis en-
gine and power supply.

</abstract>
    <keywords> Capacitive Sensing, Arduino  </keywords>
  </document>
  <document>
    <name>nime2012_226.pdf</name>
  </document>
  <document>
    <name>nime2012_228.pdf</name>
    <abstract>
A modular and reconfigurable hardware platform for analog
optoelectronic signal acquisition is presented. Its intended
application is for fiber optic sensing in electronic musical
interfaces, however the flexible design enables its use with
a wide range of analog and digital sensors. Multiple gain
and multiplexing stages as well as programmable analog and
digital hardware blocks allow for the acquisition, processing,
and communication of single-ended and differential signals.
Along with a hub board, multiple acquisition boards can
be connected to modularly extend the system's capabilities
to suit the needs of the application. Fiber optic sensors
and their application in DMIs are briefly discussed, as well
as the use of the hardware platform with specific musical
interfaces.

</abstract>
  </document>
  <document>
    <name>nime2012_229.pdf</name>
    <abstract> 
Music for Sleeping &amp; Waking Minds (2011-2012) is a new, 
overnight work in which four performers fall asleep while 
wearing custom designed EEG sensors which monitor their 
brainwave activity. The data gathered from the EEG sensors is 
applied in real time to different audio and image signal 
processing functions, resulting in continuously evolving multi-
channel sound environment and visual projection. This material 
serves as an audiovisual description of the individual and 
collective neurophysiological state of the ensemble. Audiences 
are invited to experience the work in different states of 
attention: while alert and asleep, resting and awakening.  
 
</abstract>
    <keywords>  EEG, sleep, dream, biosignals, bio art, consciousness, BCI    </keywords>
  </document>
  <document>
    <name>nime2012_230.pdf</name>
    <abstract>
This paper describes the design and realization of TC-11,
a software instrument based on programmable multi-point
controllers. TC-11 is a modular synthesizer for the iPad
that uses multi-touch and device motion sensors for control.
It has a robust patch programming interface that centers
around multi-point controllers, providing powerful flexibil-
ity. This paper details the origin, design principles, pro-
gramming implementation, and performance result of TC-
11.

</abstract>
    <keywords> TC-11, iPad, multi-touch, multi-point, controller mapping, synthesis programming  </keywords>
  </document>
  <document>
    <name>nime2012_232.pdf</name>
    <abstract>
We developed original solenoid actuator units with several
built-in sensors, and produced a box-shaped musical inter-
face "PocoPoco" using 16 units of them as a universal in-
put/output device. We applied up-and-down movement of
the solenoid-units and user's intuitive input to musical in-
terface. Using transformation of the physical interface, we
can apply movement of the units to new interaction design.
At the same time we intend to suggest a new interface whose
movement itself can attract the user.

</abstract>
    <keywords> musical interface, interaction design, tactile, moving, kinetic  </keywords>
  </document>
  <document>
    <name>nime2012_235.pdf</name>
    <abstract>
In this paper we discuss aspects of our work in develop-
ing performance systems that are geared towards human-
machine co-performance with a particular emphasis on im-
provisation. We present one particular system, FILTER,
which was created in the context of a larger project re-
lated to artificial intelligence and performance, and has been
tested in the context of our electro-acoustic performance
trio. We discuss how this timbrally rich and highly non-
idiomatic musical context has challenged the design of the
system, with particular emphasis on the mapping of ma-
chine listening parameters to higher-level behaviors of the
system in such a way that spontaneity and creativity are
encouraged while maintaining a sense of novel dialogue.

</abstract>
    <keywords> Electroacoustic Improvisation, Machine Learning, Mapping, Sonic Gestures, Spatialization  </keywords>
  </document>
  <document>
    <name>nime2012_237.pdf</name>
    <abstract>
The purpose of the Musician Assistance and Score Distri-
bution (MASD) system is to assist novice musicians with
playing in an orchestra, concert band, choir or other musical
ensemble. MASD helps novice musicians in three ways. It
removes the confusion that results from page turns, aides a
musician's return to the proper location in the music score
after the looking at the conductor and notifies musicians
of conductor instructions. MASD is currently verified by
evaluating the time between sending beats or conductor in-
formation and this information being rendered for the mu-
sician. Future work includes user testing of this system.

There are three major components to the MASD system.
These components are Score Distribution, Score Render-
ing and Information Distribution. Score Distribution passes
score information to clients and is facilitated by the Inter-
net Communication Engine (ICE). Score Rendering uses the
GUIDO Library to display the musical score. Information
Distribution uses ICE and the IceStorm service to pass beat
and instruction information to musicians.

</abstract>
    <keywords> score distribution, score-following, score rendering, musi- cian assistance  </keywords>
  </document>
  <document>
    <name>nime2012_240.pdf</name>
    <abstract> 
Mobile devices represent a growing research field within 
NIME, and a growing area for commercial music software. 
They present unique design challenges and opportunities, 
which are yet to be fully explored and exploited. In this paper, 
we propose using a survey method combined with qualitative 
analysis to investigate the way in which people use mobiles 
musically. We subsequently present as an area of future 
research our own PDplayer, which provides a completely self 
contained end application in the mobile device, potentially 
making the mobile a more viable and expressive tool for 
musicians.  
 
</abstract>
    <keywords>  NIME, Mobile Music, Pure Data   </keywords>
  </document>
  <document>
    <name>nime2012_241.pdf</name>
    <abstract>
This paper presents a system for mobile percussive collabo-
ration. We show that reinforcement learning can incremen-
tally learn percussive beat patterns played by humans and
supports real-time collaborative performance in the absence
of one or more performers. This work leverages an existing
integration between urMus and Soar and addresses multiple
challenges involved in the deployment of machine-learning
algorithms for mobile music expression, including tradeoffs
between learning speed &amp; quality; interface design for hu-
man collaborators; and real-time performance and improvi-
sation.

</abstract>
  </document>
  <document>
    <name>nime2012_243.pdf</name>
    <abstract> 
Force-feedback and physical modeling technologies now allow 
to achieve the same kind of relation with virtual instruments as 
with acoustic instruments, but the design of such elaborate 
models needs guidelines based on the study of the human 
sensory-motor system and behaviour. This article presents a 
qualitative study of a simulated instrumental interaction in the 
case of the virtual bowed string, using both waveguide and 
mass-interaction models. Subjects were invited to explore the 
possibilities of the simulations and to express themselves 
verbally at the same time, allowing us to identify key qualities 
of the proposed systems that determine the construction of an 
intimate and rich relationship with the users. 
</abstract>
    <keywords>  Instrumental interaction, presence, force-feedback, physical  modeling, simulation, haptics, bowed string.   </keywords>
  </document>
  <document>
    <name>nime2012_247.pdf</name>
  </document>
  <document>
    <name>nime2012_248.pdf</name>
    <abstract>
This paper presents Digito, a gesturally controlled virtual
musical instrument. Digito is controlled through a num-
ber of intricate hand gestures, providing both discrete and
continuous control of Digito's sound engine; with the fine-
grain hand gestures captured by a 3D depth sensor and
recognized using computer vision and machine learning al-
gorithms. We describe the design and initial iterative devel-
opment of Digito, the hand and finger tracking algorithms
and gesture recognition algorithms that drive the system,
and report the insights gained during the initial develop-
ment cycles and user testing of this gesturally controlled
virtual musical instrument.

</abstract>
    <keywords> Gesture Recognition, Virtual Musical Instrument  </keywords>
  </document>
  <document>
    <name>nime2012_253.pdf</name>
    <abstract>
A study is presented examining the participatory design
of digital musical interactions. The study takes into con-
sideration the entire ecology of digital musical interactions
including the designer, performer and spectator. A new
instrument is developed through iterative participatory de-
sign involving a group of performers. Across the study the
evolution of creative practice and skill development in an
emerging community of practice is examined and a specta-
tor study addresses the cognition of performance and the
perception of skill with the instrument. Observations are
presented regarding the cognition of a novel interaction and
evolving notions of skill. The design process of digital mu-
sical interactions is reflected on focusing on involvement of
the spectator in design contexts.

</abstract>
    <keywords> participatory design, DMIs, skill, cognition, spectator  </keywords>
  </document>
  <document>
    <name>nime2012_254.pdf</name>
    <abstract> 
In order to further understand our emotional reaction to music, 

a museum-based installation was designed to collect 

physiological and self-report data from people listening to 

music. This demo will describe the technical implementation of 

this installation as a tool for collecting large samples of data in 

public spaces. The Emotion in Motion terminal is built upon a 

standard desktop computer running Max/MSP and using 

sensors that measure physiological indicators of emotion that 

are connected to an Arduino. The terminal has been installed in 

museums and galleries in Europe and the USA, helping create 

the largest database of physiology and self-report data while 

listening to music. 

</abstract>
    <keywords>  Biosignals, EDA, SC, GSR, HR, POX, Self-Report, Database,   Physiological Signals, Max/MSP, FTM, SAM, GEMS   </keywords>
  </document>
  <document>
    <name>nime2012_256.pdf</name>
    <abstract>
From a technical point of view, instrumental music mak-
ing involves audible, visible and hidden playing parameters.
Hidden parameters like force, pressure and fast movements,
happening within milliseconds are particularly difficult to
capture. Here, we present data focusing on movement co-
ordination parameters of the left hand fingers with the bow
hand in violinists and between two violinists in group play-
ing. Data was recorded with different position sensors, a
micro camcorder fixed on a violin and an acceleration sen-
sor placed on the bow. Sensor measurements were obtained
at a high sampling rate, gathering the data with a small mi-
crocontroller unit, connected with a laptop computer. To
capture bow's position, rotation and angle directly on the
bow to string contact point, the micro camcorder was fixed
near the bridge. Main focuses of interest were the changes
of the left hand finger, the temporal synchronization be-
tween left hand fingers with the right hand, the close up
view to the bow to string contact point and the contact of
the left hand finger and/or string to the fingerboard. Seven
violinists, from beginners to master class students played
scales in different rhythms, speeds and bowings and music
excerpts of free choice while being recorded. One measure-
ment with 2 violinists was made to see the time differences
between two musicians while playing together. For simple
integration of a conventional violin into electronic music en-
vironments, left hand sensor data were exemplary converted
to MIDI and OSC.

</abstract>
    <keywords> Strings, violin, coordination, left, finger, right, hand  </keywords>
  </document>
  <document>
    <name>nime2012_257.pdf</name>
    <abstract>
Tangible tabletop musical interfaces allowing for a collabo-
rative real-time interaction in live music performances are
one of the promising fields in NIMEs. At present, this kind
of interfaces present at least some of the following charac-
teristics that limit their musical use: latency in the inter-
action, and partial or complete lack of responsiveness to
gestures such as tapping, scrubbing or pressing force. Our
current research is exploring ways of improving the quality
of interaction with this kind of interfaces, and in particular
with the tangible tabletop instrument Reactable . In this
paper we present a system based on a circular array of me-
chanically intercoupled force sensing resistors used to obtain
a low-latency, affordable, and easily embeddable hardware
system able to detect surface impacts and pressures on the
tabletop perimeter. We also consider the option of com-
pleting this detected gestural information with the sound
information coming from a contact microphone attached to
the mechanical coupling layer, to control physical modelling
synthesis of percussion instruments.

</abstract>
    <keywords> tangible tabletop interfaces, force sensing resistor, mechan- ical coupling, fast low-noise analog to digital conversion, low-latency sensing, micro controller, multimodal systems, complementary sensing.  </keywords>
  </document>
  <document>
    <name>nime2012_258.pdf</name>
    <abstract>
This paper introduces Simpletones, an interactive sound system 
that enables a sense of musical collaboration for non-musicians. 
Participants can easily create simple sound compositions in real 
time by collaboratively operating physical artifacts as sound 
controllers. The physical configuration of the artifacts requires 
coordinated actions between participants to control sound (thus 
requiring, and emphasizing collaboration).

Simpletones encourages playful  human-to-human interaction 
by  introducing a simple interface and a set of basic rules [1]. 
This enables novices to focus on the collaborative aspects of 
making music as a group (such as synchronization and taking 
collective decisions through non-verbal communication) to 
ultimately engage a state of group flow[2]. 

This project is relevant to a contemporary discourse on 
musical expression because it allows novices to experience the 
social aspects of group music making, something that is usually  
reserved only for trained performers [3].

</abstract>
    <keywords> Collaboration, Artifacts, Computer Vision, Color Tracking,  State of Flow. </keywords>
  </document>
  <document>
    <name>nime2012_259.pdf</name>
    <abstract>
Composing music for ensembles of computer-based instru-
ments, such as laptop orchestra or mobile phone orchestra,
is a multi-faceted and challenging endeavor whose param-
eters and criteria for success are ill-defined. In the design
community, tasks with these qualities are known as wicked
problems. This paper frames composing for computer-based
ensemble as a design task, shows how Buchanan's four do-
mains of design are present in the task, and discusses its
wicked properties. The themes of visibility, risk, and em-
bodiment, as formulated by Klemmer, are shown to be im-
plicitly present in this design task. Composers are encour-
aged to address them explicitly and to take advantage of
the practices of prototyping and iteration.

</abstract>
    <keywords> Design, laptop orchestra, mobile phone orchestra, instru- ment design, interaction design, composition  </keywords>
  </document>
  <document>
    <name>nime2012_260.pdf</name>
    <abstract>
This paper presents the LoopJam installation which allows
participants to interact with a sound map using a 3D com-
puter vision tracking system. The sound map results from
similarity-based clustering of sounds. The playback of these
sounds is controlled by the positions or gestures of partic-
ipants tracked with a Kinect depth-sensing camera. The
beat-inclined bodily movements of participants in the in-
stallation are mapped to the tempo of played sounds, while
the playback speed is synchronized by default among all
sounds. We presented and tested an early version of the in-
stallation to three exhibitions in Belgium, Italy and France.
The reactions among participants ranged between curiosity
and amusement.

</abstract>
    <keywords> Interactive music systems and retrieval, user interaction and interfaces, audio similarity, depth sensors  </keywords>
  </document>
  <document>
    <name>nime2012_262.pdf</name>
    <abstract> 
This paper describes the conceptualization and development of 
an open source tool for controlling the sound of a saxophone 
via the gestures of its performer. The motivation behind this 
work is the need for easy access tools to explore, compose and 
perform electroacoustic music in Colombian music schools and 
conservatories. This work led to the adaptation of common 
hardware to be used as a sensor attached to an acoustic 
instrument and the development of software applications to 
record, visualize and map performers gesture data into signal 
processing parameters. The scope of this work suggested that 
focus was to be made on a specific instrument so the saxophone 
was chosen. Gestures were selected in an iterative process with 
the performer, although a more ambitious strategy to figure out 
main gestures of an instruments performance was first defined. 
Detailed gesture-to-sound processing mappings are exposed in 
the text. An electroacoustic musical piece was successfully 
rehearsed and recorded using the Gest-O system. 

</abstract>
  </document>
  <document>
    <name>nime2012_264.pdf</name>
    <abstract> 
The Fingerphone, a reworking of the Stylophone in conductive 
paper, is presented as an example of new design approaches for 
sustainability and playability of electronic musical instruments. 
</abstract>
  </document>
  <document>
    <name>nime2012_271.pdf</name>
    <abstract> 
This short paper follows an earlier NIME paper [1] describing 
the invention and construction of the Electrumpet. Revisions 
and playing experience are both part of the current paper. 
The Electrumpet can be heard in the performance given by 
Hans Leeuw and Diemo Schwarz at this NIME conference. 
 
</abstract>
    <keywords>  NIME, Electrumpet, live-electronics, hybrid instruments.   </keywords>
  </document>
  <document>
    <name>nime2012_272.pdf</name>
    <abstract>
This paper presents a toolbox of gestural control mecha-
nisms which are available when the input sensing apparatus
is a pair of data gloves fitted with orientation sensors. The
toolbox was developed in advance of a live music perfor-
mance in which the mapping from gestural input to audio
output was to be developed rapidly in collaboration with the
performer. The paper begins with an introduction to the as-
sociated literature before introducing a range of continuous,
discrete and combined control mechanisms, enabling a flex-
ible range of mappings to be explored and modified easily.
An application of the toolbox within a live music perfor-
mance is then described with an evaluation of the system
with ideas for future developments.

</abstract>
  </document>
  <document>
    <name>nime2012_275.pdf</name>
    <abstract>
I present a novel low-tech multidimensional gestural con-
troller, based on the resistive properties of a 2D field of
pencil markings on paper. A set of movable electrodes (+,
-, ground) made from soldered stacks of coins create a dy-
namic voltage potential field in the carbon layer, and an-
other set of movable electrodes tap voltages from this field.
These voltages are used to control complex sound engines
in an analogue modular synthesizer. Both the voltage field
and the tap electrodes can be moved freely. The design
was inspired by previous research in complex mappings for
advanced digital instruments, and provides a similarly dy-
namic playing environment for analogue synthesis. The in-
terface is cheap to build, and provides flexible control over
a large set of parameters. It is musically satisfying to play,
and allows for a wide range of playing techniques, from wild
exploration to subtle expressions. I also present an inven-
tory of the available playing techniques, motivated by the
interface design, musically, conceptually and theatrically.
The performance aspects of the interface are also discussed.
The interface has been used in a number of performances
in Sweden and Japan in 2011, and is also used by other
musicians.

</abstract>
  </document>
  <document>
    <name>nime2012_279.pdf</name>
    <abstract>

As a 2010 Artist in Residence in Musical Research at IRCAM,

Mari Kimura used the Augmented Violin to develop new

compositional approaches, and new ways of creating interactive

performances [1]. She contributed her empirical and historical

knowledge of violin bowing technique, working with the Real

Time Musical Interactions Team at IRCAM.  Thanks to this

residency, her ongoing long-distance collaboration with the

team since 2007 dramatically accelerated, and led to solving

several compositional and calibration issues of the Gesture

Follower (GF) [2]. Kimura was also the first artist to develop

projects between the two teams at IRCAM, using OMAX

(Musical Representation Team) with GF.  In the past year, the

performance with Augmented Violin has been expanded in

larger scale interactive audio/visual projects as well.  In this

paper, we report on the various techniques developed for the

Augmented Violin and compositions by Kimura using them,
offering specific examples and scores.

</abstract>
    <keywords> Augmented Violin, Gesture Follower, Interactive Performance  </keywords>
  </document>
  <document>
    <name>nime2012_284.pdf</name>
    <abstract>
The Electromagnetically Sustained Rhodes Piano is an orig-
inal Rhodes Piano modified to provide control over the
amplitude envelope of individual notes through aftertouch
pressure. Although there are many opportunities to shape
the amplitude envelope before loudspeaker amplification,
they are all governed by the ever-decaying physical vibra-
tions of the tone generating mechanism. A single-note proof
of concept for electromagnetic control over this vibrating
mechanism was presented at NIME 2011.

In the past year, virtually every aspect of the system has
been improved. We use a different vibration sensor that
is immune to electromagnetic interference, thus eliminat-
ing troublesome feedback. For control, we both reduce cost
and gain continuous position sensing throughout the entire
range of key motion in addition to aftertouch pressure. Fi-
nally, the entire system now fits within the space constraints
presented by the original piano, allowing it to be installed
on adjacent notes.

</abstract>
    <keywords> Rhodes, piano, mechanical synthesizer, electromagnetic, sus- tain, feedback  INTRODUCTION The Rhodes Piano sound has been a staple of mainstream music since its </keywords>
  </document>
  <document>
    <name>nime2012_291.pdf</name>
    <abstract>
We have added a dynamic bio-mechanical mapping layer
that contains a model of the human vocal tract with tongue
muscle activations as input and tract geometry as output to
a real time gesture controlled voice synthesizer system used
for musical performance and speech research. Using this
mapping layer, we conducted user studies comparing con-
trolling the model muscle activations using a 2D set of force
sensors with a position controlled kinematic input space
that maps directly to the sound. Preliminary user evalu-
ation suggests that it was more difficult to using force input
but the resultant output sound was more intelligible and
natural compared to the kinematic controller. This result
shows that force input is a potentially feasible for browsing
through a vowel space for an articulatory voice synthesis
system, although further evaluation is required.

</abstract>
    <keywords> Gesture, Mapping, Articulatory, Speech, Singing, Synthesis  </keywords>
  </document>
  <document>
    <name>nime2012_292.pdf</name>
    <abstract> 
This paper presents the author's _derivations system, an 
interactive performance system for solo improvising 
instrumentalist. The system makes use of a combination of real-
time audio analysis, live sampling and spectral re-synthesis to 
build a vocabulary of possible performative responses to live 
instrumental input throughout an improvisatory performance. A 
form of timbral matching is employed to form a link between 
the live performer and an expanding database of musical 
materials. In addition, the system takes into account the unique 
nature of the rehearsal/practice space in musical performance 
through the implementation of performer-configurable 
cumulative rehearsal databases into the final design. This paper 
discusses the system in detail with reference to related work in 
the field, making specific reference to the system's interactive 
potential both inside and outside of a real-time performance 
context. 
 
</abstract>
    <keywords>  Interactivity, performance systems, improvisation   </keywords>
  </document>
  <document>
    <name>nime2012_293.pdf</name>
    <abstract>
We present Patchwerk, a networked synthesizer module with
tightly coupled web browser and tangible interfaces. Patch-
werk connects to a pre-existing modular synthesizer using
the emerging cross-platform HTML5 WebSocket standard
to enable low-latency, high-bandwidth, concurrent control
of analog signals by multiple users. Online users control
physical outputs on a custom-designed cabinet that reflects
their activity through a combination of motorized knobs
and LEDs, and streams the resultant audio. In a typical
installation, a composer creates a complex physical patch
on the modular synth that exposes a set of analog and
digital parameters (knobs, buttons, toggles, and triggers)
to the web-enabled cabinet. Both physically present and
online audiences can control those parameters, simultane-
ously seeing and hearing the results of each other's actions.
By enabling collaborative interaction with a massive analog
synthesizer, Patchwerk brings a broad audience closer to a
rare and historically important instrument. Patchwerk is
available online at http://synth.media.mit.edu.

</abstract>
    <keywords> Modular synthesizer, HTML5, tangible interface, collabora- tive musical instrument  </keywords>
  </document>
  <document>
    <name>nime2012_299.pdf</name>
    <abstract>
Message mapping between control interfaces and sound en-
gines is an important task that could benefit from tools
that streamline development. A new Open Sound Con-
trol (OSC) namespace called Nexus Data Exchange Format
(NDEF) streamlines message mapping by offering develop-
ers the ability to manage sound engines as network nodes
and to query those nodes for the messages in their OSC ad-
dress spaces. By using NDEF, developers will have an eas-
ier time managing nodes and their messages, especially for
scenarios in which a single application or interface controls
multiple sound engines. NDEF is currently implemented
in the JunctionBox interaction toolkit but could easily be
implemented in other toolkits.

</abstract>
    <keywords> OSC, namespace, interaction, node  </keywords>
  </document>
  <document>
    <name>nime2012_301.pdf</name>
    <abstract>
Aural - of or relateing to the ear or hearing
Aura - an invisible breath, emanation, or radiation
AR - Augmented Reality

AuRal is an environmental audio system in which individ-
ual participants form ad hoc ensembles based on geolocation
and affect the overall sound of the music associated with the
location that they are in.

The AuRal environment binds physical location and the
choices of multiple, simultaneous performers to act as the
generative force of music tied to the region. Through a mo-
bile device interface, musical participants, or agents, have a
degree of input into the generated music essentially defin-
ing the sound of a given region. The audio landscape is
superimposed onto the physical one. The resultant mu-
sical experience is not tied simply to the passage of time,
but through the incorporation of participants over time and
spatial proximity, it becomes an aural location as much as
a piece of music. As a result, walking through the same
location at different times results in unique collaborative
listening experiences.

</abstract>
  </document>
  <document>
    <name>nime2012_303.pdf</name>
    <abstract>ion for Distributed NIMEs

Charles Roberts
Media Arts and Technology

Program, UCSB
charlie@charlie-roberts.com

Graham Wakefield
Media Arts and Technology

Program, UCSB
wakefield@mat.ucsb.edu

Matthew Wright
Media Arts and Technology

Program, UCSB
matt@create.ucsb.edu

ABSTRACT
Designing mobile interfaces for computer-based musical per-
formance is generally a time-consuming task that can be ex-
asperating for performers. Instead of being able to experi-
ment freely with physical interfaces' affordances, performers
must spend time and attention on non-musical tasks includ-
ing network configuration, development environments for
the mobile devices, defining OSC address spaces, and han-
dling the receipt of OSC in the environment that will control
and produce sound. Our research seeks to overcome such
obstacles by minimizing the code needed to both generate
and read the output of interfaces on mobile devices. For iOS
and Android devices, our implementation extends the appli-
cation Control to use a simple set of OSC messages to define
interfaces and automatically route output. On the desktop,
our implementations in Max/MSP/Jitter, LuaAV, and Su-
perCollider allow users to create mobile widgets mapped to
sonic parameters with a single line of code. We believe the
fluidity of our approach will encourage users to incorporate
mobile devices into their everyday performance practice.

</abstract>
    <keywords> NIME, OSC, Zeroconf, iOS, Android, Max/MSP/Jitter, Lu- aAV, SuperCollider, Mobile  </keywords>
  </document>
  <document>
    <name>nime2012_308.pdf</name>
    <abstract>
This  paper  provides  an  overview  of  a  new  method  for 
approaching beat sequencing.  As we have come to know them 
drum machines provide means to loop rhythmic patterns over a 
certain interval.   Usually with the option to specify different 
beat divisions.  What I developed and propose for consideration 
is a rethinking of the traditional drum machine confines.  The 
Sinkapater  is  an  untethered  beat  sequencer  in  that  the  beat 
division,  and the loop length can be arbitrarily  modified for 
each  track.   The  result  is  the  capability  to  create  complex 
syncopated patterns which evolve over time as different tracks 
follow their own loop rate.  To keep cohesion all channels can 
be locked to a master channel forcing a loop to be an integer 
number  of  "Master  Beats".   Further  a  visualization  mode 
enables  exploring  the  patterns  in  another  new  way.   Using 
synchronized OpenGL a 3-Dimensional environment visualizes 
the  beats  as  droplets  falling  from faucets  of  varying heights 
determined by the loop length.  Waves form in the bottom as 
beats splash into the virtual "sink".  By combining compelling 
visuals  and  a  new  approach  to  sequencing  a  new  way  of 
exploring beats and experiencing music has been created.

</abstract>
    <keywords> NIME, proceedings, drum machine, sequencer, visualization  </keywords>
  </document>
  <document>
    <name>nime2012_309.pdf</name>
    <abstract> 
This research aims to improve the correspondence between 

music and dance, and explores the use of human respiration 

pattern for musical applications with focus on the motional 

aspect of breathing. While respiration is frequently considered 

as an indicator of the metabolic state of human body that 

contains meaningful information for medicine or psychology, 

motional aspect of respiration has been relatively unnoticed in 

spite of its strong correlation with muscles and the brain.  

 This paper introduces an interactive system to control music 

playback for dance performances based on the respiration 

pattern of the dancer. A wireless wearable sensor device 

detects the dancer's respiration, which is then utilized to 
modify the dynamic of music. Two different respiration-

dynamic mappings were designed and evaluated through 

public performances and private tests by professional 

choreographers. Results from this research suggest a new 

conceptual approach to musical applications of respiration 

based on the technical characteristics of music and dance. 

 

</abstract>
    <keywords>  Music, dance, respiration, correspondence, wireless interface,   interactive performance   </keywords>
  </document>
  <document>
    <name>nime2012_310.pdf</name>
    <abstract>
We present the integration of two musical interfaces into a
new music-making system that seeks to capture the expe-
rience of a choir and bring it into the mobile space. This
system relies on three pervasive technologies that each sup-
port a different part of the musical experience. First, the
mobile device application for performing with an artificial
voice, called ChoirMob. Then, a central composing and con-
ducting application running on a local interactive display,
called Vuzik. Finally, a network protocol to synchronize
the two. ChoirMob musicians can perform music together
at any location where they can connect to a Vuzik central
conducting device displaying a composed piece of music. We
explored this system by creating a chamber choir of Choir-
Mob performers, consisting of both experienced musicians
and novices, that performed in rehearsals and live concert
scenarios with music composed using the Vuzik interface.

</abstract>
    <keywords> singing synthesis, mobile music, interactive display, inter- face design, OSC, ChoirMob, Vuzik, social music, choir  </keywords>
  </document>
  <document>
    <name>nime2012_315.pdf</name>
    <abstract> 
In the following paper we propose a new tiered granularity 
approach to developing modules or abstractions in the Pd-
L2Ork visual multimedia programming environment with the 
specific goal of devising creative environments that scale their 
educational scope and difficulty to encompass several stages 
within the context of primary and secondary (K-12) education. 
As part of a preliminary study, the team designed modules 
targeting 4th and 5th grade students, the primary focus being 
exploration of creativity and collaborative learning. The 
resulting environment infrastructure - coupled with the Boys &amp; 
Girls Club of Southwest Virginia Satellite Linux Laptop 
Orchestra - offers opportunities for students to design and build 
original instruments, master them through a series of rehearsals, 
and ultimately utilize them as part of an ensemble in a 
performance of a predetermined piece whose parameters are 
coordinated by instructor through an embedded networked 
module. The ensuing model will serve for the assessment and 
development of a stronger connection with content-area 
standards and the development of creative thinking and 
collaboration skills. 
 
</abstract>
    <keywords>  Granular, Learning Objects, K-12, Education, L2Ork, Pd- L2Ork   </keywords>
  </document>
</documents>
