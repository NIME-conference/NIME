<documents>
  <document>
    <name>nime2001_003.pdf</name>
    <abstract>This paper will present observations on the design, artistic, and human factors of creating digital music controllers. Specific projects will be presented, and a set of design principles will be supported from those examples. </abstract>
    <keywords>Musical control, artistic interfaces. </keywords>
  </document>
  <document>
    <name>nime2001_007.pdf</name>
    <abstract>Over the last four years, we have developed a series oflectures, labs and project assignments aimed at introducingenough technology so that students from a mix ofdisciplines can design and build innovative interfacedevices.</abstract>
    <keywords>Input devices, music controllers, CHI technology, courses. </keywords>
  </document>
  <document>
    <name>nime2001_011.pdf</name>
    <abstract>In this paper we describe our efforts towards thedevelopment of live performance computer-based musicalinstrumentation. Our design criteria include initial easeof use coupled with a long term potential for virtuosity,minimal and low variance latency, and clear and simplestrategies for programming the relationship betweengesture and musical result. We present customcontrollers and unique adaptations of standard gesturalinterfaces, a programmable connectivity processor, acommunications protocol called Open Sound Control(OSC), and a variety of metaphors for musical control. We further describe applications of our technology to avariety of real musical performances and directions forfuture research.</abstract>
  </document>
  <document>
    <name>nime2001_015.pdf</name>
    <abstract>This paper reviews the existing literature on input deviceevaluation and design in human-computer interaction (HCI)and discusses possible applications of this knowledge tothe design and evaluation of new interfaces for musicalexpression. Specifically, a set of musical tasks is suggestedto allow the evaluation of different existing controllers. </abstract>
    <keywords>Input device design, gestural control, interactive systems </keywords>
  </document>
  <document>
    <name>nime2001_019.pdf</name>
    <abstract>?&lt;.0) L'L$&amp;) L&amp;$0$"#0) #&lt;$) ."#$&amp;T',$) ($Q$+-LN$"#0) '"()N40.,-T) #&lt;$) (4-) U."#$&amp;T',$1V) T-&amp;N$() J;) 64&amp;#.0) W'&lt;") '"() 5'"?&amp;4$N'"M) ) X$) ($0,&amp;.J$) %$0#4&amp;'+) ."0#&amp;4N$"#) ($0.%"1."#$&amp;',#.Q$) L$&amp;T-&amp;N'",$) ."#$&amp;T',$0) T-&amp;) .NL&amp;-Q.0'#.-"'+N40.,1)0L&lt;$&amp;.,'+)0L$'S$&amp;0)EN4+#.C,&lt;'""$+1)-4#R'&amp;(C&amp;'(.'#."%%$-($0.,) 0L$'S$&amp;) '&amp;&amp;';0F) '"() 3$"0-&amp;C3L$'S$&amp;C/&amp;&amp;';0E3$"3/0Y) ,-NJ."'#.-"0) -T) Q'&amp;.-40) 0$"0-&amp;) ($Q.,$0) R.#&lt;0L&lt;$&amp;.,'+) 0L$'S$&amp;) '&amp;&amp;';0FM)X$) (.0,400) #&lt;$) ,-",$L#1) ($0.%"'"(),-"0#&amp;4,#.-")-T)#&lt;$0$)0;0#$N01)'"(1)%.Q$)$Z'NL+$0)T&amp;-N0$Q$&amp;'+)"$R)L4J+.0&lt;$()650)-T)R-&amp;S)J;)W'&lt;")'"()?&amp;4$N'"M</abstract>
    <keywords>!"#$&amp;',#.Q$) O40.,) $&amp;T-&amp;N'",$1) [$0#4&amp;'+ !"#$&amp;T',$1) 3-".,5.0L+';1)3$"0-&amp;\3L$'S$&amp;)/&amp;&amp;';1)3$"3/M </keywords>
  </document>
  <document>
    <name>nime2001_024.pdf</name>
    <keywords>multidimensionality, control, resonance, pitch tracking </keywords>
  </document>
  <document>
    <name>nime2001_027.pdf</name>
    <abstract>The Accordiatron is a new MIDI controller for real-timeperformance based on the paradigm of a conventionalsqueeze box or concertina. It translates the gestures of aperformer to the standard communication protocol ofMIDI, allowing for flexible mappings of performance datato sonic parameters. When used in conjunction with a realtime signal processing environment, the Accordiatronbecomes an expressive, versatile musical instrument. Acombination of sensory outputs providing both discrete andcontinuous data gives the subtle expressiveness and controlnecessary for interactive music.</abstract>
    <keywords>MIDI controllers, computer music, interactive music, electronic musical instruments, musical instrument design, human computer interface </keywords>
  </document>
  <document>
    <name>nime2001_030.pdf</name>
    <abstract>The technologies behind passive resonant magneticallycoupled tags are introduced and their application as amusical controller is illustrated for solo or groupperformances, interactive installations, and music toys. </abstract>
    <keywords>RFID, resonant tags, EAS tags, musical controller, tangible interface </keywords>
  </document>
  <document>
    <name>nime2001_034.pdf</name>
    <abstract>In this paper, we introduce our research challenges for creating new musical instruments using everyday-life media with intimate interfaces, such as the self-body, clothes, water and stuffed toys. Various sensor technologies including image processing and general touch sensitive devices are employed to exploit these interaction media. The focus of our effort is to provide user-friendly and enjoyable experiences for new music and sound performances. Multimodality of musical instruments is explored in each attempt. The degree of controllability in the performance and the richness of expressions are also discussed for each installation. </abstract>
    <keywords>New interface, music controller, dance, image processing, water interface, stuffed toy </keywords>
  </document>
  <document>
    <name>nime2001_038.pdf</name>
    <abstract>The MATRIX (Multipurpose Array of Tactile Rods forInteractive eXpression) is a new musical interface foramateurs and professionals alike. It gives users a 3dimensional tangible interface to control music using theirhands, and can be used in conjunction with a traditionalmusical instrument and a microphone, or as a stand-alonegestural input device. The surface of the MATRIX acts as areal-time interface that can manipulate the parameters of asynthesis engine or effect algorithm in response to aperformer's expressive gestures. One example is to have therods of the MATRIX control the individual grains of agranular synthesizer, thereby "sonically sculpting" themicrostructure of a sound. In this way, the MATRIXprovides an intuitive method of manipulating sound with avery high level of real-time control.</abstract>
    <keywords>Musical controller, tangible interface, real-time expression, audio synthesis, effects algorithms, signal processing, 3-D interface, sculptable surface </keywords>
  </document>
  <document>
    <name>nime2001_051.pdf</name>
    <abstract>KL0/! %,%()! )(M0(N/! ,! 2&amp;$H()! #4! %)#O(F'/! 'L,'! (P%+#)(!H&amp;0+-021! (+(F')#20F! $&amp;/0F,+! !"#$%&amp;Q! 02'()4,F(/! ,2-! #HO(F'/!-(/012(-! '#! H(! &amp;/(-! ,2-! (2O#3(-! H3! ,23H#-3! H&amp;'! 02!%,)'0F&amp;+,)! 'L#/(! NL#! -#! 2#'! /((! 'L($/(+M(/! ,/! 2,'&amp;),++3!$&amp;/0F,+J! R2! )(4+(F'021! #2! 'L(! /')(21'L/! #4! 'L(/(! %)#O(F'/Q!02'()(/'021! -0)(F'0#2/! 4#)! /0$0+,)! N#)S! 02! 'L(! 4&amp;'&amp;)(! ,)(!F#2/0-()(-J!</abstract>
    <keywords>T+,3Q!(P%+#),'0#2Q!/#&amp;2-!$,%%021Q!(21,1021!F#2'(2'Q!/#&amp;2-! -(/012J! </keywords>
  </document>
  <document>
    <name>nime2002_001.pdf</name>
    <abstract>In this paper we describe the digital emulation of a optical photosonic instrument. First we briefly describe theoptical instrument which is the basis of this emulation.Then we give a musical description of the instrumentimplementation and its musical use and we concludewith the "duo" possibility of such an emulation.</abstract>
    <keywords>Photosonic synthesis, digital emulation, Max-Msp, gestural devices. </keywords>
  </document>
  <document>
    <name>nime2002_005.pdf</name>
    <abstract>In this paper we will have a short overview of some of the systems we have been developing as an independent company over the last years. We will focus especially on our latest experiments in developing wireless gestural systems using the camera as an interactive tool to generate 2D and 3D visuals and music.  </abstract>
  </document>
  <document>
    <name>nime2002_010.pdf</name>
    <abstract>This paper describes the design and development of several musical instruments and MIDI controllers built byDavid Bernard (as part of The Sound Surgery project:www.thesoundsurgery.co.uk) and used in club performances around Glasgow during 1995-2002. It argues thatchanging technologies and copyright are shifting ourunderstanding of music from "live art" to "recorded medium" whilst blurring the boundaries between sound andvisual production.</abstract>
    <keywords>Live electronic music, experimental instruments, MIDI controllers, audio-visual synchronisation, copyright, SKINS digital hand drum. </keywords>
  </document>
  <document>
    <name>nime2002_012.pdf</name>
    <abstract>This paper discusses the Jam-O-Drum multi-player musical controller and its adaptation into a gaming controller interface known as the Jam-O-Whirl. The Jam-O-World project positioned these two controller devices in a dedicated projection environment that enabled novice players to participate in immersive musical gaming experiences. Players' actions, detected via embedded sensors in an integrated tabletop surface, control game play, real-time computer graphics and musical interaction. Jam-O-World requires physical and social interaction as well as collaboration among players. </abstract>
    <keywords>Collaboration, computer graphics, embedded sensors, gam- ing controller, immersive musical gaming experiences, mu- sical controller, multi-player, novice, social interaction.  </keywords>
  </document>
  <document>
    <name>nime2002_038.pdf</name>
    <abstract>Mapping, which describes the way a performer's controls are connected to sound variables, is a useful concept when applied to the structure of electronic instruments modelled after traditional acoustic instruments. But mapping is a less useful concept when applied to the structure of complex and interactive instruments in which algorithms generate control information. This paper relates the functioning and benefits of different types of electronic instruments to the structural principles on which they are based. Structural models of various instruments will be discussed and musical examples played. </abstract>
    <keywords>mapping fly-by-wire algorithmic network in- teractivity instrument deterministic indetermin- istic  </keywords>
  </document>
  <document>
    <name>nime2002_043.pdf</name>
    <abstract>This paper describes a virtual musical instrument based on the scanned synthesis technique and implemented in Max-Msp. The device is composed of a computer and three gesture sensors. The timbre of the produced sound is rich and changing. The instrument proposes an intuitive and expressive control of the sound thanks to a complex mapping between gesture and sound. </abstract>
  </document>
  <document>
    <name>nime2002_050.pdf</name>
    <abstract>In this paper we describe three new music controllers, eachdesigned to be played by two players. As the intimacy between two people increases so does their ability to anticipateand predict the other's actions. We hypothesize that this intimacy between two people can be used as a basis for newcontrollers for musical expression. Looking at ways peoplecommunicate non-verbally, we are developing three new instruments based on different communication channels. TheTooka is a hollow tube with a pressure sensor and buttonsfor each player. Players place opposite ends in their mouthsand modulate the pressure in the tube with their tongues andlungs, controlling sound. Coordinated button presses control the music as well. The Pushka, yet to be built, is a semirigid rod with strain gauges and position sensors to track therod's position. Each player holds opposite ends of the rodand manipulates it together. Bend, end point position, velocity and acceleration and torque are mapped to musicalparameters. The Pullka, yet to be built, is simply a string attached at both ends with two bridges. Tension is measuredwith strain gauges. Players manipulate the string tensionat each end together to modulate sound. We are looking atdifferent musical mappings appropriate for two players.</abstract>
    <keywords>Two person musical instruments, intimacy, human-human communication, cooperative music, passive haptic interface </keywords>
  </document>
  <document>
    <name>nime2002_056.pdf</name>
    <abstract>The Cardboard Box Garden (CBG) originated from a dissatisfaction with current computer technology as it is presented to children. This paper shall briefly review the process involved in the creation of this installation, from motivation through to design and subsequent implementation and user experience with the CBG. Through the augmentation of an everyday artefact, namely the standard cardboard box, a simple yet powerful interactive environment was created that has achieved its goal of stirring childrens imagination - judging from the experience of our users. </abstract>
    <keywords>Education, play, augmented reality, pervasive comput- ing, disappearing computer, assembly, cardboard box  </keywords>
  </document>
  <document>
    <name>nime2002_059.pdf</name>
    <abstract>Research and musical creation with gestural-orientedinterfaces have recently seen a renewal of interest andactivity at Ircam [1][2]. In the course of several musicalprojects, undertaken by young composers attending theone-year Course in Composition and Computer Music or byguests artists, Ircam Education and Creation departmentshave proposed various solutions for gesture-controlledsound synthesis and processing. In this article, we describethe technical aspects of AtoMIC Pro, an Analog to MIDIconverter proposed as a re-usable solution for digitizingseveral sensors in different contexts such as interactivesound installation or virtual instruments.The main direction of our researches, and of this one inparticular, is to create tools that can be fully integrated intoan artistic project as a real part of the composition andperformance processes.</abstract>
    <keywords>Gestural controller, Sensor, MIDI, Music. SOLUTION FOR MULTI-SENSOR ACQUISITION </keywords>
  </document>
  <document>
    <name>nime2002_065.pdf</name>
    <abstract>We explore the role that metaphor plays in developing expressive devices by examining the MetaMuse system. MetaMuse is a prop-based system that uses the metaphor of rainfall to make the process of granular synthesis understandable. We discuss MetaMuse within a framework we call"transparency" that can be used as a predictor of the expressivity of musical devices. Metaphor depends on a literature,or cultural basis, which forms the basis for making transparent device mappings. In this context we evaluate the effectof metaphor in the MetaMuse system.</abstract>
    <keywords>Expressive interface, transparency, metaphor, prop-based con- troller, granular synthesis. </keywords>
  </document>
  <document>
    <name>nime2002_071.pdf</name>
    <abstract>The use of free gesture in making music has usually been confined to instruments that use direct mappings between movement and sound space. Here we demonstrate the use of categories of gesture as the basis of musical learning and performance collaboration. These are used in a system that reinterprets the approach to learning through performance that is found in many musical cultures and discussed here through the example of Kpelle music. </abstract>
    <keywords>Collaboration, Performance, Metaphor, Gesture  </keywords>
  </document>
  <document>
    <name>nime2002_073.pdf</name>
    <abstract>This paper presents a novel coupling of haptics technology and music, introducing the notion of tactile composition or aesthetic composition for the sense of touch. A system that facilitates the composition and perception of intricate, musically structured spatio-temporal patterns of vibration on the surface of the body is described. An initial test of the system in a performance context is discussed. The fundamental building blocks of a compositional language for touch are considered. </abstract>
  </document>
  <document>
    <name>nime2002_080.pdf</name>
    <abstract>The Circular Optical Object Locator is a collaborative and cooperative music-making device. It uses an inexpensive digital video camera to observe a rotating platter. Opaque objects placed on the platter are detected by the camera during rotation. The locations of the objects passing under the camera are used to generate music. </abstract>
    <keywords>Input devices, music controllers, collaborative, real-time score manipulation.  </keywords>
  </document>
  <document>
    <name>nime2002_082.pdf</name>
    <abstract>We have created a new electronic musical instrument, referred to as the Termenova (Russian for "daughter of Theremin") that combines a free-gesture capacitivesensing device with an optical sensing system that detects the reflection of a hand when it intersects a beam of an array of red lasers. The laser beams, which are made visible by a thin layer of theatrical mist, provide visual feedback and guidance to the performer to alleviate the difficulties of using a non-contact interface as well as adding an interesting component for the audience to observe. The system uses capacitive sensing to detect the proximity of the player's hands; this distance is mapped to pitch, volume, or other continuous effect. The laser guide positions are calibrated before play with positioncontrolled servo motors interfaced to a main controller board; the location of each beam corresponds to the position where the performer should move his or her hand to achieve a pre-specified pitch and/or effect. The optical system senses the distance of the player's hands from the source of each laser beam, providing an additional dimension of musical control. </abstract>
    <keywords>Theremin, gesture interface, capacitive sensing, laser harp, optical proximity sensing, servo control, musical controller  </keywords>
  </document>
  <document>
    <name>nime2002_094.pdf</name>
    <keywords>musical controller, Tactex, tactile interface, tuning sys- tems  </keywords>
  </document>
  <document>
    <name>nime2002_101.pdf</name>
    <abstract>We are interested in exhibiting our programs at your demo section at the conference. We believe that the subject of your conference is precisely what we are experimenting with in our musical software. </abstract>
    <keywords>Further info on our website http//www.ixi-software.net.  </keywords>
  </document>
  <document>
    <name>nime2002_102.pdf</name>
    <abstract>In this paper we present Afasia, an interactive multimedia performance based in Homer's Odyssey [2]. Afasia is a one-man digital theater play in which a lone performer fitted with a sensor-suit conducts, like Homer, the whole show by himself, controlling 2D animations, DVD video and conducting the music mechanically performed by a robot quartet. After contextualizing the piece, all of its technical elements, starting with the hardware input and output components, are described. A special emphasis is given to the interactivity strategies and the subsequent software design. Since its first version premiered in Barcelona in 1998, Afasia has been performed in many European and American countries and has received several international awards. </abstract>
    <keywords>Multimedia interaction, musical robots, real-time musi- cal systems.  </keywords>
  </document>
  <document>
    <name>nime2002_108.pdf</name>
    <abstract>This paper describes the design of an electronic Tabla controller. The E-Tabla controls both sound and graphics simultaneously. It allows for a variety of traditional Tabla strokes and new performance techniques. Graphical feedback allows for artistical display and pedagogical feedback. </abstract>
    <keywords>Electronic Tabla, Indian Drum Controller, Physical Models, Graphical Feedback   </keywords>
  </document>
  <document>
    <name>nime2002_113.pdf</name>
    <abstract>In this paper, we describe a computer-based solo musical instrument for live performance. We have adapted a Wacom graphic tablet equipped with a stylus transducer and a game joystick to use them as a solo expressive instrument. We have used a formant-synthesis model that can produce a vowel-like singing voice. This instrument allows multidimensional expressive fundamental frequency control and vowel articulation. The fundamental frequency angular control used here allows different mapping adjustments that correspond to different melodic styles. </abstract>
    <keywords>Bi-manual, off-the-shelf input devices, fundamental fre- quency control, sound color navigation, mapping.  </keywords>
  </document>
  <document>
    <name>nime2002_118.pdf</name>
    <abstract>This paper introduces a subtle interface, which evolved from the design of an alternative gestural controller in the development of a performance interface. The conceptual idea used is based on that of the traditional Bodhran instrument, an Irish frame drum. The design process was user-centered and involved professional Bodhran players and through prototyping and usertesting the resulting Vodhran emerged. </abstract>
    <keywords>Virtual instrument, sound modeling, gesture, user- centered design  </keywords>
  </document>
  <document>
    <name>nime2002_120.pdf</name>
    <abstract>Here we present 2Hearts, a music system controlled bythe heartbeats of two people. As the players speak andtouch, 2Hearts extracts meaningful variables from theirheartbeat signals. These variables are mapped to musical parameters, conveying the changing patterns of tension and relaxation in the players' relationship. We describe the motivation for creating 2Hearts, observationsfrom the prototypes that have been built, and principleslearnt in the ongoing development process.</abstract>
    <keywords>Heart Rate, Biosensor, Interactive Music, Non-Verbal Communication, Affective Computing, Ambient Display </keywords>
  </document>
  <document>
    <name>nime2002_126.pdf</name>
    <keywords>Gesture, weight distribution, effort, expression, intent, movement, 3D sensing pressure, force, sensor, resolu- tion, control device, sound, music, input.  </keywords>
  </document>
  <document>
    <name>nime2002_131.pdf</name>
    <abstract>This paper briefly describes a number of performance interfaces under the broad theme of Interactive Gesture Music (IGM). With a short introduction, this paper discusses the main components of a Trans-Domain Mapping (TDM) framework, and presents various prototypes developed under this framework, to translate meaningful activities from one creative domain onto another, to provide real-time control of musical events with physical movements. </abstract>
    <keywords>Gesture, Motion, Interactive, Performance, Music.  </keywords>
  </document>
  <document>
    <name>nime2002_137.pdf</name>
    <abstract>The design of a virtual keyboard, capable of reproducing the tactile feedback of several musical instruments is reported. The key is driven by a direct drive motor, which allows friction free operations. The force to be generated by the motor is calculated in real time by a dynamic simulator, which contains the model of mechanisms' components and constraints. Each model is tuned on the basis of measurements performed on the real system. So far, grand piano action, harpsichord and Hammond organ have been implemented successfully on the system presented here. </abstract>
    <keywords>Virtual mechanisms, dynamic simulation  </keywords>
  </document>
  <document>
    <name>nime2002_143.pdf</name>
    <abstract>Interactivity has become a major consideration in the development of a contemporary art practice that engages with the proliferation of computer based technologies. Keywords </abstract>
    <keywords>are your choice.  </keywords>
  </document>
  <document>
    <name>nime2002_145.pdf</name>
    <abstract>Passive RF Tagging can provide an attractive medium for development of free-gesture musical interfaces. This was initially explored in our Musical Trinkets installation, which used magnetically-coupled resonant LC circuits to identify and track the position of multiple objects in real-time. Manipulation of these objects in free space over a read coil triggered simple musical interactions. Musical Navigatrics builds upon this success with new more sensitive and stable sensing, multi-dimensional response, and vastly more intricate musical mappings that enable full musical exploration of free space through the dynamic use and control of arpeggiatiation and effects. The addition of basic sequencing abilities also allows for the building of complex, layered musical interactions in a uniquely easy and intuitive manner. </abstract>
    <keywords>passive tag, position tracking, music sequencer interface  </keywords>
  </document>
  <document>
    <name>nime2002_148.pdf</name>
    <abstract>We present Audiopad, an interface for musical performance that aims to combine the modularity of knob based controllers with the expressive character of multidimensional tracking interfaces. The performer's manipulations of physical pucks on a tabletop control a real-time synthesis process. The pucks are embedded with LC tags that the system tracks in two dimensions with a series of specially shaped antennae. The system projects graphical information on and around the pucks to give the performer sophisticated control over the synthesis process.</abstract>
    <keywords>RF tagging, MIDI, tangible interfaces, musical controllers, object tracking </keywords>
  </document>
  <document>
    <name>nime2002_156.pdf</name>
    <abstract>In this paper, we develop the concept of "composed instruments". We will look at this idea from two perspectives: the design of computer systems in the context of live performed music and musicological considerations. A historical context is developed. Examples will be drawn from recent compositions. Finally basic concepts from computer science will be examined for their relation ship to this concept. </abstract>
    <keywords>Instruments, musicology, composed instrument, Theremin, Martenot, interaction, streams, MAX.  </keywords>
  </document>
  <document>
    <name>nime2002_161.pdf</name>
    <abstract>The cicada uses a rapid sequence of buckling ribs to initiate and sustain vibrations in its tymbal plate (the primarymechanical resonator in the cicada's sound production system). The tymbalimba, a music controller based on thissame mechanism, has a row of 4 convex aluminum ribs (ason the cicada's tymbal) arranged much like the keys on acalimba. Each rib is spring loaded and capable of snappingdown into a V-shape (a motion referred to as buckling), under the downward force of the user's finger. This energygenerated by the buckling motion is measured by an accelerometer located under each rib and used as the inputto a physical model.</abstract>
    <keywords>Bioacoustics, Physical Modeling, Controllers, Cicada, Buck- ling mechanism. </keywords>
  </document>
  <document>
    <name>nime2002_167.pdf</name>
    <abstract>This paper describes the hardware and the software of a computer-based doppler-sonar system for movement detection. The design is focused on simplicity and lowcost do-it-yourself construction. </abstract>
    <keywords>sonar  </keywords>
  </document>
  <document>
    <name>nime2002_171.pdf</name>
    <abstract>This paper describes a technique of multimodal, multichannel control of electronic musical devices using two control methodologies, the Electromyogram (EMG) and relative position sensing. Requirements for the application of multimodal interaction theory in the musical domain are discussed. We introduce the concept of bidirectional complementarity to characterize the relationship between the component sensing technologies. Each control can be used independently, but together they are mutually complementary. This reveals a fundamental difference from orthogonal systems. The creation of a concert piece based on this system is given as example. </abstract>
    <keywords>Human Computer Interaction, Musical Controllers, Electromyogram, Position Sensing, Sensor Instruments  </keywords>
  </document>
  <document>
    <name>nime2002_177.pdf</name>
    <abstract>Active force-feedback holds the potential for precise andrapid controls. A high performance device can be builtfrom a surplus disk drive and controlled from an inexpensive microcontroller. Our new design,The Plank hasonly one axis of force-feedback with limited range ofmotion. It is being used to explore methods of feelingand directly manipulating sound waves and spectra suitable for live performance of computer music.</abstract>
    <keywords>Haptics, music controllers, scanned synthesis. </keywords>
  </document>
  <document>
    <name>nime2002_181.pdf</name>
    <abstract>Here we propose a novel musical controller which acquiresimaging data of the tongue with a two-dimensional medicalultrasound scanner. A computer vision algorithm extractsfrom the image a discrete tongue shape to control, in realtime, a musical synthesizer and musical effects. We evaluate the mapping space between tongue shape and controllerparameters and its expressive characteristics.</abstract>
    <keywords>Tongue model, ultrasound, real-time, music synthesis, speech interface </keywords>
  </document>
  <document>
    <name>nime2002_186.pdf</name>
    <abstract>The Beatbugs are hand-held percussive instruments that allow the creation, manipulation, and sharing of rhythmic motifs through a simple interface. When multiple Beatbugs are connected in a network, players can form large-scale collaborative compositions by interdependently sharing and developing each other's motifs. Each Beatbug player can enter a motif that is then sent through a stochastic computerized "Nerve Center" to other players in the network. Receiving players can decide whether to develop the motif further (by continuously manipulating pitch, timbre, and rhythmic elements using two bend sensor antennae) or to keep it in their personal instrument (by entering and sending their own new motifs to the group.) The tension between the system's stochastic routing scheme and the players' improvised real-time decisions leads to an interdependent, dynamic, and constantly evolving musical experience. A musical composition entitled "Nerve" was written for the system by author Gil Weinberg. It was premiered on February 2002 as part of Tod Machover's Toy Symphony [1] in a concert with the Deutsches Symphonie Orchester Berlin, conducted by Kent Nagano. The paper concludes with a short evaluative discussion of the concert and the weeklong workshops that led to it. </abstract>
    <keywords>Interdependent Musical Networks, group playing, per- cussive controllers.  </keywords>
  </document>
  <document>
    <name>nime2002_192.pdf</name>
    <abstract>In this demonstration we will show a variety of computer-based musical instruments designed for live performance. Our design criteria include initial ease of use coupled with a long term potential for virtuosity, minimal and low variance latency, and clear and simple strategies for programming the relationship between gesture and musical result. We present custom controllers and unique adaptations of standard gestural interfaces, a programmable connectivity processor, a communications protocol called Open Sound Control (OSC), and a variety of metaphors for musical control. </abstract>
    <keywords>Expressive control, mapping gestures to acoustic results, metaphors for musical control, Tactex, Buchla Thunder, digitizing tablets.  </keywords>
  </document>
  <document>
    <name>nime2002_195.pdf</name>
    <abstract>The Mutha Rubboard is a musical controller based on therubboard, washboard or frottoir metaphor commonly usedin the Zydeco music genre of South Louisiana. It is not onlya metamorphosis of a traditional instrument, but a modernbridge of exploration into a rich musical heritage. It usescapacitive and piezo sensing technology to output MIDI andraw audio data.This new controller reads the key placement in two parallelplanes by using radio capacitive sensing circuitry expanding greatly on the standard corrugated metal playing surface. The percussive output normally associated with therubboard is captured through piezo contact sensors mounteddirectly on the keys (the playing implements). Additionally,mode functionality is controlled by discrete switching onthe keys.This new instrument is meant to be easily played by bothexperienced players and those new to the rubboard. It lendsitself to an expressive freedom by placing the control surface on the chest and allowing the hands to move uninhibited about it or by playing it in the usual way, preserving itsmusical heritage.</abstract>
    <keywords>MIDI controllers, computer music, Zydeco music, interac- tive music, electronic musical instrument, human computer interface, Louisiana heritage, physical modeling, bowl res- onators. </keywords>
  </document>
  <document>
    <name>nime2002_199.pdf</name>
    <abstract>Falling Up is an evening-length performance incorporatingdance and theatre with movement-controlled audio/videoplayback and processing. The solo show is a collaboration between Cindy Cummings (performance) and Todd Winkler(sound, video), first performed at the Dublin Fringe Festival,2001. Each thematic section of the work shows a different typeof interactive relationship between movement, video andsound. This demonstration explains the various technical configurations and aesthetic thinking behind aspects of the work.</abstract>
    <keywords>Dance, Video processing, Movement sensor, VNS, Very Nervous System </keywords>
  </document>
  <document>
    <name>nime2002_201.pdf</name>
    <keywords>Hyperbow, Hyperviolin, Hyperinstrument, violin, bow, po- sition sensor, accelerometer, strain sensor </keywords>
  </document>
  <document>
    <name>nime2003_003.pdf</name>
    <abstract>In this paper we present a design for the EpipE, a newexpressive electronic music controller based on the IrishUilleann Pipes, a 7-note polyphonic reeded woodwind. Thecore of this proposed controller design is a continuouselectronic tonehole-sensing arrangement, equally applicableto other woodwind interfaces like those of the flute, recorder orJapanese shakuhachi. The controller will initially be used todrive a physically-based synthesis model, with the eventualgoal being the development of a mapping layer allowing theEpipE interface to operate as a MIDI-like controller of arbitrarysynthesis models.</abstract>
    <keywords>Controllers, continuous woodwind tonehole sensor, uilleann pipes, Irish bagpipe, physical modelling, double reed, conical bore, tonehole. </keywords>
  </document>
  <document>
    <name>nime2003_015.pdf</name>
    <keywords>MIDI Controller, Wind Controller, Breath Control, Human Computer Interaction. </keywords>
  </document>
  <document>
    <name>nime2003_019.pdf</name>
    <abstract>The STRIMIDILATOR is an instrument that uses the deviation and the vibration of strings as MIDI-controllers. Thismethod of control gives the user direct tactile force feedbackand allows for subtle control. The development of the instrument and its different functions are described.</abstract>
    <keywords>MIDI controllers, tactile force feedback, strings. Figure The STRIMIDILATOR </keywords>
  </document>
  <document>
    <name>nime2003_024.pdf</name>
    <abstract>Over the past year the instructors of the Human ComputerInteraction courses at CCRMA have undertaken a technology shift to a much more powerful teaching platform. Wedescribe the technical features of the new Atmel AVR basedplatform, contrasting it with the Parallax BASIC Stampplatform used in the past. The successes and failures ofthe new platform are considered, and some student projectsuccess stories described.</abstract>
  </document>
  <document>
    <name>nime2003_030.pdf</name>
    <abstract>The Disc Jockey (DJ) software system Mixxx is presented.Mixxx makes it possible to conduct studies of new interaction techniques in connection with the DJ situation, by itsopen design and easy integration of new software modulesand MIDI connection to external controllers. To gain a better understanding of working practices, and to aid the designprocess of new interfaces, interviews with two contemporarymusicians and DJ's are presented. In contact with thesemusicians development of several novel prototypes for DJinteraction have been made. Finally implementation detailsof Mixxx are described.</abstract>
  </document>
  <document>
    <name>nime2003_036.pdf</name>
  </document>
  <document>
    <name>nime2003_054.pdf</name>
    <abstract>In this paper, we examine the use of spatial layouts of musicalmaterial for live performance control. Emphasis is given tosoftware tools that provide for the simple and intuitivegeometric organization of sound material, sound processingparameters, and higher-level musical structures.</abstract>
  </document>
  <document>
    <name>nime2003_070.pdf</name>
    <abstract>This paper first introduces two previous software-based musicinstruments designed by the author, and analyses the crucialimportance of the visual feedback introduced by theirinterfaces. A quick taxonomy and analysis of the visualcomponents in current trends of interactive music software isthen proposed, before introducing the reacTable*, a newproject that is currently under development. The reacTable* isa collaborative music instrument, aimed both at novices andadvanced musicians, which employs computer vision andtangible interfaces technologies, and pushes further the visualfeedback interface ideas and techniques aforementioned.</abstract>
  </document>
  <document>
    <name>nime2003_077.pdf</name>
    <abstract>A handheld electronic musical instrument, named the BentoBox, was developed. The motivation was to develop aninstrument which one can easily carry around and play inmoments of free time, for example when riding publictransportation or during short breaks at work. The device wasdesigned to enable quick learning by having various scalesprogrammed for different styles of music, and also beexpressive by having hand controlled timbral effects whichcan be manipulated while playing. Design analysis anditeration lead to a compact and ergonomic device. This paperfocuses on the ergonomic design process of the hardware.</abstract>
    <keywords>MIDI controller, electronic musical instrument, musical instrument design, ergonomics, playability, human computer interface. </keywords>
  </document>
  <document>
    <name>nime2003_083.pdf</name>
    <keywords>Chemical music, Applied chemistry, Battery Controller. </keywords>
  </document>
  <document>
    <name>nime2003_087.pdf</name>
    <abstract>This report details work on the interdisciplinary mediaproject TGarden. The authors discuss the challengesencountered while developing a responsive musicalenvironment for the general public involving wearable,sensor-integrated clothing as the central interface and inputd e v i c e . T h e p r o j e c t ' s d r a m a t u r g i c a l andtechnical/implementation background are detailed toprovide a framework for the creation of a responsive hardwareand software system that reinforces a tangible relationshipbetween the participant's improvised movement and musicalresponse. Finally, the authors take into consideration testingscenarios gathered from public prototypes in two Europeanlocales in 2001 to evaluate user experience of the system.</abstract>
    <keywords>Gesture, interaction, embodied action, enaction, physical model, responsive environment, interactive musical systems, affordance, interface, phenomenology, energy, kinetics, time constant, induced ballistics, wearable computing, accelerometer, audience participation, dynamical system, dynamic compliance, effort, wearable instrument, augmented physicality. </keywords>
  </document>
  <document>
    <name>nime2003_091.pdf</name>
    <abstract>We present a sensor-doll interface as a musical outlet forpersonal expression. A doll serves the dual role of being bothan expressive agent and a playmate by allowing solo andaccompanied performance. An internal computer and sensorsystem allow the doll to receive input from the user and itssurroundings, and then respond accordingly with musicalfeedback. Sets of musical timbres and melodies may bechanged by presenting the doll with a series of themed clothhats, each suggesting a different style of play. The doll mayperform by itself and play a number of melodies, or it maycollaborate with the user when its limbs are squeezed or bent.Shared play is further encouraged by a basic set of aural tonesmimicking conversation.</abstract>
    <keywords>Musical improvisation, toy interface agent, sensor doll, context awareness. </keywords>
  </document>
  <document>
    <name>nime2003_095.pdf</name>
  </document>
  <document>
    <name>nime2003_109.pdf</name>
    <abstract>In the project Sonic City, we have developed a system thatenables users to create electronic music in real time by walkingthrough and interacting with the urban environment. Weexplore the use of public space and everyday behaviours forcreative purposes, in particular the city as an interface andmobility as an interaction model for electronic music making.A multi-disciplinary design process resulted in theimplementation of a wearable, context-aware prototype. Thesystem produces music by retrieving information aboutcontext and user action and mapping it to real-time processingof urban sounds. Potentials, constraints, and implications ofthis type of music creation are discussed.</abstract>
  </document>
  <document>
    <name>nime2003_116.pdf</name>
    <abstract>The role of the face and mouth in speech production as well asnon-verbal communication suggests the use of facial action tocontrol musical sound. Here we document work on theMouthesizer, a system which uses a headworn miniaturecamera and computer vision algorithm to extract shapeparameters from the mouth opening and output these as MIDIcontrol changes. We report our experience with variousgesture-to-sound mappings and musical applications, anddescribe a live performance which used the Mouthesizerinterface.</abstract>
    <keywords>Video-based interface; mouth controller; alternative input devices. </keywords>
  </document>
  <document>
    <name>nime2003_122.pdf</name>
    <keywords>Alternate controller, gesture, microphone technique, vocal performance, performance interface, electronic music. </keywords>
  </document>
  <document>
    <name>nime2003_129.pdf</name>
    <abstract>We explore a variety of design criteria applicable to thecreation of collaborative interfaces for musical experience. Themain factor common to the design of most collaborativeinterfaces for novices is that musical control is highlyrestricted, which makes it possible to easily learn andparticipate in the collective experience. Balancing this tradeoff is a key concern for designers, as this happens at theexpense of providing an upward path to virtuosity with theinterface. We attempt to identify design considerationsexemplified by a sampling of recent collaborative devicesprimarily oriented toward novice interplay. It is our intentionto provide a non-technical overview of design issues inherentin configuring multiplayer experiences, particularly for entrylevel players.</abstract>
    <keywords>Design, collaborative interface, musical experience, multiplayer, novice, musical control. </keywords>
  </document>
  <document>
    <name>nime2003_135.pdf</name>
    <abstract>MidiGrid is a computer-based musical instrument, primarilycontrolled with the computer mouse, which allows liveperformance of MIDI-based musical material by mapping 2dimensional position onto musical events. Since itsinvention in 1987, it has gained a small, but enthusiastic,band of users, and has become the primary instrument forseveral people with physical disabilities. This paper reviewsits development, uses and user interface issues, and highlightsthe work currently in progress for its transformation intoMediaGrid.</abstract>
  </document>
  <document>
    <name>nime2003_140.pdf</name>
    <abstract>This paper presents a study of bimanual control applied tosound synthesis. This study deals with coordination,cooperation, and abilities of our hands in musical context. Wedescribe examples of instruments made using subtractivesynthesis, scanned synthesis in Max/MSP and commercialstand-alone software synthesizers via MIDI communicationprotocol. These instruments have been designed according to amulti-layer-mapping model, which provides modular design.They have been used in concerts and performanceconsiderations are discussed too.</abstract>
    <keywords>Gesture control, mapping, alternate controllers, musical instruments. </keywords>
  </document>
  <document>
    <name>nime2003_146.pdf</name>
    <abstract>This paper describes the implementation of Time Delay NeuralNetworks (TDNN) to recognize gestures from video images.Video sources are used because they are non-invasive and do notinhibit performer's physical movement or require specialistdevices to be attached to the performer which experience hasshown to be a significant problem that impacts musiciansperformance and can focus musical rehearsals and performancesupon technical rather than musical concerns (Myatt 2003).We describe a set of hand gestures learned by an artificial neuralnetwork to control musical parameters expressively in real time.The set is made up of different types of gestures in order toinvestigate:-aspects of the recognition process-expressive musical control-schemes of parameter mapping-generalization issues for an extended set for musicalcontrolThe learning procedure of the Neural Network is describedwhich is based on variations by affine transformations of imagesequences of the hand gestures.The whole application including the gesture capturing isimplemented in jMax to achieve real time conditions and easyintegration into a musical environment to realize differentmappings and routings of the control stream.The system represents a practice-based research using actualmusic models like compositions and processes of compositionwhich will follow the work described in the paper.</abstract>
    <keywords>Gesture Recognition, Artificial Neural Network, Expressive Control, Real-time Interaction </keywords>
  </document>
  <document>
    <name>nime2003_151.pdf</name>
    <abstract>This paper describes the artistic projects undertaken at ImmersionMusic, Inc. (www.immersionmusic.org) during its three-yearexistence. We detail work in interactive performance systems,computer-based training systems, and concert production.</abstract>
    <keywords>Interactive computer music systems, gestural interaction, Conductor's Jacket, Digital Baton  </keywords>
  </document>
  <document>
    <name>nime2003_161.pdf</name>
    <keywords>Motion capture, gestural control, mapping. </keywords>
  </document>
  <document>
    <name>nime2003_164.pdf</name>
    <abstract>In this paper, we discuss a design principle for the musicalinstruments that are useful for both novices and professionalmusicians and that facilitate musically rich expression. Webelieve that the versatility of conventional musicalinstruments causes difficulty in performance. By dynamicallyspecializing a musical instrument for performing a specific(genre of) piece, the musical instrument could become moreuseful for performing the piece and facilitates expressiveperformance. Based on this idea, we developed two new typesof musical instruments, i.e., a "given-melody-based musicalinstrument" and a "harmonic-function-based musicalinstrument." From the experimental results using twoprototypes, we demonstrate the efficiency of the designprinciple.</abstract>
  </document>
  <document>
    <name>nime2003_170.pdf</name>
    <abstract>In this paper, we introduce Block Jam, a Tangible UserInterface that controls a dynamic polyrhythmic sequencerusing 26 physical artifacts. These physical artifacts, that wecall blocks, are a new type of input device for manipulatingan interactive music system. The blocks' functional andtopological statuses are tightly coupled to an ad hocsequencer, interpreting the user's arrangement of the blocksas meaningful musical phrases and structures.We demonstrate that we have created both a tangible andvisual language that enables both the novice and musicallytrained users by taking advantage of both their explorativeand intuitive abilities. The tangible nature of the blocks andthe intuitive interface promotes face-to-face collaborationand social interaction within a single system. The principleof collaboration is further extended by linking two BlockJam systems together to create a network.We discuss our project vision, design rational, relatedworks, and the implementation of Block Jam prototypes.Figure 1. A cluster of blocks, note the mother block on thebottom right</abstract>
    <keywords>Tangible interface, modular system, polyrhythmic sequencer. VISION We believe in a future where music will no longer be considered a linear composition, but a dynamic structure, and musical composition will extend to interaction. We also believe that through the </keywords>
  </document>
  <document>
    <name>nime2003_180.pdf</name>
    <abstract>In this paper I present the gluiph, a single-board computer thatwas conceived as a platform for integrated electronic musicalinstruments. It aims to provide new instruments as well asexisting ones with a stronger identity by untethering themfrom the often lab-like stage setups built around general purpose computers. The key additions to its core are a flexiblesensor subsystem and multi-channel audio I/O. In contrast toother stand-alone approaches it retains a higher degree offlexibility by supporting popular music programming languages, with Miller Puckette's pd [1] being the current focus.</abstract>
  </document>
  <document>
    <name>nime2003_184.pdf</name>
    <abstract>In this paper, we describe a new interface for musicalperformance, using the interaction with a graphical userinterface in a powerful manner: the user directly touches ascreen where graphical objects are displayed and can useseveral fingers simultaneously to interact with the objects. Theconcept of this interface is based on the superposition of thegesture spatial place and the visual feedback spatial place; i tgives the impression that the graphical objects are real. Thisconcept enables a huge freedom in designing interfaces. Thegesture device we have created gives the position of fourfingertips using 3D sensors and the data is performed in theMax/MSP environment. We have realized two practicalexamples of musical use of such a device, using PhotosonicSynthesis and Scanned Synthesis.</abstract>
    <keywords>HCI, touch screen, multimodality, mapping, direct interaction, gesture devices, bimanual interaction, two-handed, Max/MSP. </keywords>
  </document>
  <document>
    <name>nime2003_201.pdf</name>
    <abstract>This paper suggests that there is a need for formalizing acomponent model of gestural primitive throughput in musicinstrument design. The purpose of this model is to construct acoherent and meaningful interaction between performer andinstrument. Such a model has been implicit in previous researchfor interactive performance systems. The model presented heredistinguishes gestural primitives from units of measure ofgestures. The throughput model identifies symmetry betweenperformance gestures and musical gestures, and indicates a rolefor gestural primitives when a performer navigates regions ofstable oscillations in a musical instrument. The use of a highdimensional interface tool is proposed for instrument design, forfine-tuning the mapping between movement sensor data andsound synthesis control data.</abstract>
    <keywords>Performance gestures, musical gestures, instrument design, mapping, tuning, affordances, stability. </keywords>
  </document>
  <document>
    <name>nime2003_208.pdf</name>
    <abstract>SensorBox is a low cost, low latency, high-resolutioninterface for obtaining gestural data from sensors for use inrealtime with a computer-based interactive system. Wediscuss its implementation, benefits, current limitations, andcompare it with several popular interfaces for gestural dataacquisition.</abstract>
    <keywords>Sensors, gestural acquisition, audio interface, interactive music, SensorBox. </keywords>
  </document>
  <document>
    <name>nime2003_211.pdf</name>
    <abstract>This software tool, developed in Max/MSP, presentsperformers with image files consisting of traditional notationas well as conducting in the form of video playback. Theimpetus for this work was the desire to allow the musicalmaterial for each performer of a given piece to differ withregard to content and tempo.</abstract>
    <keywords>Open form, notation, polymeter, polytempi, Max/MSP. </keywords>
  </document>
  <document>
    <name>nime2003_213.pdf</name>
    <abstract>This document describes modular software supporting livesignal processing and sound file playback within theMax/MSP environment. Dsp.rack integrates signalprocessing, memory buffer recording, and pre-recordedmulti-channel file playback using an interconnected,programmable signal flow matrix, and an eight-channel i/oformat.</abstract>
    <keywords>Digital signal processing, Max/MSP, computer music performance, matrix routing, live performance processing. </keywords>
  </document>
  <document>
    <name>nime2003_216.pdf</name>
    <abstract>This paper is a demo proposal for a new musical interfacebased on a DNA-like double-helix and concepts in charactergeneration. It contains a description of the interface,motivations behind developing such an interface, variousmappings of the interface to musical applications, and therequirements to demo the interface.</abstract>
    <keywords>Performance, Design, Experimentation, DNA, Big Five. </keywords>
  </document>
  <document>
    <name>nime2003_218.pdf</name>
    <abstract>This paper describes a system which uses the output fromhead-tracking and gesture recognition software to drive aparameterized guitar effects synthesizer in real-time.</abstract>
    <keywords>Head-tracking, gestural control, continuous control, parameterized effects processor. </keywords>
  </document>
  <document>
    <name>nime2003_222.pdf</name>
    <abstract>Sodaconductor is a musical interface for generating OSCcontrol data based on the dynamic physical simulation toolSodaconstructor as it can be seen and heard onhttp://www.sodaplay.com.</abstract>
  </document>
  <document>
    <name>nime2003_225.pdf</name>
    <abstract>Ircam has been deeply involved into gesture analysis and sensingfor about four years now, as several artistic projects demonstrate.Ircam has often been solicited for sharing software and hardwaretools for gesture sensing, especially devices for the acquisition andconversion of sensor data, such as the AtoMIC Pro [1][2]. Thisdemo-paper describes the recent design of a new sensor to MIDIinterface called EoBody1</abstract>
    <keywords>Gestural controller, Sensor, MIDI, Computer Music. </keywords>
  </document>
  <document>
    <name>nime2004_001.pdf</name>
    <abstract>The Tooka was created as an exploration of two personinstruments. We have worked with two Tooka performers toenhance the original experimental device to make a musicalinstrument played and enjoyed by them. The main additions tothe device include: an additional button that behaves as amusic capture button, a bend sensor, an additional thumbactuated pressure sensor for vibrato, additional musicalmapping strategies, and new interfacing hardware. Thesedevelopments a rose through exper iences andrecommendations from the musicians playing it. In addition tothe changes to the Tooka, this paper describes the learningprocess and experiences of the musicians performing with theTooka.</abstract>
    <keywords>Musician-centred design, two-person musical instrument. </keywords>
  </document>
  <document>
    <name>nime2004_007.pdf</name>
    <abstract>This paper describes the design of an Electronic Sitar controller, adigitally modified version of Saraswati's (the Hindu Goddess ofMusic) 19-stringed, pumpkin shelled, traditional North Indianinstrument. The ESitar uses sensor technology to extract gesturalinformation from a performer, deducing music information suchas pitch, pluck timing, thumb pressure, and 3-axes of head tilt totrigger real-time sounds and graphics. It allows for a variety oftraditional sitar technique as well as new performance methods.Graphical feedback allows for artistic display and pedagogicalfeedback. The ESitar uses a programmable Atmel microprocessorwhich outputs control messages via a standard MIDI jack.</abstract>
  </document>
  <document>
    <name>nime2004_013.pdf</name>
    <keywords>Sound feedback, Karate, Learning environment, Wearable device </keywords>
  </document>
  <document>
    <name>nime2004_019.pdf</name>
    <abstract>This article reflects the current state of the reacTable* project,an electronic music instrument with a tangible table-basedinterface, which is currently under development at theAudiovisual Institute at the Universitat Pompeu Fabra. In thispaper we are focussing on the issue of Dynamic Patching,which is a particular and unique aspect of the sound synthesisand control paradigms of the reacTable*. Unlike commonvisual programming languages for sound synthesis, whichconceptually separate the patch building process from theactual musical performance, the reacTable* combines theconstruction and playing of the instrument in a unique way.The tangible interface allows direct manipulation control overany of the used building blocks, which physically representthe whole synthesizer function.</abstract>
  </document>
  <document>
    <name>nime2004_023.pdf</name>
  </document>
  <document>
    <name>nime2004_027.pdf</name>
  </document>
  <document>
    <name>nime2004_031.pdf</name>
    <abstract>This paper presents a project involving a percussionist playing on a virtual percussion. Both artistic and technical aspects of the project are developed. Especially, a method forstrike recognition using the Flock of Birds is presented, aswell as its use for artistic purpose.</abstract>
    <keywords>Gesture analysis, virtual percussion, strike recognition. </keywords>
  </document>
  <document>
    <name>nime2004_039.pdf</name>
    <abstract>In this paper, we describe an adaptive approach to gesture mapping for musical applications which serves as a mapping system for music instrument design. A neural network approach is chosen for this goal and all the required interfaces and abstractions are developed and demonstrated in the Pure Data environment. In this paper, we will focus on neural network representation and implementation in a real-time musical environment. This adaptive mapping is evaluated in different static and dynamic situations by a network of sensors sampled at a rate of 200Hz in real-time. Finally, some remarks are given on the network design and future works. </abstract>
    <keywords>Real-time gesture control, adaptive interfaces, Sensor and actuator technologies for musical applications, Musical mapping algorithms and intelligent controllers, Pure Data.  </keywords>
  </document>
  <document>
    <name>nime2004_047.pdf</name>
    <abstract>This paper describes the use of evolutionary and artificial life techniques in sound design and the development of performance mapping to facilitate the real-time manipulation of such sounds through some input device controlled by the performer. A concrete example of such a system is described which allows musicians without detailed knowledge and experience of sound synthesis techniques to interactively develop new sounds and performance manipulation mappings according to their own aesthetic judgements. Experiences with the system are discussed. </abstract>
  </document>
  <document>
    <name>nime2004_051.pdf</name>
    <abstract>In this report, we discuss Tree Music, an interactive computermusic installation created using GAIA (Graphical Audio InterfaceApplication), a new open-source interface for controlling theRTcmix synthesis and effects processing engine. Tree Music,commissioned by the University of Virginia Art Museum, used awireless camera with a wide-angle lens to capture motion andocclusion data from exhibit visitors. We show how GAIA wasused to structure and navigate the compositional space, and howthis program supports both graphical and text-based programmingin the same application. GAIA provides a GUI which combinestwo open-source applications: RTcmix and Perl.</abstract>
    <keywords>Composition, new interfaces, interactive systems, open source, Real time audio, GUI controllers, video tracking </keywords>
  </document>
  <document>
    <name>nime2004_055.pdf</name>
    <abstract>This essay outlines a framework for understanding newmusical compositions and performances that utilizepre-existing sound recordings. In attempting toarticulate why musicians are increasingly using soundrecordings in their creative work, the author calls fornew performance tools that enable the dynamic use ofpre-recorded music. </abstract>
    <keywords>Call and response, turntablism, DJ tools, oral culture </keywords>
  </document>
  <document>
    <name>nime2004_059.pdf</name>
    <abstract>When envisaging new digital instruments, designers do not have to limit themselves to their sonic capabilities (which can be absolutely any), not even to their algorithmic power; they must be also especially careful about the instruments' conceptual capabilities, to the ways instruments impose or suggest to their players new ways of thinking, new ways of establishing relations, new ways of interacting, new ways of organizing time and textures; new ways, in short, of playing new musics. This article explores the dynamic relation that builds between the player and the instrument, introducing concepts such as efficiency, apprenticeship and learning curve It aims at constructing a framework in which the possibilities and the diversity of music instruments as well as the possibilities and the expressive freedom of human music performers could start being evaluated. </abstract>
    <keywords>Musical instruments design, learning curve, apprenticeship, musical efficiency.  </keywords>
  </document>
  <document>
    <name>nime2004_064.pdf</name>
    <abstract>This report presents a novel interface for musical performance which utilizes a record-player turntable augmented with a computation engine and a high-density optical sensing array. The turntable functions as a standalone step sequencer for MIDI events transmitted to a computer or another device and it is programmed in real-time using visual disks. The program instructions are represented on printed paper disks directly as characters of English alphabet that could be read by human as effectively as they are picked up by the machine's optical cartridge. The result is a tangible interface that allows the user to manipulate pre-arranged musical material by hand, by adding together instrumental tracks to form a dynamic mix. A functional implementation of this interface is discussed in view of historical background and other examples of electronic instruments for music creation and performance incorporating optical turntable as a central element.</abstract>
    <keywords>Interaction, visualization, tangible interface, controllers, optical turntable, performance. </keywords>
  </document>
  <document>
    <name>nime2004_068.pdf</name>
    <abstract>This paper describes the first system designed to allow children to conduct an audio and video recording of an orchestra. No prior music experience is required to control theorchestra, and the system uses an advanced algorithm totime stretch the audio in real-time at high quality and without altering the pitch. We will discuss the requirements andchallenges of designing an interface to target our particularuser group (children), followed by some system implementation details. An overview of the algorithm used for audiotime stretching will also be presented. We are currently using this technology to study and compare professional andnon-professional conducting behavior, and its implicationswhen designing new interfaces for multimedia. You're theConductor is currently a successful exhibit at the Children'sMuseum in Boston, USA.</abstract>
  </document>
  <document>
    <name>nime2004_074.pdf</name>
    <abstract>The PebbleBox and the CrumbleBag are examples of a granular interaction paradigm, in which the manipulation ofphysical grains of arbitrary material becomes the basis forinteracting with granular sound synthesis models. The soundsmade by the grains as they are manipulated are analysed,and parameters such as grain rate, grain amplitude andgrain density are extracted. These parameters are then usedto control the granulation of arbitrary sound samples in realtime. In this way, a direct link is made between the haptic sensation of interacting with grains and the control ofgranular sounds.</abstract>
    <keywords>Musical instrument, granular synthesis, haptic </keywords>
  </document>
  <document>
    <name>nime2004_080.pdf</name>
  </document>
  <document>
    <name>nime2004_087.pdf</name>
    <abstract>The choice of mapping strategies to effectively map controller variables to sound synthesis algorithms is examined.Specifically, we look at continuous mappings that have ageometric representation. Drawing from underlying mathematical theory, this paper presents a way to compare mapping strategies, with the goal of achieving an appropriatematch between mapping and musical performance context.This method of comparison is applied to existing techniques,while a suggestion is offered on how to integrate and extendthis work through a new implementation.</abstract>
    <keywords>Mapping, Interface Design, Interpolation, Computational Geometry </keywords>
  </document>
  <document>
    <name>nime2004_092.pdf</name>
    <abstract>This paper discusses some of the issues pertaining to thedesign of digital musical instruments that are to effectively fillthe role of traditional instruments (i.e. those based on physicalsound production mechanisms). The design andimplementation of a musical instrument that addresses some ofthese issues, using scanned synthesis coupled to a "smart"physical system, is described.</abstract>
    <keywords>Digital musical instruments, real-time performance, scanned synthesis, pd, tactile interfaces, sensors, Shapetape, mapping. </keywords>
  </document>
  <document>
    <name>nime2004_096.pdf</name>
    <abstract>This paper describes an approach to match visual and acoustic parameters to produce an animated musical expression.Music may be generated to correspond to animation, asdescribed here; imagery may be created to correspond tomusic; or both may be developed simultaneously. This approach is intended to provide new tools to facilitate bothcollaboration between visual artists and musicians and examination of perceptual issues between visual and acousticmedia. As a proof-of-concept, a complete example is developed with linear fractals as a basis for the animation, andarranged rhythmic loops for the music. Since both visualand acoustic elements in the example are generated fromconcise specifications, the potential of this approach to create new works through parameter space exploration is accentuated, however, there are opportunities for applicationto a wide variety of source material. These additional applications are also discussed, along with issues encounteredin development of the example.</abstract>
    <keywords>Multimedia creation and interaction, parameter space, visu- alization, sonification. </keywords>
  </document>
  <document>
    <name>nime2004_100.pdf</name>
    <keywords>Interactive Music Systems, Networking and Control, Voice and Speech Analysis, Auracle, JSyn, TransJam, Linear Pre- diction, Neural Networks, Voice Interface, Open Sound Con- trol </keywords>
  </document>
  <document>
    <name>nime2004_104.pdf</name>
    <abstract>In this paper, we propose Thermoscore, a musical score form-that dynamically alters the temperature of the instrument/player interface. We developed the first version of theThermoscore display by lining Peltier devices on piano keys.The system is controlled by MIDI notes-on messages from anMIDI sequencer, so that a composer can design songs that aresequences of temperature for each piano key. We also discussmethodologies for composing with this system, and suggesttwo approaches. The first is to make desirable keys (or otherkeys) hot. The second one uses chroma-profile, that is, a radarchart representation of the frequency of pitch notations in the-piece. By making keys of the same chroma hot in reverse proportion to the value of the chroma-profile, it is possible to-constrain the performer's improvisation and to bring the tonality space close to a certain piece.</abstract>
    <keywords>musical score, improvisation, peltier device, chroma profile </keywords>
  </document>
  <document>
    <name>nime2004_112.pdf</name>
    <abstract>This paper describes ThumbTEC, a novel general purposeinput device for the thumb or finger that is useful in a widevariety of applications from music to text entry. The device i smade up of three switches in a row and one miniature joystickon top of the middle switch. The combination of joystickdirection and switch(es) controls what note or alphanumericcharacter is selected by the finger. Several applications aredetailed.</abstract>
    <keywords>One-Thumb Input Device, HCI, Isometric Joystick, Mobile Computing, Handheld Devices, Musical Instrument. </keywords>
  </document>
  <document>
    <name>nime2004_120.pdf</name>
    <keywords>Rencon, Turing Test, Musical Expression, Performance Ren- dering </keywords>
  </document>
  <document>
    <name>nime2004_124.pdf</name>
    <abstract>This paper describes an approach for playing expressivemusic, as it refers to a pianist's expressiveness, with atapping-style interface. MIDI-formatted expressiveperformances played by pianists were first analyzed andtransformed into performance templates, in which thedeviations from a canonical description was separatelydescribed for each event. Using one of the templates as askill complement, a player can play music expressivelyover and under the beat level. This paper presents ascheduler that allows a player to mix her/his own intensionand the expressiveness in the performance template. Theresults of a forty-subject user study suggest that using theexpression template contributes the subject's joy of playingmusic with the tapping-style performance interface. Thisresult is also supported by a brain activation study that wasdone using a near-infrared spectroscopy (NIRS).Categories and Subject DescriptorsH.5.5 [Information Interfaces and Presentation]: Sound andMusic Computing methodologies and techniques.</abstract>
    <keywords>Rencon, interfaces for musical expression, visualization </keywords>
  </document>
  <document>
    <name>nime2004_130.pdf</name>
    <abstract>A series of demonstrations of synthesized acappella songsbased on an auditory morphing using STRAIGHT [5] willbe presented. Singing voice data for morphing were extracted from the RWCmusic database of musical instrument sound. Discussions on a new extension of the morphing procedure to deal with vibrato will be introduced basedon the statistical analysis of the database and its effect onsynthesized acappella will also be demonstrated.</abstract>
    <keywords>Rencon, Acappella, RWCdatabase, STRAIGHT, morph- ing </keywords>
  </document>
  <document>
    <name>nime2004_138.pdf</name>
    <abstract>On-the-fly programming is a style of programming in which the programmer/performer/composer augments and modifies the program while it is running, without stopping or restarting, in order to assert expressive, programmable control at runtime. Because of the fundamental powers of programming languages, we believe the technical and aesthetic aspects of on-the-fly programming are worth exploring. In this paper, we present a formalized framework for on-the-fly programming, based on the ChucK synthesis language, which supports a truly concurrent audio programming model with sample-synchronous timing, and a highly on-the-fly style of programming. We first provide a well-defined notion of on-thefly programming. We then address four fundamental issues that confront the on-the-fly programmer: timing, modularity, conciseness, and flexibility. Using the features and properties of ChucK, we show how it solves many of these issues. In this new model, we show that (1) concurrency provides natural modularity for on-the-fly programming, (2) the timing mechanism in ChucK guarantees on-the-fly precision and consistency, (3) the Chuck syntax improves conciseness, and (4) the overall system is a useful framework for exploring on-the-fly programming. Finally, we discuss the aesthetics of on-the-fly performance. </abstract>
  </document>
  <document>
    <name>nime2004_144.pdf</name>
    <abstract>This paper describes the design of an expressive tangible interface for cinema editing as a live performance. A short survey of live video practices is provided. The Live Cinema instrument is a cross between a musical instrument and a film editing tool, tailored for improvisational control as well as performance presence. Design specifications for the instrument evolved based on several types of observations including: our own performances in which we used a prototype based on available tools; an analysis of performative aspects of contemporary DJ equipment; and an evaluation of organizational aspects of several generations of film editing tools. Our instrument presents the performer with a large canvas where projected images can be grabbed and moved around with both hands simultaneously; the performer also has access to two video drums featuring haptic display to manipulate the shots and cut between streams. The paper ends with a discussion of issues related to the tensions between narrative structure and hands-on control, live and recorded arts and the scoring of improvised films. </abstract>
    <keywords>live cinema, video controller, visual music, DJ, VJ, film editing, tactile interface, two-hand interaction, improvisation, performance, narrative structure.  </keywords>
  </document>
  <document>
    <name>nime2004_150.pdf</name>
    <abstract>A system is introduced that allows a string player to control asynthesis engine with the gestural skills he is used to. Theimplemented system is based on an electric viola and asynthesis engine that is directly controlled by the unanalysedaudio signal of the instrument and indirectly by controlparameters mapped to the synthesis engine. This method offersa highly string-specific playability, as it is sensitive to thekinds of musical articulation produced by traditional playingtechniques. Nuances of sound variation applied by the playerwill be present in the output signal even if those nuances arebeyond traditionally measurable parameters like pitch,amplitude or brightness. The relatively minimal hardwarerequirements make the instrument accessible with littleexpenditure.</abstract>
    <keywords>Electronic bowed string instrument, playability, musical instrument design, human computer interface, oscillation controlled sound synthesis </keywords>
  </document>
  <document>
    <name>nime2004_154.pdf</name>
    <abstract>We present a system for collaborative musical creation onmobile wireless networks. The work extends on simple peerto-peer file sharing systems towards ad-hoc mobility andstreaming. It extends upon music listening from a passiveact to a proactive, participative activity. The system consistsof a network based interactive music engine and a portablerendering player. It serves as a platform for experiments onstudying the sense of agency in collaborative creativeprocess, and requirements for fostering musical satisfactionin remote collaboration. </abstract>
  </document>
  <document>
    <name>nime2004_157.pdf</name>
    <abstract>This paper reports our recent developments on sensor acquisition systems, taking advantage of computer network technology. We present a versatile hardware system which can be connected to wireless modules, Analog to Digital Converters, and enables Ethernet communication. We are planning to make freely available the design of this architecture. We describe also several approaches we tested for wireless communication. Such technology developments are currently used in our newly formed Performance Arts Technology Group. </abstract>
    <keywords>Gesture, Sensors, Ethernet, 802.11, Computer Music.   </keywords>
  </document>
  <document>
    <name>nime2004_161.pdf</name>
    <abstract>Sonic City is a wearable system enabling the use of the urban environment as an interface for real-time electronic music making, when walking through and interacting with a city. The device senses everyday interactions and surrounding contexts, and maps this information in real time to the sound processing of urban sounds. We conducted a short-term study with various participants using our prototype in everyday settings. This paper describes the course of the study and preliminary results in terms of how the participants used and experienced the system. These results showed that the city was perceived as the main performer but that the user improvised different tactics and ad hoc interventions to actively influence and participate in how the music was created. </abstract>
    <keywords>User study, new interface for musical expression, interactive music, wearable computing, mobility, context-awareness.  </keywords>
  </document>
  <document>
    <name>nime2004_165.pdf</name>
    <abstract> This paper begins by evaluating various systems in terms of factors for building interactive audiovisual environments. The main issues for flexibility and expressiveness in the generation of dynamic sounds and images are then isolated. The design and development of an audiovisual system prototype is described at the end. </abstract>
    <keywords>Audiovisual, composition, performance, gesture, image, representation, mapping, expressiveness.  </keywords>
  </document>
  <document>
    <name>nime2004_169.pdf</name>
    <abstract>We describe a simple, computationally light, real-time system for tracking the lower face and extracting informationabout the shape of the open mouth from a video sequence.The system allows unencumbered control of audio synthesismodules by action of the mouth. We report work in progressto use the mouth controller to interact with a physical modelof sound production by the avian syrinx.</abstract>
    <keywords>Mouth Controller, Face Tracking, Bioacoustics </keywords>
  </document>
  <document>
    <name>nime2004_177.pdf</name>
    <keywords>Improvisation support, jam session, melody correction, N-gram model, melody modeling, musical instrument </keywords>
  </document>
  <document>
    <name>nime2004_181.pdf</name>
    <abstract>This paper describes new work and creations of LEMUR, agroup of artists and technologists creating robotic musicalinstruments.</abstract>
  </document>
  <document>
    <name>nime2004_185.pdf</name>
    <abstract>Though musical performers routinely use eye movements to communicate with each other during musical performances, very few performers or composers have used eye tracking devices to direct musical compositions and performances. EyeMusic is a system that uses eye movements as an input to electronic music compositions. The eye movements can directly control the music, or the music can respond to the eyes moving around a visual scene. EyeMusic is implemented so that any composer using established composition software can incorporate prerecorded eye movement data into their musical compositions.</abstract>
    <keywords>Electronic music composition, eye movements, eye tracking, human-computer interaction, Max/MSP. </keywords>
  </document>
  <document>
    <name>nime2004_189.pdf</name>
    <abstract>When working with sample-based media, a performer is managing timelines, loop points, sample parameters and effects parameters. The Slidepipe is a performance controller that gives the artist a visually simple way to work with their material. Its design is modular and lightweight, so it can be easily transported and quickly assembled. Also, its large stature magnifies the gestures associated with its play, providing a more convincing performance. In this paper, I will describe what the controller is, how this new controller interface has affected my live performance, and how it can be used in different performance scenarios. </abstract>
    <keywords>Controller, Sample Manipulation, Live Performance, Open Sound Control, Human Computer Interaction  </keywords>
  </document>
  <document>
    <name>nime2004_193.pdf</name>
    <abstract>This paper describes a theory for modulated objects based onobservations of recent musical interface design trends. Thetheory implies extensions to an object-based approach tocontroller design. Combining NIME research withethnographic study of shamanic traditions. The authordiscusses the creation of new controllers based on theshamanic use of ritual objects.</abstract>
    <keywords>Music and Video Controllers, New Interface Design, Music Composition, Multimedia, Mythology, Shamanism, Ecoacoustics </keywords>
  </document>
  <document>
    <name>nime2004_199.pdf</name>
    <abstract>The Epipe is a novel electronic woodwind controller with continuous tonehole coverage sensing, an initial design for which was introduced at NIME '03. Since then, we have successfully completed two fully operational prototypes. This short paper describes some of the issues encountered during the design and construction of this controller. It also details our own early experiences and impressions of the interface as well as its technical specifications. </abstract>
    <keywords>woodwind controller, variable tonehole control, MIDI, capacitive sensing  </keywords>
  </document>
  <document>
    <name>nime2004_201.pdf</name>
    <abstract>This paper describes the SillyTone Squish Factory, a haptically engaging musical interface. It contains the motivation behind the device's development, a description of the interface, various mappings of the interface to musical applications, details of its construction, and the requirements to demo the interface. </abstract>
  </document>
  <document>
    <name>nime2004_203.pdf</name>
    <abstract>StickMusic is an instrument comprised of two haptic devices, a joystick and a mouse, which control a phase vocoder in real time. The purpose is to experiment with ideas of how to apply haptic feedback when controlling synthesis algorithms that have no direct analogy to methods of generating sound in the physical world. </abstract>
    <keywords>haptic feedback, gestural control, performance, joystick, mouse  </keywords>
  </document>
  <document>
    <name>nime2004_205.pdf</name>
    <abstract>High capacity of transmission lines (Ethernet in particular) is much higher than what imposed by MIDI today. So it is possible to use capturing interfaces with high-speed and high-resolution, thanks to the OSC protocol, for musical synthesis (either in realtime or non real-time). These new interfaces offer many advantages, not only in the area of musical composition with use of sensors but also in live and interactive performances. In this manner, the processes of calibration and signal processing are delocalized on a personal computer and augments possibilities of processing. In this demo, we present two hardware interfaces developed in La kitchen with corresponding processing to achieve a high-resolution, high-speed sensor processing for musical applications. </abstract>
    <keywords>Interface, Sensors, Calibration, Precision, OSC, Pure Data, Max/MSP.  </keywords>
  </document>
  <document>
    <name>nime2004_207.pdf</name>
    <abstract>We will discuss the case study of application of the VirtualMusical Instrument and Sound Synthesis. Doing thisapplication, the main subject is advanced Mapping Interface inorder to connect these. For this experiment, our discussionalso refers to Neural Network, as well as a brief introduction ofthe Virtual Musical Instrument "Le SuperPolm" and GestureController "BodySuit".</abstract>
    <keywords>Virtual Musical Instrument, Gesture Controller, Mapping Interface </keywords>
  </document>
  <document>
    <name>nime2004_209.pdf</name>
    <abstract>In this paper, we describe a new MIDI controller, the LightPipes. The Light Pipes are a series of pipes that respond toincident light. The paper will discuss the design of theinstrument, and the prototype we built. A piece was composedfor the instrument using algorithms designed in Pure Data.</abstract>
    <keywords>Controllers, MIDI, light sensors, Pure Data. </keywords>
  </document>
  <document>
    <name>nime2004_211.pdf</name>
    <abstract>In this paper, I describe a realtime sampling system for theturntablist, and the hardware and software design of the secondprototype, 16padjoystickcontroller.</abstract>
    <keywords>DJ, Turntablism, Realtime Sampling, MAX/MSP, Microchip PIC microcontroller, MIDI </keywords>
  </document>
  <document>
    <name>nime2004_213.pdf</name>
    <abstract>This paper describes the design and on-going development ofan expressive gestural MIDI interface and how this couldenhance live performance of electronic music.</abstract>
    <keywords>gestural control, mapping, Pure Data (pd), accelerometers, MIDI, microcontrollers, synthesis, musical instruments </keywords>
  </document>
  <document>
    <name>nime2004_215.pdf</name>
    <abstract>This paper proposes an interface for improvisational ensemble plays which synthesizes musical sounds and graphical images on the floor from people's act of "walking." The aim of this paper is to develop such a system that enables nonprofessional people in our public spaces to play good contrapuntal music without any knowledge of music theory. The people are just walking. This system is based on the i-trace system [1] which can capture the people's behavior and give some visual feedback. </abstract>
    <keywords>Improvisational Ensemble Play, Contrapuntal Music, Human Tracking, Traces, Spatially Augmented Reality  </keywords>
  </document>
  <document>
    <name>nime2005_002.pdf</name>
  </document>
  <document>
    <name>nime2005_005.pdf</name>
    <keywords>Infra-instruments, hyperinstruments, meta-instruments, virtual instruments, design concepts and principles. </keywords>
  </document>
  <document>
    <name>nime2005_011.pdf</name>
    <abstract>In this paper, we introduce and analyze four gesture-controlled musical instruments. We briefly discuss the test platform designed to allow for rapid experimentation of new interfaces and control mappings. We describe our design experiences and discuss the effects of system features such as latency, resolution and lack of tactile feedback. The instruments use virtual reality hardware and computer vision for user input, and three-dimensional stereo vision as well as simple desktop displays for providing visual feedback. The instrument sounds are synthesized in real-time using physical sound modeling. </abstract>
  </document>
  <document>
    <name>nime2005_023.pdf</name>
    <abstract>In this paper we study the potential and the challenges posed by multi-user instruments, as tools that can facilitate interaction and responsiveness not only between performers and their instrument but also between performers as well. Several previous studies and taxonomies are mentioned, after what different paradigms exposed with examples based on traditional mechanical acoustic instruments. In the final part, several existing systems and implementations, now in the digital domain, are described and identified according to the models and paradigms previously introduced. </abstract>
    <keywords>Multi-user instruments, collaborative music, new instruments design guidelines.  </keywords>
  </document>
  <document>
    <name>nime2005_027.pdf</name>
    <abstract>This paper will investigate a variety of alternate controllers that are making an impact in interactive entertainment, particularly in the video game industry. Since the late 1990's, the surging popularity of rhythmic and musical performance games in Japanese arcades has led to the development of new interfaces and alternate controllers for the consumer market worldwide. Rhythm action games such as Dance Dance Revolution, Taiko No Tatsujin (Taiko: Drum Master), and Donkey Konga are stimulating collaborative gameplay and exposing consumers to custom controllers designed specifically for musical and physical interaction. We are witnessing the emergence and acceptance of these breakthrough controllers and models for gameplay as an international cultural phenomenon penetrating the video game and toy markets in record numbers. Therefore, it is worth considering the potential benefits to developers of musical interfaces, electronic devices and alternate controllers in light of these new and emerging opportunities, particularly in the realm of video gaming, toy development, arcades, and other interactive entertainment experiences. </abstract>
  </document>
  <document>
    <name>nime2005_038.pdf</name>
    <abstract>The Self-Contained Unified Bass Augmenter (SCUBA) is a new augmentative OSC (Open Sound Control) [5] controller for the tuba. SCUBA adds new expressive possibilities to the existing tuba interface through onboard sensors. These sensors provide continuous and discrete user-controlled parametric data to be mapped at will to signal processing parameters, virtual instrument control parameters, sound playback, and various other functions. In its current manifestation, control data is mapped to change the processing of the instrument's natural sound in Pd (Pure Data) [3]. SCUBA preserves the unity of the solo instrument interface by acoustically mixing direct and processed sound in the instrument's bell via mounted satellite speakers, which are driven by a subwoofer below the performer's chair. The end result augments the existing interface while preserving its original unity and functionality. </abstract>
    <keywords>Interactive music, electro-acoustic musical instruments, musical instrument design, human computer interface, signal processing, Open Sound Control (OSC)  </keywords>
  </document>
  <document>
    <name>nime2005_042.pdf</name>
    <abstract>This paper presents a novel controller built to exploit thephysical behaviour of a simple dynamical system, namely aspinning wheel. The phenomenon of gyroscopic precessioncauses the instrument to slowly oscillate when it is spunquickly, providing the performer with proprioceptive feedback. Also, due to the mass of the wheel and tire and theresulting rotational inertia, it maintains a relatively constant angular velocity once it is set in motion. Various sensors were used to measure continuous and discrete quantitiessuch as the the angular frequency of the wheel, its spatialorientation, and the performer's finger pressure. In addition, optical and hall-effect sensors detect the passing of aspoke-mounted photodiode and two magnets. A base software layer was developed in Max/MSP and various patcheswere written with the goal of mapping the dynamic behaviorof the wheel to varied musical processes.</abstract>
    <keywords>HCI, Digital Musical Instruments, Gyroscopic Precession, Rotational Inertia, Open Sound Control </keywords>
  </document>
  <document>
    <name>nime2005_046.pdf</name>
    <abstract>The Smart Controller is a portable hardware device that responds to input control voltage, OSC, and MIDI messages; producing output control voltage, OSC, and MIDI messages (depending upon the loaded custom patch). The Smart Controller is a stand alone device; a powerful, reliable, and compact instrument capable of reducing the number of electronic modules required in a live performance or installation, particularly the requirement of a laptop computer. More powerful, however, is the Smart Controller Workbench, a complete interactive development environment. In addition to enabling the composer to create and debug their patches, the Smart Controller Workbench accurately simulates the behaviour of the hardware, and functions as an incircuit debugger that enables the performer to remotely monitor, modify, and tune patches running in an installation without the requirement of stopping or interrupting the live performance. </abstract>
    <keywords>Control Voltage, Open Sound Control, Algorithmic Composition, MIDI, Sound Installations, programmable logic control, synthesizers, electronic music, Sensors, Actuators, Interaction.  </keywords>
  </document>
  <document>
    <name>nime2005_050.pdf</name>
    <abstract>This paper describes an installation created by LEMUR(League of Electronic Musical Urban Robots) in January, 2005.The installation included over 30 robotic musical instrumentsand a multi-projector real-time video projection and wascontrollable and programmable over a MIDI network. Theinstallation was also controllable remotely via the Internet andcould be heard and viewed via room mics and a robotic webcam connected to a streaming server.</abstract>
  </document>
  <document>
    <name>nime2005_060.pdf</name>
    <abstract>When learning a classical instrument, people often eithertake lessons in which an existing body of "technique" is delivered, evolved over generations of performers, or in somecases people will "teach themselves" by watching people playand listening to existing recordings. What does one do witha complex new digital instrument?In this paper I address this question drawing on my experience in learning several very different types of sophisticatedinstruments: the Glove Talk II real-time gesture-to-speechinterface, the Digital Marionette controller for virtual 3Dpuppets, and pianos and keyboards. As the primary userof the first two systems, I have spent hundreds of hourswith Digital Marionette and Glove-Talk II, and thousandsof hours with pianos and keyboards (I continue to work asa professional musician). I will identify some of the underlying principles and approaches that I have observed duringmy learning and playing experience common to these instruments. While typical accounts of users learning new interfaces generally focus on reporting beginner's experiences, forvarious practical reasons, this is fundamentally different byfocusing on the expert's learning experience.</abstract>
    <keywords>performance, learning new instruments </keywords>
  </document>
  <document>
    <name>nime2005_065.pdf</name>
    <keywords>Adaptive System, Sound Installation, Smart Interfaces, Music Robots, Spatial Music, Conscious Subconscious Interaction.  </keywords>
  </document>
  <document>
    <name>nime2005_080.pdf</name>
    <abstract>McBlare is a robotic bagpipe player developed by the Robotics Institute at Carnegie Mellon University. McBlare plays a standard set of bagpipes, using a custom air compressor to supply air and electromechanical "fingers" to control the chanter. McBlare is MIDI controlled, allowing for simple interfacing to a keyboard, computer, or hardware sequencer. The control mechanism exceeds the measured speed of expert human performers. On the other hand, human performers surpass McBlare in their ability to compensate for limitations and imperfections in reeds, and we discuss future enhancements to address these problems. McBlare has been used to perform traditional bagpipe music as well as experimental computer generated music. </abstract>
    <keywords>bagpipes, robot, music, instrument, MIDI  </keywords>
  </document>
  <document>
    <name>nime2005_085.pdf</name>
    <abstract>In this report, we describe our development on the Max/MSPtoolbox MnM dedicated to mapping between gesture andsound, and more generally to statistical and machine learningmethods. This library is built on top of the FTM library, whichenables the efficient use of matrices and other data structuresin Max/MSP. Mapping examples are described based onvarious matrix manipulations such as Single ValueDecomposition. The FTM and MnM libraries are freelyavailable.</abstract>
    <keywords>Mapping, interface design, matrix, Max/MSP. </keywords>
  </document>
  <document>
    <name>nime2005_089.pdf</name>
    <abstract>This paper describes DspMap, a graphical user interface (GUI)designed to assist the dynamic routing of signal generators andmodifiers currently being developed at the International Academyof Media Arts &amp; Sciences. Instead of relying on traditional boxand-line approaches, DspMap proposes a design paradigm whereconnections are determined by the relative positions of the variouselements in a single virtual space.</abstract>
    <keywords>Graphical user interface, real-time performance, map, dynamic routing </keywords>
  </document>
  <document>
    <name>nime2005_093.pdf</name>
    <abstract>The breath pressure signal applied to wind music instruments is generally considered to be a slowly varying function of time. In a context of music control, this assumptionimplies that a relatively low digital sample rate (100-200Hz) is sufficient to capture and/or reproduce this signal.We tested this assumption by evaluating the frequency content in breath pressure, particularly during the use of extended performance techniques such as growling, humming,and flutter tonguing. Our results indicate frequency contentin a breath pressure signal up to about 10 kHz, with especially significant energy within the first 1000 Hz. We furtherinvestigated the frequency response of several commerciallyavailable pressure sensors to assess their responsiveness tohigher frequency breath signals. Though results were mixed,some devices were found capable of sensing frequencies upto at least 1.5 kHz. Finally, similar measurements were conducted with Yamaha WX11 and WX5 wind controllers andresults suggest that their breath pressure outputs are sampled at about 320 Hz and 280 Hz, respectively.</abstract>
    <keywords>Breath Control, Wind Controller, Breath Sensors </keywords>
  </document>
  <document>
    <name>nime2005_097.pdf</name>
    <abstract>Tangible Acoustic Interfaces (TAI) rely on various acousticsensing technologies, such as sound source location and acoustic imaging, to detect the position of contact of users interacting with the surface of solid materials. With their ability to transform almost any physical objects, flat or curved surfaces and walls into interactive interfaces, acoustic sensing technologies show a promising way to bring the sense of touch into the realm of computer interaction. Because music making has been closely related to this sense during centuries, an application of particular interest is the use of TAI's for the design of new musical instruments that matches the physicality and expressiveness of classical instruments. This paper gives an overview of the various acoustic-sensing technologies involved in the realisation of TAI's and develops on the motivation underlying their use for the design of new musical instruments. </abstract>
    <keywords>Tangible interfaces, new musical instruments design.  </keywords>
  </document>
  <document>
    <name>nime2005_101.pdf</name>
  </document>
  <document>
    <name>nime2005_105.pdf</name>
    <abstract>This paper aims to present some perspectives on mappingembouchure gestures of flute players and their use as controlvariables. For this purpose, we have analyzed several typesof sensors, in terms of sensitivity, dimension, accuracy andprice, which can be used to implement a system capable ofmapping embouchure parameters such as air jet velocity andair jet direction. Finally, we describe the implementationof a sensor system used to map embouchure gestures of aclassical Boehm flute.</abstract>
    <keywords>Embouchure, air pressure sensors, hot wires, mapping, aug- mented flute. </keywords>
  </document>
  <document>
    <name>nime2005_109.pdf</name>
    <keywords>Haptic, interaction, sound, music, control, installation.  </keywords>
  </document>
  <document>
    <name>nime2005_115.pdf</name>
    <abstract>We report on The Manual Input Sessions, a series of audiovisual vignettes which probe the expressive possibilities of free-form hand gestures. Performed on a hybrid projection system which combines a traditional analog overhead projector and a digital PC video projector, our vision-based software instruments generate dynamic sounds and graphics solely in response to the forms and movements of the silhouette contours of the user's hands. Interactions and audiovisual mappings which make use of both positive (exterior) and negative (interior) contours are discussed. </abstract>
    <keywords>Audiovisual performance, hand silhouettes, computer vision, contour analysis, sound-image relationships, augmented reality.  </keywords>
  </document>
  <document>
    <name>nime2005_121.pdf</name>
    <abstract>The HandySinger system is a personified tool developedto naturally express a singing voice controlled by the gestures of a hand puppet. Assuming that a singing voice is akind of musical expression, natural expressions of the singingvoice are important for personification. We adopt a singingvoice morphing algorithm that effectively smoothes out thestrength of expressions delivered with a singing voice. Thesystem's hand puppet consists of a glove with seven bendsensors and two pressure sensors. It sensitively capturesthe user's motion as a personified puppet's gesture. Tosynthesize the different expressional strengths of a singingvoice, the "normal" (without expression) voice of a particular singer is used as the base of morphing, and three different expressions, "dark," "whisper" and "wet," are used asthe target. This configuration provides musically expressedcontrols that are intuitive to users. In the experiment, weevaluate whether 1) the morphing algorithm interpolatesexpressional strength in a perceptual sense, 2) the handpuppet interface provides gesture data at sufficient resolution, and 3) the gestural mapping of the current systemworks as planned.</abstract>
    <keywords>Personified Expression, Singing Voice Morphing, Voice Ex- pressivity, Hand-puppet Interface </keywords>
  </document>
  <document>
    <name>nime2005_127.pdf</name>
    <abstract>The central role of the face in social interaction and non-verbal communication suggest we explore facial action as a means of musical expression. This paper presents the design, implementation, and preliminary studies of a novel system utilizing face detection and optic flow algorithms to associate facial movements with sound synthesis in a topographically specific fashion. We report on our experience with various gesture-to-sound mappings and applications, and describe our preliminary experiments at musical performance using the system. </abstract>
    <keywords>Video-based musical interface; gesture-based interaction; facial expression; facial therapy interface.  </keywords>
  </document>
  <document>
    <name>nime2005_132.pdf</name>
    <abstract>In this paper we present an example of the use of the singingvoice as a controller for digital music synthesis. The analysis of the voice with spectral processing techniques, derivedfrom the Short-Time Fourier Transform, provides ways ofdetermining a performer's vocal intentions. We demonstratea prototype, in which the extracted vocal features drive thesynthesis of a plucked bass guitar. The sound synthesis stageincludes two different synthesis techniques, Physical Modelsand Spectral Morph.</abstract>
    <keywords>Singing voice, musical controller, sound synthesis, spectral processing. </keywords>
  </document>
  <document>
    <name>nime2005_136.pdf</name>
    <abstract>Electronic Musical Instrument Design is an excellent vehiclefor bringing students from multiple disciplines together towork on projects, and help bridge the perennial gap betweenthe arts and the sciences. This paper describes how at TuftsUniversity, a school with no music technology program,students from the engineering (electrical, mechanical, andcomputer), music, performing arts, and visual arts areas usetheir complementary skills, and teach each other, to developnew devices and systems for music performance and control.</abstract>
    <keywords>Science education, music education, engineering, electronic music, gesture controllers, MIDI. </keywords>
  </document>
  <document>
    <name>nime2005_140.pdf</name>
    <abstract>The [hid] toolkit is a set of software objects for designingcomputer-based gestural instruments. All too frequently,computer-based performers are tied to the keyboard-mousemonitor model, narrowly constraining the range of possiblegestures. A multitude of gestural input devices are readilyavailable, making it easy to utilize a broader range of gestures. Human Interface Devices (HIDs) such as joysticks,tablets, and gamepads are cheap and can be good musicalcontrollers. Some even provide haptic feedback. The [hid]toolkit provides a unified, consistent framework for gettinggestural data from these devices, controlling the feedback,and mapping this data to the desired output. The [hid]toolkit is built in Pd, which provides an ideal platform forthis work, combining the ability to synthesize and controlaudio and video. The addition of easy access to gesturaldata allows for rapid prototypes. A usable environmentalso makes computer music instrument design accessible tonovices.</abstract>
    <keywords>Instrument design, haptic feedback, gestural control, HID </keywords>
  </document>
  <document>
    <name>nime2005_144.pdf</name>
    <abstract>An experimental study comparing different user interfaces for a virtual drum is reported. Virtual here means that the drum is not a physical object. 16 subjects played the drum on five different interfaces and two metronome patterns trying to match their hits to the metronome clicks. Temporal accuracy of the playing was evaluated. The subjects also rated the interfaces subjectively. The results show that hitting the drum alternately from both sides with motion going through the drum plate was less accurate than the traditional one sided hitting. A physical stick was more accurate than a virtual computer graphic stick. Visual feedback of the drum slightly increased accuracy compared to receiving only auditory feedback. Most subjects evaluated the physical stick to offer a better feeling and to be more pleasant than the virtual stick. </abstract>
    <keywords>Virtual drum, user interface, feedback, musical instrument design, virtual reality, sound control, percussion instrument.  </keywords>
  </document>
  <document>
    <name>nime2005_148.pdf</name>
    <abstract>This paper takes the reader through various elements of the GoingPublik sound artwork for distributive ensemble and introduces the Realtime Score Synthesis tool (RSS) used as a controller in the work. The collaboration between artists and scientists, details concerning the experimental hardware and software, and new theories of sound art are briefly explained and illustrated. The scope of this project is too broad to be fully covered in this paper, therefore the selection of topics made attempts to draw attention to the work itself and balance theory with practice. </abstract>
    <keywords>Mobile Multimedia, Wearable Computers, Score Synthesis, Sound Art, System Research, HCIs   </keywords>
  </document>
  <document>
    <name>nime2005_152.pdf</name>
    <keywords>Motion tracking, mapping strategies, public installation, multiple participants music interfaces.   </keywords>
  </document>
  <document>
    <name>nime2005_156.pdf</name>
    <abstract>This paper describes software tools used to create java applications for performing music using mobile phones. The tools provide a means for composers working in the Pure Data composition environment to design and audition performances using ensembles of mobile phones. These tools were developed as part of a larger project motivated by the desire to allow large groups of non-expert players to perform music based on just intonation using ubiquitous technology. The paper discusses the process that replicates a Pure Data patch so that it will operate within the hardware and software constraints of the Java 2 Micro Edition. It also describes development of objects that will enable mobile phone performances to be simulated accurately in PD and to audition microtonal tuning implemented using MIDI in the j2me environment. These tools eliminate the need for composers to compose for mobile phones by writing java code. In a single desktop application, they offer the composer the flexibility to write music for multiple phones. </abstract>
    <keywords>Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media Performance; Just Intonation.  </keywords>
  </document>
  <document>
    <name>nime2005_160.pdf</name>
    <abstract>This paper details the motivations, design, and realization of Sustainable, a dynamic, robotic sound installation that employs a generative algorithm for music and sound creation. The piece is comprised of seven autonomous water gong nodes that are networked together by water tubes to distribute water throughout the system. A water resource allocation algorithm guides this distribution process and produces an ever-evolving sonic and visual texture. A simple set of behaviors govern the individual gongs, and the system as a whole exhibits emergent properties that yield local and large scale forms in sound and light. </abstract>
  </document>
  <document>
    <name>nime2005_164.pdf</name>
    <abstract>We present our work in the development of an interface for an actor/singer and its use in performing. Our work combines aspects of theatrical music with technology. Our interface has allowed the development of a new vocabulary for musical and theatrical expression and the possibility for merging classical and experimental music. It gave rise to a strong, strange, unpredictable, yet coherent, "character" and opens up the possibility for a full performance that will explore aspects of voice, theatrical music and, in the future, image projection. </abstract>
    <keywords>Theatrical music, computer interaction, voice, gestural control.  </keywords>
  </document>
  <document>
    <name>nime2005_168.pdf</name>
    <abstract>This paper describes the development, function andperformance contexts of a digital musical instrument called"boomBox." The instrument is a wireless, orientation-awarelow-frequency, high-amplitude human motion controller forlive and sampled sound. The instrument has been used inperformance and sound installation contexts. I describe someof what I have learned from the project herein.</abstract>
  </document>
  <document>
    <name>nime2005_176.pdf</name>
    <abstract>In this paper, we describe a course of research investigating thepotential for new types of music made possible by locationtracking and wireless technologies. Listeners walk arounddowntown Culver City, California and explore a new type ofmusical album by mixing together songs and stories based ontheir movement. By using mobile devices as an interface, wecan create new types of musical experiences that allowlisteners to take a more interactive approach to an album.</abstract>
    <keywords>Mobile Music, Digital Soundscape, Location-Based Entertainment, Mobility, Interactive Music, Augmented Reality </keywords>
  </document>
  <document>
    <name>nime2005_180.pdf</name>
    <abstract>Bangarama is a music controller using headbanging as the primaryinteraction metaphor. It consists of a head-mounted tilt sensor and aguitar-shaped controller that does not require complex finger positions. We discuss the specific challenges of designing and buildingthis controller to create a simple, yet responsive and playable instrument, and show how ordinary materials such as plywood, tinfoil, and copper wire can be turned into a device that enables a fun,collaborative music-making experience.</abstract>
    <keywords>head movements, music controllers, interface design, input devices </keywords>
  </document>
  <document>
    <name>nime2005_184.pdf</name>
    <abstract>In recent years Computer Network-Music has increasingly captured the attention of the Computer Music Community. With the advent of Internet communication, geographical displacement amongst the participants of a computer mediated music performance achieved world wide extension. However, when established over long distance networks, this form of musical communication has a fundamental problem: network latency (or net-delay) is an impediment for real-time collaboration. From a recent study, carried out by the authors, a relation between network latency tolerance and Music Tempo was established. This result emerged from an experiment, in which simulated network latency conditions were applied to the performance of different musicians playing jazz standard tunes. The Public Sound Objects (PSOs) project is web-based shared musical space, which has been an experimental framework to implement and test different approaches for on-line music communication. This paper describe features implemented in the latest version of the PSOs system, including the notion of a network-music instrument incorporating latency as a software function, by dynamically adapting its tempo to the communication delay measured in real-time. </abstract>
    <keywords>Network Music Instruments; Latency in Real-Time Performance; Interface-Decoupled Electronic Musical Instruments; Behavioral Driven Interfaces; Collaborative Remote Music Performance;  </keywords>
  </document>
  <document>
    <name>nime2005_188.pdf</name>
    <abstract>We present the Pin&amp;Play&amp;Perform system: an interface inthe form of a tablet on which a number of physical controlscan be added, removed and arranged on the fly. These controls can easily be mapped to existing music sofware usingthe MIDI protocol. The interface provides a mechanism fordirect manipulation of application parameters and eventsthrough a set of familiar controls, while also encouraging ahigh degree of customisation through the ability to arrange,rearrange and annotate the spatial layout of the interfacecomponents on the surface of the tablet.The paper describes how we have realized this concept using the Pin&amp;Play technology. As an application example, wedescribe our experiences in using our interface in conjunction with Propellerheads' Reason, a popular piece of musicsynthesis software.</abstract>
    <keywords>tangible interface, rearrangeable interface, midi controllers </keywords>
  </document>
  <document>
    <name>nime2005_196.pdf</name>
    <abstract>ChucK is a programming language for real-time sound synthesis. It provides generalized audio abstractions and precise control over timing and concurrency - combining the rapid-prototyping advantages of high-level programming tools, such as Pure Data, with the flexibility and controllability of lower-level, text-based languages like C/C++. In this paper, we present a new time-based paradigm for programming controllers with ChucK. In addition to real-time control over sound synthesis, we show how features such as dynamic patching, on-the-fly controller mapping, multiple control rates, and precisely-timed recording and playback of sensors can be employed under the ChucK programming model. Using this framework, composers, programmers, and performers can quickly write (and read/debug) complex controller/synthesis programs, and experiment with controller mapping on-the-fly. </abstract>
    <keywords>Controller mapping, programming language, on-the-fly programming, real-time interaction, concurrency.  </keywords>
  </document>
  <document>
    <name>nime2005_200.pdf</name>
    <abstract>Drum controllers designed by researchers and commercialcompanies use a variety of techniques for capturing percussive gestures. It is challenging to obtain both quick responsetimes and low-level data (such as position) that contain expressive information. This research is a comprehensive studyof current methods to evaluate the available strategies andtechnologies. This study aims to demonstrate the benefitsand detriments of the current state of percussion controllersas well as yield tools for those who would wish to conductthis type of study in the future.</abstract>
    <keywords>Percussion Controllers, Timbre-recognition based instruments, Electronic Percussion, Sensors for Interface Design </keywords>
  </document>
  <document>
    <name>nime2005_204.pdf</name>
    <abstract>Discussion of time in interactive computer music systems engineering has been largely limited to data acquisition rates and latency.Since music is an inherently time-based medium, we believe thattime plays a more important role in both the usability and implementation of these systems. In this paper, we present a time designspace, which we use to expose some of the challenges of developing computer music systems with time-based interaction. Wedescribe and analyze the time-related issues we encountered whilstdesigning and building a series of interactive music exhibits thatfall into this design space. These issues often occur because ofthe varying and sometimes conflicting conceptual models of timein the three domains of user, application (music), and engineering.We present some of our latest work in conducting gesture interpretation and frameworks for digital audio, which attempt to analyzeand address these conflicts in temporal conceptual models.</abstract>
  </document>
  <document>
    <name>nime2005_208.pdf</name>
    <keywords>Reconfigurable, Sensors, Computer Music </keywords>
  </document>
  <document>
    <name>nime2005_212.pdf</name>
    <abstract>This paper describes the audio human computer interface experiments of ixi in the past and outlines the current platform for future research. ixi software [5] was founded by Thor Magnusson and Enrike Hurtado Mendieta in year 2000 and since then we've been working on building prototypes in the form of screen-based graphical user interfaces for musical performance, researching human computer interaction in the field of music and creating environments which other people can use to do similar work and for us to use in our workshops. Our initial starting point was that computer music software and the way their interfaces are built need not necessarily be limited to copying the acoustic musical instruments and studio technology that we already have, but additionally we can create unique languages and work processes for the virtual world. The computer is a vast creative space with specific qualities that can and should be explored. </abstract>
    <keywords>Graphical user interfaces, abstract graphical interfaces, hyper- control, intelligent instruments, live performance, machine learning, catalyst software, OSC, interfacing code, open source, Pure Data, SuperCollider.  </keywords>
  </document>
  <document>
    <name>nime2005_216.pdf</name>
    <abstract> Musicians and composers have been using brainwaves as generative sources in music for at least 40 years and the possibility of a brain-computer interface for direct communication and control was first seriously investigated in the early 1970s. Work has been done by many artists and technologists in the intervening years to attempt to control music systems with brainwaves and - indeed - many other biological signals. Despite the richness of EEG, fMRI and other data which can be read from the human brain, there has up to now been only limited success in translating the complex encephalographic data into satisfactory musical results. We are currently pursuing research which we believe will lead to the possibility of direct brain-computer interfaces for rich and expressive musical control. This report will outline the directions of our current research and results. </abstract>
  </document>
  <document>
    <name>nime2005_220.pdf</name>
    <abstract>We present a real-time system which allows musicians tointeract with synthetic virtual characters as they perform.Using Max/MSP to parameterize keyboard and vocal input, meaningful features (pitch, amplitude, chord information, and vocal timbre) are extracted from live performancein real-time. These extracted musical features are thenmapped to character behaviour in such a way that the musician's performance elicits a response from the virtual character. The system uses the ANIMUS framework to generatebelievable character expressions. Experimental results arepresented for simple characters.</abstract>
    <keywords>Music, synthetic characters, advanced man-machine inter- faces, virtual reality, behavioural systems, interaction tech- niques, visualization, immersive entertainment, artistic in- stallations </keywords>
  </document>
  <document>
    <name>nime2005_224.pdf</name>
    <abstract>In the Expression Synthesis Project (ESP), we propose adriving interface for expression synthesis. ESP aims toprovide a compelling metaphor for expressive performance soas to make high-level expressive decisions accessible to nonexperts. In ESP, the user drives a car on a virtual road thatrepresents the music with its twists and turns; and makesdecisions on how to traverse each part of the road. The driver'sdecisions affect in real-time the rendering of the piece. Thepedals and wheel provide a tactile interface for controlling thecar dynamics and musical expression, while the displayportrays a first person view of the road and dashboard from thedriver's seat. This game-like interface allows non-experts tocreate expressive renderings of existing music without havingto master an instrument, and allows expert musicians toexperiment with expressive choice without having to firstmaster the notes of the piece. The prototype system has beentested and refined in numerous demonstrations. This paperpresents the concepts underlying the ESP system and thearchitectural design and implementation of a prototype.</abstract>
    <keywords>Music expression synthesis system, driving interface. </keywords>
  </document>
  <document>
    <name>nime2005_228.pdf</name>
    <abstract>While many new interfaces for musical expression have been presented in the past, methods to evaluate these interfaces are rare.This paper presents a method and a study comparing the potentialfor musical expression of different string-instrument based musicalinterfaces. Cues for musical expression are defined based on results of research in musical expression and on methods for musicaleducation in instrumental pedagogy. Interfaces are evaluated according to how well they are estimated to allow players making useof their existing technique for the creation of expressive music.</abstract>
    <keywords>Musical Expression, electronic bowed string instrument, evaluation of musical input devices, audio signal driven sound synthesis </keywords>
  </document>
  <document>
    <name>nime2005_232.pdf</name>
  </document>
  <document>
    <name>nime2005_236.pdf</name>
    <abstract>A wide variety of singing synthesis models and methods exist,but there are remarkably few real-time controllers for thesemodels. This paper describes a variety of devices developedover the last few years for controlling singing synthesismodels implemented in the Synthesis Toolkit in C++ (STK),Max/MSP, and ChucK. All of the controllers share somecommon features, such as air-pressure sensing for breathingand/or loudness control, means to control pitch, and methodsfor selecting and blending phonemes, diphones, and words.However, the form factors, sensors, mappings, and algorithmsvary greatly between the different controllers.</abstract>
    <keywords>Singing synthesis, real-time singing synthesis control. </keywords>
  </document>
  <document>
    <name>nime2005_238.pdf</name>
    <abstract>The author describes a recent composition for piano andcomputer in which the score performed by the pianist, readfrom a computer monitor, is generated in real-time from avocabulary of predetermined scanned score excerpts. Theauthor outlines the algorithm used to choose and display aparticular excerpt and describes some of the musicaldifficulties faced by the pianist in a performance of the work. </abstract>
    <keywords>Score generation, Jitter. </keywords>
  </document>
  <document>
    <name>nime2005_240.pdf</name>
    <abstract>No Clergy is an interactive music performance/installation inwhich the audience is able to shape the ongoing music. In it,members of a small acoustic ensemble read music notation fromcomputer screens. As each page refreshes, the notation is alteredand shaped by both stochastic transformations of earlier musicwith the same performance and audience feedback, collected viastandard CGI forms. </abstract>
    <keywords>notation, stochastic, interactive, audience, Python, Lilypond </keywords>
  </document>
  <document>
    <name>nime2005_242.pdf</name>
    <abstract>This paper describes the design of SoniMime, a system forthe sonification of hand movement for real-time timbre shaping. We explore the application of the tristimulus timbremodel for the sonification of gestural data, working towardthe goals of musical expressivity and physical responsiveness. SoniMime uses two 3-D accelerometers connected toan Atmel microprocessor which outputs OSC control messages. Data filtering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.</abstract>
    <keywords>Sonification, Musical Controller, Human Computer Interac- tion </keywords>
  </document>
  <document>
    <name>nime2005_244.pdf</name>
    <keywords>Musical controller, sensate surface, mapping system </keywords>
  </document>
  <document>
    <name>nime2005_246.pdf</name>
    <abstract>This paper describes the design and implementation of BeatBoxing, a percussive gestural interface for the liveperformance of electronic music and control of computerbased games and musical activities.</abstract>
    <keywords>Performance, Gestural Mapping, Music Controller, Human- Computer Interaction, PureData (Pd), OSC </keywords>
  </document>
  <document>
    <name>nime2005_248.pdf</name>
    <abstract>This paper describes the development of AirStick, an interface for musical expression. AirStick is played "in the air", in a Theremin style. It is composed of an array of infrared proximity sensors, which allow the mapping of the position of any interfering obstacle inside a bi-dimensional zone. This controller sends both x and y control data to various real-time synthesis algorithms. </abstract>
    <keywords>Music Controller, Infrared Sensing, Computer Music.  </keywords>
  </document>
  <document>
    <name>nime2005_250.pdf</name>
    <keywords>Musical Controller, Collaborative Control, Haptic Interfaces </keywords>
  </document>
  <document>
    <name>nime2005_252.pdf</name>
    <abstract>We present a Virtual Interface to Feel Emotions called VIFE _alpha v.01 (Virtual Interface to Feel Emotions). The work investigates the idea of Synaesthesia and her enormous possibilities creating new realities, sensations and zones where the user can find new points of interaction. This interface allows the user to create sonorous and visual compositions in real time. 6 three-dimensional sonorous forms are modified according to the movements of the user. These forms represent sonorous objects that respond to this by means of sensorial stimuli. Multiple combinations of colors and sound effects superpose to an a the others to give rise to a unique experience. </abstract>
    <keywords>Synaesthesia, 3D render, new reality, virtual interface, creative interaction, sensors.  </keywords>
  </document>
  <document>
    <name>nime2005_254.pdf</name>
    <abstract>The Sonictroller was originally conceived as a means ofintroducing competition into an improvisatory musicalperformance. By reverse-engineering a popular video gameconsole, we were able to map sound information (volume,pitch, and pitch sequences) to any continuous or momentaryaction of a video game sprite.</abstract>
    <keywords>video game, Nintendo, music, sound, controller, Mortal Kombat, trumpet, guitar, voice </keywords>
  </document>
  <document>
    <name>nime2005_258.pdf</name>
    <abstract>In this presentation, we discuss and demonstrate a multiple touch sensitive (MTS) keyboard developed by Robert Moog for John Eaton. Each key of the keyboard is equipped with sensors that detect the three-dimensional position of the performer's finger. The presentation includes some of Eaton's performances for certain earlier prototypes as well as this keyboard. </abstract>
    <keywords>Multiple touch sensitive, MTS, keyboard, key sensor design, upgrading to present-day computers  </keywords>
  </document>
  <document>
    <name>nime2005_260.pdf</name>
    <abstract>This paper will demonstrate the use of the Smart Controller workbench in the Interactive Bell Garden. </abstract>
    <keywords>Control Voltage, Open Sound Control, Algorithmic Composition, MIDI, Sound Installations, Programmable Logic Control, Synthesizers.  </keywords>
  </document>
  <document>
    <name>nime2005_262.pdf</name>
    <abstract>The Swayway is an audio/MIDI device inspired by the simpleconcept of the wind chime.This interactive sculpture translates its swaying motion,triggered by the user, into sound and light. Additionally, themotion of the reeds contributes to the visual aspect of thepiece, converting the whole into a sensory and engagingexperience.</abstract>
    <keywords>Interactive sound sculpture, flex sensors, midi chimes, LEDs, sound installation. </keywords>
  </document>
  <document>
    <name>nime2005_264.pdf</name>
    <abstract>This paper describes the transformation of an everyday object into a digital musical instrument. By tracking hand movements and tilt on one of two axes, the Bubbaboard, a transformed handheld washboard, allows a user to play scales at different octaves while simultaneously offering the ability to use its inherent acoustic percussive qualities. Processed sound is fed to the Mommaspeaker, which creates physically generated vibrato at a speed determined by tilting the Bubbaboard on its second axis. </abstract>
    <keywords>Gesture based controllers, Musical Performance, MIDI, Accelerometer, Microcontroller, Contact Microphone  </keywords>
  </document>
  <document>
    <name>nime2005_266.pdf</name>
    <abstract>The Wise Box is a new wireless digitizing interface for sensors and controllers. An increasing demand for this kind of hardware, especially in the field of dance and computer performance lead us to design a wireless digitizer that allows for multiple users, with high bandwidth and accuracy. The interface design was initiated in early 2004 and shortly described in reference [1]. Our recent effort was directed to make this device available for the community on the form of a manufactured product, similarly to our previous interfaces such as AtoMIC Pro, Eobody or Ethersense [1][2][3]. We describe here the principles we used for the design of the device as well as its technical specifications. The demo will show several devices running at once and used in real-time with a various set of sensors. </abstract>
    <keywords>Gesture, Sensors, WiFi, 802.11, OpenSoundControl.   </keywords>
  </document>
  <document>
    <name>nime2005_268.pdf</name>
    <abstract>Soundstone is a small wireless music controller that tracks movement and gestures, and maps these signals to characteristics of various synthesized and sampled sounds. It is intended to become a general-purpose platform for exploring the sonification of movement, with an emphasis on tactile (haptic) feedback. </abstract>
    <keywords>Gesture recognition, haptics, human factors, force, acceleration,  tactile feedback, general purpose controller, wireless.  </keywords>
  </document>
  <document>
    <name>nime2005_271.pdf</name>
    <abstract>Contemplace is a spatial personality that redesigns itselfdynamically according to its conversations with its visitors.Sometimes welcoming, sometimes shy, and sometimeshostile, Contemplace's mood is apparent through a display ofprojected graphics, spatial sound, and physical motion.Contemplace is an environment in which inhabitationbecomes a two-way dialogue.</abstract>
    <keywords>Interactive space, spatial installation, graphic and aural display, motion tracking, Processing, Flosc </keywords>
  </document>
  <document>
    <name>nime2005_272.pdf</name>
    <abstract>Mocean is an immersive environment that creates sensoryrelationships between natural media, particularly exploringthe potential of water as an emotive interface.</abstract>
    <keywords>New interface, water, pipe organ, natural media, PIC microcontroller, wind instrument, human computer interface. </keywords>
  </document>
  <document>
    <name>nime2006_026.pdf</name>
    <abstract>This paper presents the concepts and techniques used in afamily of location based multimedia works. The paper hasthree main sections: 1.) to describe the architecture of anaudio-visual hardware/software framework we havedeveloped for the realization of a series of locative mediaartworks, 2.) to discuss the theoretical and conceptualunderpinnings motivating the design of the technicalframework, and 3.) to elicit from this, fundamental issuesand questions that can be generalized and applicable to thegrowing practice of locative media.</abstract>
    <keywords>Mobile music, urban fiction, locative media. </keywords>
  </document>
  <document>
    <name>nime2006_037.pdf</name>
    <abstract>This paper describes two new live performance scenarios for performing music using bluetooth-enabled mobile phones. Interaction between mobile phones via wireless link is a key feature of the performance interface for each scenario. Both scenarios are discussed in the context of two publicly performed works for an ensemble of players in which mobile phone handsets are used both as sound sources and as hand-held controllers. In both works mobile phones are mounted in a specially devised pouch attached to a cord and physically swung to produce audio chorusing. During performance some players swing phones while others operate phones as hand-held controllers. Wireless connectivity enables interaction between flying and hand-held phones. Each work features different bluetooth implementations. In one a dedicated mobile phone acts as a server that interconnects multiple clients, while in the other point to point communication takes place between clients on an ad hoc basis. The paper summarises bluetooth tools designed for live performance realisation and concludes with a comparative evaluation of both scenarios for future implementation of performance by large ensembles of nonexpert players performing microtonal music using ubiquitous technology. </abstract>
    <keywords>Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media Performance; Just Intonation.  </keywords>
  </document>
  <document>
    <name>nime2006_043.pdf</name>
    <abstract>Physically situated public art poses significant challenges for the design and realization of interactive, electronic sound works. Consideration of diverse audiences, environmental sensitivity, exhibition conditions, and logistics must guide the artwork. We describe our work in this area, using a recently installed public piece, Transition Soundings, as a case study that reveals a specialized interface and open-ended approach to interactive music making. This case study serves as a vehicle for examination of the real world challenges posed by public art and its outcomes. </abstract>
    <keywords>Music, Sound, Interactivity, Arts, Public Art, Network Systems,  Sculpture, Installation Art, Embedded Electronics.  </keywords>
  </document>
  <document>
    <name>nime2006_049.pdf</name>
    <keywords>Graphical interfaces, collaborative performance, networking,  computer music ensemble, emergence, visualization,  education.  </keywords>
  </document>
  <document>
    <name>nime2006_053.pdf</name>
    <abstract>The culture of laptop improvisation has grown tremendously in recent years. The development of personalized software instruments presents interesting issues in the context of improvised group performances. This paper examines an approach that is aimed at increasing the modes of interactivity between laptop performers and at the same time suggests ways in which audiences can better discern and identify the sonic characteristics of each laptop performer. We refer to software implementation that was developed for the BLISS networked laptop ensemble with view to designing a shared format for the exchange of messages within local and internet based networks. </abstract>
    <keywords>Networked audio technologies, laptop ensemble, centralized  audio server, improvisation  </keywords>
  </document>
  <document>
    <name>nime2006_061.pdf</name>
    <keywords>touch screen, PDA, Pure Data, controller, mobile musical instrument, human computer interaction </keywords>
  </document>
  <document>
    <name>nime2006_065.pdf</name>
    <abstract>This paper discusses the concept of using background music to control video game parameters and thus actions on the screen. Each song selected by the player makes the game look different and behave variedly. The concept is explored by modifying an existing video game and playtesting it with different kinds of MIDI music. Several examples of mapping MIDI parameters to game events are presented. As mobile phones' MIDI players do not usually have a dedicated callback API, a real-time MIDI analysis software for Symbian OS was implemented. Future developments including real-time group performance as a way to control game content are also considered. </abstract>
    <keywords>Games, MIDI, music, rhythm games, background music  reactive games, musically controlled games, MIDI-controlled  games, Virtual Sequencer.  </keywords>
  </document>
  <document>
    <name>nime2006_071.pdf</name>
    <abstract>Turntable musicians have yet to explore new expressions with digital technology. New higher-level development tools open possibilities for these artists to build their own instruments that can achieve artistic goals commercial products cannot. This paper will present a rough overview on the practice and recent development of turntable music, followed by descriptions of two projects by the author. </abstract>
    <keywords>Turntable music, DJ, turntablist, improvisation, Max/MSP, PIC  Microcontroller, Physical Computing  </keywords>
  </document>
  <document>
    <name>nime2006_075.pdf</name>
    <abstract>This report presents an interface for musical performance called the spinCycle. spinCycle enables performers to make visual patterns with brightly colored objects on a spinning turntable platter that get translated into musical arrangements in realtime. I will briefly describe the hardware implementation and the sound generation logic used, as well as provide a historical background for the project.</abstract>
    <keywords>Color-tracking, turntable, visualization, interactivity, synesthesia </keywords>
  </document>
  <document>
    <name>nime2006_081.pdf</name>
    <abstract>The PETECUBE project consists of a series of musical interfaces designed to explore multi-modal feedback. This paper will briefly describe the definition of multimodal feedback, the aim of the project, the development of the first PETECUBE and proposed further work. </abstract>
    <keywords>Multi-modal Feedback. Haptics. Musical Instrument.  </keywords>
  </document>
  <document>
    <name>nime2006_085.pdf</name>
    <keywords>Digital musical instrument, kinesthetic feedback </keywords>
  </document>
  <document>
    <name>nime2006_089.pdf</name>
    <abstract>The Orbophone is a new interface that radiates rather thanprojects sound and image. It provides a cohesive platformfor audio and visual presentation in situations where bothmedia are transmitted from the same location andlocalization in both media is perceptually correlated. Thispaper discusses the advantages of radiation overconventional sound and image projection for certain kindsof interactive public multimedia exhibits and describes theartistic motivation for its development against a historicalbackdrop of sound systems used in public spaces. Oneexhibit using the Orbophone is described in detail togetherwith description and critique of the prototype, discussingaspects of its design and construction. The paper concludeswith an outline of the Orbophone version 2.</abstract>
    <keywords>Immersive Sound; Multi-channel Sound; Loud-speaker Array; Multimedia; Streaming Media; Real-Time Media Performance; Sound Installation. </keywords>
  </document>
  <document>
    <name>nime2006_093.pdf</name>
    <abstract>The gluion is a sensor interface that was designed to overcomesome of the limitations of more traditional designs based onmicrocontrollers, which only provide a small, fixed number ofdigital modules such as counters and serial interfaces. These areoften required to handle sensors where the physical parametercannot easily be converted into a voltage. Other sensors arepacked into modules that include converters and communicatevia SPI or I2C. Finallly, many designs require outputcapabilities beyond simple on/off.The gluion approaches these challenges thru its FPGA-baseddesign which allows for a large number of digital I/O modules.It also provides superior flexibility regarding theirconfiguration, resolution, and functionality. In addition, theFPGA enables a software implementation of the host link - inthe case of the gluion the OSC protocol as well as theunderlying Ethernet layers.</abstract>
  </document>
  <document>
    <name>nime2006_097.pdf</name>
    <abstract>A new sensor integration system and its first incarnation i sdescribed. As well as supporting existing analog sensorarrays a new architecture allows for easy integration of thenew generation of low-cost digital sensors used in computermusic performance instruments and installation art.</abstract>
    <keywords>Gesture, sensor, MEMS, FPGA, network, OSC, configurability </keywords>
  </document>
  <document>
    <name>nime2006_101.pdf</name>
    <abstract>How can we provide interfaces to synthesis algorithms thatwill allow us to manipulate timbre directly, using the sametimbre-words that are used by human musicians to communicate about timbre? This paper describes ongoingwork that uses machine learning methods (principally genetic algorithms and neural networks) to learn (1) to recognise timbral characteristics of sound and (2) to adjust timbral characteristics of existing synthesized sounds.</abstract>
    <keywords>timbre; natural language; neural networks </keywords>
  </document>
  <document>
    <name>nime2006_103.pdf</name>
    <keywords>composition, process, materials, gesture, controller, cross- modal interaction  </keywords>
  </document>
  <document>
    <name>nime2006_114.pdf</name>
    <abstract>This paper reports on ongoing studies of the design and use ofsupport for remote group music making. In this paper weoutline the initial findings of a recent study focusing on thefunction of decay of contributions in collaborative musicmaking. Findings indicate that persistent contributions lendthemselves to individual musical composition and learningnovel interfaces, whilst contributions that quickly decayengender a more focused musical interaction in experiencedparticipants.</abstract>
  </document>
  <document>
    <name>nime2006_118.pdf</name>
    <keywords>Collaborative interface, remote jamming, network music, interaction design, novice, media space INTRODUCTION Most would agree that music is an inherently social ac- tivity [30], but since the </keywords>
  </document>
  <document>
    <name>nime2006_124.pdf</name>
    <abstract>In this paper, we describe the networking of multiple Integral Music Controllers (IMCs) to enable an entirely new method for creating music by tapping into the composite gestures and emotions of not just one, but many performers. The concept and operation of an IMC is reviewed as well as its use in a network of IMC controllers. We then introduce a new technique of Integral Music Control by assessing the composite gesture(s) and emotion(s) of a group of performers through the use of a wireless mesh network. The Telemuse, an IMC designed precisely for this kind of performance, is described and its use in a new musical performance project under development by the authors is discussed. </abstract>
  </document>
  <document>
    <name>nime2006_129.pdf</name>
    <abstract>This paper explores the use of perturbation in designing multiperformer or multi-agent interactive musical interfaces. A problem with the multi-performer approach is how to cohesively organize the independent data inputs into useable control information for synthesis engines. Perturbation has proven useful for navigating multi-agent NIMEs. The author's Windtree is discussed as an example multi-performer instrument in which perturbation is used for multichannel ecological modeling. The Windtree uses a physical system turbulence model controlled in real time by four performers. </abstract>
  </document>
  <document>
    <name>nime2006_134.pdf</name>
    <abstract>We describe the design of a system of compact, wirelesssensor modules meant to capture expressive motion whenworn at the wrists and ankles of a dancer. The sensors form ahigh-speed RF network geared toward real-time dataacquisition from multiple devices simultaneously, enabling asmall dance ensemble to become a collective interface formusic control. Each sensor node includes a 6-axis inertialmeasurement unit (IMU) comprised of three orthogonalgyroscopes and accelerometers in order to capture localdynamics, as well as a capacitive sensor to measure closerange node-to-node proximity. The nodes may also beaugmented with other digital or analog sensors. This paperdescribes application goals, presents the prototype hardwaredesign, introduces concepts for feature extraction andinterpretation, and discusses early test results.</abstract>
    <keywords>Interactive dance, wearable sensor networks, inertial gesture tracking, collective motion analysis, multi-user interface </keywords>
  </document>
  <document>
    <name>nime2006_140.pdf</name>
    <keywords>Sound Spatialization, Ambisonics, Vector Based Additive Panning (VBAP), Wave Field Synthesis, Acousmatic Music </keywords>
  </document>
  <document>
    <name>nime2006_144.pdf</name>
    <abstract>Traditional uses of virtual audio environments tend to focus onperceptually accurate acoustic representations. Though spatialization of sound sources is important, it is necessary to leveragecontrol of the sonic representation when considering musical applications. The proposed framework allows for the creation ofperceptually immersive scenes that function as musical instruments. Loudspeakers and microphones are modeled within thescene along with the listener/performer, creating a navigable 3Dsonic space where sound sources and sinks process audio according to user-defined spatial mappings.</abstract>
    <keywords>Control paradigms, 3D audio, spatialization, immersive audio environments, auditory display, acoustic modeling, spatial inter- faces, virtual instrument design </keywords>
  </document>
  <document>
    <name>nime2006_150.pdf</name>
    <keywords>Software Architecture, Interactive Systems, Music soft- ware </keywords>
  </document>
  <document>
    <name>nime2006_156.pdf</name>
  </document>
  <document>
    <name>nime2006_162.pdf</name>
    <abstract>The ixi software project started in 2000 with the intention to explore new interactive patterns and virtual interfaces in computer music software. The aim of this paper is not to describe these programs, as they have been described elsewhere [14][15], but rather explicate the theoretical background that underlies the design of these screen-based instruments. After an analysis of the similarities and differences in the design of acoustic and screen-based instruments, the paper describes how the creation of an interface is essentially the creation of a semiotic system that affects and influences the musician and the composer. Finally the terminology of this semiotics is explained as an interaction model. </abstract>
    <keywords>Interfaces, interaction design, HCI, semiotics, actors, OSC,  mapping, interaction models, creative tools.  </keywords>
  </document>
  <document>
    <name>nime2006_168.pdf</name>
    <keywords>Software control of computer music, laptop performance, graphical interfaces, freehand input, dynamic simulation </keywords>
  </document>
  <document>
    <name>nime2006_172.pdf</name>
    <abstract>Development of a musical interface which allows people to play music intuitively and create music visibly. </abstract>
  </document>
  <document>
    <name>nime2006_176.pdf</name>
    <keywords>Gesture description, gesture analysis, standards </keywords>
  </document>
  <document>
    <name>nime2006_180.pdf</name>
    <keywords>sensors, Wiki, collaborative website, open content </keywords>
  </document>
  <document>
    <name>nime2006_192.pdf</name>
    <abstract>In this paper we describe a new guitar-like musical controller. The 'GXtar' is an instrument which takes as a starting point a guitar but his role is to bring different and new musical possibilities while preserving the spirit and techniques of guitar. Therefore, it was conceived and carried out starting from the body of an electric guitar. The fingerboard of this guitar was equipped with two lines of sensors: linear position sensors, and tactile pressure sensors. These two lines of sensors are used as two virtual strings. Their two ends are the bridge and the nut of the guitar. The design of the instrument is made in a way that the position of a finger, on one of these virtual strings, corresponds to the note, which would have been played on a real and vibrating string. On the soundboard of the guitar, a controller, with 3 degrees of freedom, allows to drive other synthesis parameters. We then describe how this interface is integrated in a musical audio system and serves as a musical instrument. </abstract>
    <keywords>Guitar, alternate controller, sensors, synthesizer, multidimensional  control.  </keywords>
  </document>
  <document>
    <name>nime2006_196.pdf</name>
  </document>
  <document>
    <name>nime2006_200.pdf</name>
    <abstract>A cost-effective method was developed for the estimation of the bow velocity in violin playing, using an accelerometer on the bow in combination with point tracking using a standard video camera. The video data are used to detect the moments of bow direction changes. This information is used for piece-wise integration of the accelerometer signal, resulting in a drift-free reconstructed velocity signal with a high temporal resolution. The method was evaluated using a 3D motion capturing system, providing a reliable reference of the actual bow velocity. The method showed good results when the accelerometer and video stream are synchronized. Additional latency and jitter of the camera stream can importantly decrease the performance of the method, depending on the bow stroke type. </abstract>
    <keywords>Bowing gestures, bowed string, violin, bow velocity,  accelerometer, video tracking.  </keywords>
  </document>
  <document>
    <name>nime2006_208.pdf</name>
    <abstract>This paper introduces the scoreTable*, a tangible interactive music score editor which started as a simple application for demoing "traditional" approaches to music creation, using the reacTable* technology, and which has evolved into an independent research project on its own. After a brief discussion on the role of pitch in music, we present a brief overview of related tangible music editors, and discuss several paradigms in computer music creation, contrasting synchronous with asynchronous approaches. The final part of the paper describes the current state of the scoreTable* as well as its future lines of research.</abstract>
    <keywords>Musical instrument, Collaborative Music, Computer Supported  Collaborative Work, Tangible User Interface, Music Theory. </keywords>
  </document>
  <document>
    <name>nime2006_216.pdf</name>
    <abstract>In this paper, we describe our experience in musical interface design for a large scale, high-resolution, multi-touch display surface. We provide an overview of historical and presentday context in multi-touch audio interaction, and describe our approach to analysis of tracked multi-finger, multi-hand data for controlling live audio synthesis.</abstract>
    <keywords>multi-touch, touch, tactile, bi-manual, multi-user, synthesis,  dynamic patching </keywords>
  </document>
  <document>
    <name>nime2006_220.pdf</name>
    <keywords>Musical instrument design, mapping, gestures, organology. </keywords>
  </document>
  <document>
    <name>nime2006_226.pdf</name>
  </document>
  <document>
    <name>nime2006_230.pdf</name>
    <abstract>This paper presents the development of novel "home-made" touch sensors using conductive pigments and various substrate materials. We show that it is possible to build one's own position, pressure and bend sensors with various electrical characteristics, sizes and shapes, and this for a very competitive price. We give examples and provide results from experimental tests of such developments. </abstract>
    <keywords>Touch sensors, piezoresistive technology, conductive pigments,  sensitive materials, interface design  </keywords>
  </document>
  <document>
    <name>nime2006_234.pdf</name>
    <keywords>Ad hoc instruments, Pin&amp;Play, physical interfaces, music performance, new interfaces for musical expression. </keywords>
  </document>
  <document>
    <name>nime2006_240.pdf</name>
    <abstract>In this paper we introduce the Croaker, a novel input deviceinspired by Russolo's Intonarumori. We describe the components of the controller and the sound synthesis engine whichallows to reproduce several everyday sounds.</abstract>
    <keywords>Noise machines, everyday sounds, physical models. </keywords>
  </document>
  <document>
    <name>nime2006_250.pdf</name>
  </document>
  <document>
    <name>nime2006_254.pdf</name>
    <abstract>The MICON is an electronic music stand extending Maestro!, the latest in a series of interactive conducting exhibits that use real orchestral audio and video recordings. The MICON uses OpenGL-based rendering to display and animate score pages with a high degree of realism. It offers three different score display formats to match the user's level of expertise. A realtime animated visual cueing system helps users with their conducting. The MICON has been evaluated with music students. </abstract>
    <keywords>Music stand, score display, exhibit, conducting.  </keywords>
  </document>
  <document>
    <name>nime2006_260.pdf</name>
    <abstract>Designing a conducting gesture analysis system for public spacesposes unique challenges. We present conga, a software framework that enables automatic recognition and interpretation ofconducting gestures. conga is able to recognize multiple types ofgestures with varying levels of difficulty for the user to perform,from a standard four-beat pattern, to simplified up-down conducting movements, to no pattern at all. conga provides an extendablelibrary of feature detectors linked together into a directed acyclicgraph; these graphs represent the various conducting patterns asgesture profiles. At run-time, conga searches for the best profileto match a user's gestures in real-time, and uses a beat prediction algorithm to provide results at the sub-beat level, in additionto output values such as tempo, gesture size, and the gesture'sgeometric center. Unlike some previous approaches, conga doesnot need to be trained with sample data before use. Our preliminary user tests show that conga has a beat recognition rate ofover 90%. conga is deployed as the gesture recognition systemfor Maestro!, an interactive conducting exhibit that opened in theBetty Brinn Children's Museum in Milwaukee, USA in March2006.</abstract>
    <keywords>gesture recognition, conducting, software gesture frameworks </keywords>
  </document>
  <document>
    <name>nime2006_266.pdf</name>
    <keywords>Singing synthesis, voice source, voice quality, spectral model, formant synthesis, instrument, gestural control. </keywords>
  </document>
  <document>
    <name>nime2006_272.pdf</name>
    <abstract>We describe the implementation of an environment for Gesturally-Realized Audio, Speech and Song Performance (GRASSP), which includes a glove-based interface, a mapping/training interface, and a collection of Max/MSP/Jitter bpatchers that allow the user to improvise speech, song, sound synthesis, sound processing, sound localization, and video processing. The mapping/training interface provides a framework for performers to specify by example the mapping between gesture and sound or video controls. We demonstrate the effectiveness of the GRASSP environment for gestural control of musical expression by creating a gesture-to-voice system that is currently being used by performers. </abstract>
    <keywords>Speech synthesis, parallel formant speech synthesizer, gesture  control, Max/MSP, Jitter, Cyberglove, Polhemus, sound  diffusion, UBC Toolbox, Glove-Talk,  </keywords>
  </document>
  <document>
    <name>nime2006_277.pdf</name>
    <abstract>Is there a distinction between New Interfaces for MusicalExpression and New Interfaces for Controlling Sound? Thisarticle begins with a brief overview of expression in musicalperformance, and examines some of the characteristics ofeffective "expressive" computer music instruments. Itbecomes apparent that sophisticated musical expressionrequires not only a good control interface but also virtuosicmastery of the instrument it controls. By studying effectiveacoustic instruments, choosing intuitive but complexgesture-sound mappings that take advantage of establishedinstrumental skills, designing intelligent characterizationsof performance gestures, and promoting long-term dedicatedpractice on a new interface, computer music instrumentdesigners can enhance the expressive quality of computermusic performance.</abstract>
    <keywords>Expression, instrument design, performance, virtuosity. </keywords>
  </document>
  <document>
    <name>nime2006_283.pdf</name>
    <abstract>Why is a seemingly mundane issue such as airline baggageallowance of great significance in regards to the performancepractice of electronic music? This paper discusses how aperformance practice has evolved that seeks to question thebinary and corporate digital world. New 'instruments' andapproaches have emerged that explore 'dirty electronics' and'punktronics': DIY electronic instruments made from junk.These instruments are not instruments in the traditionalsense, defined by physical dimensions or by a set number ofparameters, but modular systems, constantly evolving, nevercomplete, infinitely variable and designed to be portable. Acombination of lo- and hi-fi, analogue and digital,synchronous and asynchronous devices offer new modes ofexpression. The development of these new interfaces formusical expression run side-by-side with an emerging postdigital aesthetic.</abstract>
  </document>
  <document>
    <name>nime2006_292.pdf</name>
    <abstract>This paper is intended to introduce the system, whichcombines "BodySuit" and "RoboticMusic," as well as itspossibilities and its uses in an artistic application."BodySuit" refers to a gesture controller in a Data Suit type."RoboticMusic" refers to percussion robots, which are appliedto a humanoid robot type. In this paper, I will discuss theiraesthetics and the concept, as well as the idea of the "ExtendedBody".</abstract>
    <keywords>Robot, Gesture Controller, Humanoid Robot, Artificial Intelligence, Interaction </keywords>
  </document>
  <document>
    <name>nime2006_296.pdf</name>
  </document>
  <document>
    <name>nime2006_300.pdf</name>
    <keywords>Robotics, computer control, MIDI, player pianos, mechanical music, percussion, sound effects, Dadaism. </keywords>
  </document>
  <document>
    <name>nime2006_304.pdf</name>
    <abstract>This paper deals with the first musical usage of anexperimental system dedicated to the optical detection ofthe position of a trombone's slide.</abstract>
  </document>
  <document>
    <name>nime2006_308.pdf</name>
    <keywords>saxophone, augmented instrument, live electronics, perfor- mance, gestural control </keywords>
  </document>
  <document>
    <name>nime2006_314.pdf</name>
    <keywords>khaen, sound synthesis control, mapping, musical acoustics </keywords>
  </document>
  <document>
    <name>nime2006_318.pdf</name>
    <abstract>In this paper we will report on the use of real-time soundspatialization in Challenging Bodies, a trans-disciplinaryperformance project at the University of Regina. Usingwell-understood spatialization techniques mapped to a custom interface, a computer system was built that allowedlive spatial control of ten sound signals from on-stage performers. This spatial control added a unique dynamic element to an already ultramodern performance. The systemis described in detail, including the main advantages overexisting spatialization systems: simplicity, usability, customization and scalability</abstract>
  </document>
  <document>
    <name>nime2006_322.pdf</name>
    <keywords>mapping, planning, agent, Max/MSP </keywords>
  </document>
  <document>
    <name>nime2006_326.pdf</name>
    <abstract>In this article, we present the first step of our research work todesign a Virtual Assistant for Performers and Stage Directors,able to give a feedback from performances. We use amethodology to automatically construct fuzzy rules in a FuzzyRule-Based System that detects contextual emotions from anactor's performance during a show.We collect video data from a lot of performances of the sameshow from which it should be possible to visualize all theemotions and intents or more precisely "intent graphs". Toperform this, the collected data defining low-level descriptorsare aggregated and converted into high-level characterizations.Then, depending on the retrieved data and on their distributionon the axis, we partition the universes into classes. The last stepis the building of the fuzzy rules that are obtained from theclasses and that permit to give conclusions to label the detectedemotions.</abstract>
    <keywords>Virtual Assistant, Intents, Emotion detector, Fuzzy Classes, Stage Director, Performance. </keywords>
  </document>
  <document>
    <name>nime2006_330.pdf</name>
    <abstract>This is a studio report of researches and projects in SUAC(Shizuoka University of Art and Culture). SUAC was foundedin April 2000, and organized NIME04 as you know. SUAChas "Faculty of Design" and "Department of Art and Science"and all students study interactive systems and media arts.SUAC has organized Media Art Festival (MAF) from 2001 to2005. Domestic/overseas artists participated in SUAC MAF,and SUAC students' projects also joined and exhibited theirworks in MAF. I will introduce the production cases withinteractive media-installations by SUAC students' projectsfrom the aspect "experiences with novel interfaces ineducation and entertainment" and "reports on students projectsin the framework of NIME related courses".</abstract>
    <keywords>Interactive Installation, Sensors, Media Arts, Studio Reports  </keywords>
  </document>
  <document>
    <name>nime2006_334.pdf</name>
    <abstract>In this paper we describe the intentions, the design and functionality of an Acousmatic Composition Environment that allows children or musical novices to educate their auditory curiosity by recording, manipulating and mixing sounds of everyday life. The environment consists of three stands: A stand for sound recording with a soundproof box that ensure good recording facilities in a noisy environment; a stand for sound manipulation with five simple, tangible interfaces; a stand for sound mixing with a graphical computer interface presented on two touch screens. </abstract>
    <keywords>Acousmatic listening, aesthetics, tangible interfaces.  </keywords>
  </document>
  <document>
    <name>nime2006_338.pdf</name>
    <keywords>Bioinformatics, composition, real-time score generation. </keywords>
  </document>
  <document>
    <name>nime2006_346.pdf</name>
  </document>
  <document>
    <name>nime2006_352.pdf</name>
    <abstract>Hyper-shaku (Border-Crossing) is an interactive sensor environment that uses motion sensors to trigger immediate responses and generative processes augmenting the Japanese bamboo shakuhachi in both the auditory and visual domain. The latter differentiates this process from many hyper-instruments by building a performance of visual design as well as electronic music on top of the acoustic performance. It utilizes a combination of computer vision and wireless sensing technologies conflated from preceding works. This paper outlines the use of gesture in these preparatory sound and audio-visual performative, installation and sonification works, leading to a description of the Hyper-shaku environment integrating sonification and generative elements. </abstract>
    <keywords>Gesture-controllers, sonification, hyper-instrument  </keywords>
  </document>
  <document>
    <name>nime2006_358.pdf</name>
    <abstract>Three electro-acoustic systems were devised for a newtrombone work, Rouse. This paper presents the technicalsystems and outlines their musical context and motivation. TheuSlide measures trombone slide-extension by a minimalhardware ultrasonic technique. An easy calibration proceduremaps linear extension to the slide "positions" of the player. TheeMouth is a driver that replaces the mouthpiece, with softwareemulation of trombone tone and algorithmic musical lines,allowing the trombone to appear to play itself. The eMute isbuilt around a loudspeaker unit, driven so that it affects stronglythe player's embouchure, allowing fine control of complex beatpatterns. eMouth and eMute, under control of the uSlide, set upimprovisatory worlds that are part of the composed architectureof Rouse.</abstract>
  </document>
  <document>
    <name>nime2006_376.pdf</name>
    <abstract>This paper describes recent enhancements in an interactive system designed to improvise with saxophonist John Butcher [1]. In addition to musical parameters such as pitch and loudness, our system is able to analyze timbral characteristics of the saxophone tone in real-time, and use timbral information to guide the generation of response material. We capture each saxophone gesture on the fly, extract a set of gestural and timbral contours, and store them in a repository. Improvising agents can consult the repository when generating responses. The gestural or timbral progression of a saxophone phrase can be remapped or transformed; this enables a variety of response material that also references audible contours of the original saxophone gestures. A single simple framework is used to manage gestural and timbral information extracted from analysis, and for expressive control of virtual instruments in a free improvisation context. </abstract>
    <keywords>Interactive music systems, timbre analysis, instrument control.  </keywords>
  </document>
  <document>
    <name>nime2006_384.pdf</name>
  </document>
  <document>
    <name>nime2006_390.pdf</name>
    <abstract>In this paper, some of the more recent developments in musical instruments related to the violin family are described, and analyzed according to several criteria adapted from other publications. While it is impossible to cover all such developments, we have tried to sample a variety of instruments from the last decade or so, with a greater focus on those published in the computer music literature. Experiences in the field of string players focusing on such developments are presented. Conclusions are drawn in which further research into violin-related digital instruments for string players may benefit from the presented criteria as well as the experiences. </abstract>
    <keywords>Violin, viola, cello, bass, digital, electronic, synthesis, controller.  </keywords>
  </document>
  <document>
    <name>nime2006_396.pdf</name>
    <abstract>In this paper we present progress of an ongoingcollaboration between researchers at the MIT MediaLaboratory and the Royal Academy of Music (RAM). The aimof this project is to further explore the expressive musicalpotential of the Hyperbow, a custom music controller firstdesigned for use in violin performance. Through the creationof new repertoire, we hope to stimulate the evolution of thisinterface, advancing its usability and refining itscapabilities. In preparation for this work, the Hyperbowsystem has been adapted for cello (acoustic and electric)performance. The structure of our collaboration is described,and two of the pieces currently in progress are presented.Feedback from the performers is also discussed, as well asfuture plans.</abstract>
    <keywords>Cello, bow, controller, electroacoustic music, composition. </keywords>
  </document>
  <document>
    <name>nime2006_407.pdf</name>
    <abstract>This is a description of a demonstration, regarding theuse of auditory illusions and psycho-acoustic phenomenonused in the interactive work of Jean-Claude Risset, writtenfor violinist Mari Kimura.</abstract>
    <keywords>Violin, psycho-acoustic phenomena, auditory illusions, sig- nal processing, subharmonics, Risset, Kimura. </keywords>
  </document>
  <document>
    <name>nime2006_409.pdf</name>
    <abstract>Software and hardware enhancements to an electric 6-stringcello are described with a focus on a new mechanical tuningdevice, a novel rotary sensor for bow interaction and controlstrategies to leverage a suite of polyphonic soundprocessing effects.</abstract>
    <keywords>Cello, chordophone, FSR, Rotary Absolute Position Encoder, Double Bowing, triple stops, double stops, convolution. </keywords>
  </document>
  <document>
    <name>nime2007_027.pdf</name>
    <abstract>Physical modeling has proven to be a successful method ofsynthesizing highly expressive sounds. However, providingdeep methods of real time musical control remains a majorchallenge. In this paper we describe our work towards aninstrument for percussion synthesis, in which a waveguidemesh is both excited and damped by a 2D matrix of forcesfrom a sensor. By emulating a drum skin both as controllerand sound generator, our instrument has reproduced someof the expressive qualities of hand drumming. Details of ourimplementation are discussed, as well as qualitative resultsand experience gleaned from live performances.</abstract>
    <keywords>Physical modeling, instrument design, expressive control, multi-touch, performance </keywords>
  </document>
  <document>
    <name>nime2007_031.pdf</name>
    <abstract>In this paper we describe the design and implementation ofthe PHYSMISM: an interface for exploring the possibilitiesfor improving the creative use of physical modelling soundsynthesis.The PHYSMISM is implemented in a software and hardware version. Moreover, four different physical modellingtechniques are implemented, to explore the implications ofusing and combining different techniques.In order to evaluate the creative use of physical models,a test was performed using 11 experienced musicians as testsubjects. Results show that the capability of combining thephysical models and the use of a physical interface engagedthe musicians in creative exploration of physical models.</abstract>
    <keywords>Physical models, hybrid instruments, excitation, resonator. </keywords>
  </document>
  <document>
    <name>nime2007_037.pdf</name>
    <abstract>A novel electronic percussion synthesizer prototype is presented. Our ambition is to design an instrument that will produce a high quality, realistic sound based on a physical modelling sound synthesis algorithm. This is achieved using a real-time Field Programmable Gate Array (FPGA) implementation of the model coupled to an interface that aims to make efficient use of all the subtle nuanced gestures of the instrumentalist. It is based on a complex physical model of the vibrating plate - the source of sound in the majority of percussion instruments. A Xilinx Virtex II pro FPGA core handles the sound synthesis computations with an 8 billion operations per second performance and has been designed in such a way to allow a high level of control and flexibility. Strategies are also presented to that allow the parametric space of the model to be mapped to the playing gestures of the percussionist. </abstract>
    <keywords>Physical Model, Electronic Percussion Instrument, FPGA.  </keywords>
  </document>
  <document>
    <name>nime2007_041.pdf</name>
  </document>
  <document>
    <name>nime2007_046.pdf</name>
    <abstract>FigureWe present Zstretch, a textile music controller that supports expressive haptic interactions. The musical controller takes advantage of the fabric's topological constraints to enable proportional control of musical parameters. This novel interface explores ways in which one might treat music as a sheet of cloth. This paper proposes an approach to engage simple technologies for supporting ordinary hand interactions. We show that this combination of basic technology with general tactile movements can result in an expressive musical interface. a</abstract>
    <keywords>Tangible interfaces, textiles, tactile design, musical expressivity  </keywords>
  </document>
  <document>
    <name>nime2007_056.pdf</name>
    <abstract>In this paper, we present a new system, the Orchestra Explorer, enabling a novel paradigm for active fruition of sound and music content. The Orchestra Explorer allows users to physically navigate inside a virtual orchestra, to actively explore the music piece the orchestra is playing, to modify and mold the sound and music content in real-time through their expressive full-body movement and gesture. An implementation of the Orchestra Explorer was developed and presented in the framework of the science exhibition "Cimenti di Invenzione e Armonia", held at Casa Paganini, Genova, from October 2006 to January 2007. </abstract>
    <keywords>Active listening of music, expressive interfaces, full-body motion analysis and expressive gesture processing, multimodal interactive systems for music and performing arts applications.  </keywords>
  </document>
  <document>
    <name>nime2007_062.pdf</name>
    <abstract>We present the Multimodal Music Stand (MMMS) for the untethered sensing of performance gestures and the interactive control of music. Using e-field sensing, audio analysis, and computer vision, the MMMS captures a performer's continuous expressive gestures and robustly identifies discrete cues in a musical performance. Continuous and discrete gestures are sent to an interactive music system featuring custom designed software that performs real-time spectral transformation of audio. </abstract>
    <keywords>Multimodal, interactivity, computer vision, e-field sensing, untethered control.  </keywords>
  </document>
  <document>
    <name>nime2007_066.pdf</name>
    <abstract>This paper describes the T-Stick, a new family of digitalmusical instruments. It presents the motivation behind theproject, hardware and software design, and presents insightsgained through collaboration with performers who have collectively practised and performed with the T-Stick for hundreds of hours, and with composers who have written piecesfor the instrument in the context of McGill University's Digital Orchestra project. Each of the T-Sticks is based on thesame general structure and sensing platform, but each alsodiffers from its siblings in size, weight, timbre and range.</abstract>
    <keywords>gestural controller, digital musical instrument, families of instruments </keywords>
  </document>
  <document>
    <name>nime2007_070.pdf</name>
    <keywords>Musical Instrument Design, Mapping, Musicianship, evaluation, testing.  </keywords>
  </document>
  <document>
    <name>nime2007_078.pdf</name>
    <abstract>In this paper, we present a new bi-manual gestural controller, called HandSketch, composed of purchasable devices :pen tablet and pressure-sensing surfaces. It aims at achieving real-time manipulation of several continuous and articulated aspects of pitched sounds synthesis, with a focus onexpressive voice. Both prefered and non-prefered hand issues are discussed. Concrete playing diagrams and mappingstrategies are described. These results are integrated and acompact controller is proposed.</abstract>
    <keywords>Pen tablet, FSR, bi-manual gestural control. </keywords>
  </document>
  <document>
    <name>nime2007_082.pdf</name>
    <keywords>Portable keyboard, Additional black keys, Diapason change </keywords>
  </document>
  <document>
    <name>nime2007_088.pdf</name>
    <abstract>This paper presents a line of historic electronic musical instruments designed by Erkki Kurenniemi in the 1960's and1970's. Kurenniemi's instruments were influenced by digitallogic and an experimental attitude towards user interfacedesign. The paper presents an overview of Kurenniemi'sinstruments and a detailed description of selected devices.Emphasis is put on user interface issues such as unconventional interactive real-time control and programming methods.</abstract>
    <keywords>Erkki Kurenniemi, Dimi, Synthesizer, Digital electronics, User interface design </keywords>
  </document>
  <document>
    <name>nime2007_094.pdf</name>
    <abstract>This paper reports on a survey conducted in the autumn of 2006 with the objective to understand people's relationship to their musical tools. The survey focused on the question of embodiment and its different modalities in the fields of acoustic and digital instruments. The questions of control, instrumental entropy, limitations and creativity were addressed in relation to people's activities of playing, creating or modifying their instruments. The approach used in the survey was phenomenological, i.e. we were concerned with the experience of playing, composing for and designing digital or acoustic instruments. At the time of analysis, we had 209 replies from musicians, composers, engineers, designers, artists and others interested in this topic. The survey was mainly aimed at instrumentalists and people who create their own instruments or compositions in flexible audio programming environments such as SuperCollider, Pure Data, ChucK, Max/MSP, CSound, etc. </abstract>
    <keywords>Survey, musical instruments, usability, ergonomics, embodiment, mapping, affordances, constraints, instrumental entropy, audio programming.  </keywords>
  </document>
  <document>
    <name>nime2007_100.pdf</name>
    <abstract>We summarize a decade of musical projects and research employing Wacom digitizing tablets as musical controllers, discussing general implementation schemes using Max/MSP and OpenSoundControl, and specific implementations in musical improvisation, interactive sound installation, interactive multimedia performance, and as a compositional assistant. We examine two-handed sensing strategies and schemes for gestural mapping. </abstract>
  </document>
  <document>
    <name>nime2007_106.pdf</name>
    <abstract>We describe the prevailing model of musical expression, which assumes a binary formulation of "the text" and "the act," along with its implied roles of composer and performer. We argue that this model not only excludes some contemporary aesthetic values but also limits the communicative ability of new music interfaces. As an alternative, an ecology of musical creation accounts for both a diversity of aesthetic goals and the complex interrelation of human and non-human agents. An ecological perspective on several approaches to musical creation with interactive technologies reveals an expanded, more inclusive view of artistic interaction that facilitates novel, compelling ways to use technology for music. This paper is fundamentally a call to consider the role of aesthetic values in the analysis of artistic processes and technologies. </abstract>
    <keywords>Expression, expressivity, non-expressive, emotion, discipline,  model, construct, discourse, aesthetic goal, experience,  transparency, evaluation, communication  </keywords>
  </document>
  <document>
    <name>nime2007_112.pdf</name>
    <abstract>Live coding is almost the antithesis of immediate physicalmusicianship, and yet, has attracted the attentions of a numberof computer-literate musicians, as well as the music-savvyprogrammers that might be more expected. It is within thecontext of live coding that I seek to explore the question ofpractising a contemporary digital musical instrument, which i soften raised as an aside but more rarely carried out in research(though see [12]). At what stage of expertise are the membersof the live coding movement, and what practice regimes mighthelp them to find their true potential? </abstract>
    <keywords>Practice, practising, live coding </keywords>
  </document>
  <document>
    <name>nime2007_124.pdf</name>
    <abstract>We present in this paper a complete gestural interface built to support music pedagogy. The development of this prototype concerned both hardware and software components: a small wireless sensor interface including accelerometers and gyroscopes, and an analysis system enabling gesture following and recognition. A first set of experiments was conducted with teenagers in a music theory class. The preliminary results were encouraging concerning the suitability of these developments in music education. </abstract>
    <keywords>Technology-enhanced learning, music pedagogy, wireless interface, gesture-follower, gesture recognition  </keywords>
  </document>
  <document>
    <name>nime2007_130.pdf</name>
    <abstract>Augmenting performances of live popular music with computer systems poses many new challenges. Here, "popular music" is taken to mean music with a mostly steady tempo, some improvisational elements, and largely predetermined melodies, harmonies, and other parts. The overall problem is studied by developing a framework consisting of constraints and subproblems that any solution should address. These problems include beat acquisition, beat phase, score location, sound synthesis, data preparation, and adaptation. A prototype system is described that offers a set of solutions to the problems posed by the framework, and future work is suggested. </abstract>
  </document>
  <document>
    <name>nime2007_136.pdf</name>
    <abstract>We present a system for rhythmic analysis of human motion inreal-time. Using a combination of both spectral (Fourier) andspatial analysis of onsets, we are able to extract repeating rhythmic patterns from data collected using accelerometers. These extracted rhythmic patterns show the relative magnitudes of accentuated movements and their spacing in time. Inspired by previouswork in automatic beat detection of audio recordings, we designedour algorithms to be robust to changes in timing using multipleanalysis techniques and methods for sensor fusion, filtering andclustering. We tested our system using a limited set of movements,as well as dance movements collected from a professional, bothwith promising results.</abstract>
    <keywords>rhythm analysis, dance movement analysis, onset analysis </keywords>
  </document>
  <document>
    <name>nime2007_142.pdf</name>
    <abstract>Remote real-time musical interaction is a domain where endto-end latency is a well known problem. Today, the mainexplored approach aims to keep it below the musicians perception threshold. In this paper, we explore another approach, where end-to-end delays rise to several seconds, butcomputed in a controlled (and synchronized) way dependingon the structure of the musical pieces. Thanks to our fullydistributed prototype called nJam, we perform user experiments to show how this new kind of interactivity breaks theactual end-to-end latency bounds.</abstract>
    <keywords>Remote real-time musical interaction, end-to-end delays, syn- chronization, user experiments, distributed metronome, NMP. </keywords>
  </document>
  <document>
    <name>nime2007_148.pdf</name>
    <abstract>This paper describes the Ashitaka audiovisual instrumentand the process used to develop it. The main idea guidingthe design of the instrument is that motion can be used toconnect audio and visuals, and the first part of the paperconsists of an exploration of this idea. The issue of mappings is raised, discussing both audio-visual mappings andthe mappings between the interface and synthesis methods.The paper concludes with a detailed look at the instrumentitself, including the interface, synthesis methods, and mappings used.</abstract>
  </document>
  <document>
    <name>nime2007_154.pdf</name>
    <abstract>This paper describes several example hybrid acoustic / electronic percussion instruments using realtime convolution toaugment and modify the apparent acoustics of damped physical objects. Examples of cymbal, frame drum, practice pad,brush, and bass drum controllers are described.</abstract>
    <keywords>Musical controllers, extended acoustic instruments </keywords>
  </document>
  <document>
    <name>nime2007_160.pdf</name>
    <abstract>CaMus2 allows collaborative performance with mobile camera phones. The original CaMus project was extended tosupport multiple phones performing in the same space andgenerating MIDI signals to control sound generation andmanipulation software or hardware. Through an opticalflow technology the system can be used without a referencemarker grid. When using a marker grid, the use of dynamicdigital zoom extends the range of performance. Semanticinformation display helps guide the performer visually.</abstract>
    <keywords>Camera phone, mobile phone, music performance, mobile sound generation, sensing-based interaction, collaboration </keywords>
  </document>
  <document>
    <name>nime2007_168.pdf</name>
    <abstract>In this paper the authors present the MIDI Scrapyard Challenge (MSC) workshop, a one-day hands-on experience which asks participants to create musical controllers out of cast-off electronics, found materials and junk. The workshop experience, principles, and considerations are detailed, along with sample projects which have been created in various MSC workshops. Observations and implications as well as future developments for the workshop are discussed. </abstract>
  </document>
  <document>
    <name>nime2007_172.pdf</name>
    <abstract>We present REXband, an interactive music exhibit for collaborative improvisation to medieval music. This audio-only system consists of three digitally augmented medieval instrument replicas: thehurdy gurdy, harp, and frame drum. The instruments communicatewith software that provides users with both musical support andfeedback on their performance using a "virtual audience" set in amedieval tavern. REXband builds upon previous work in interactivemusic exhibits by incorporating aspects of e-learning to educate, inaddition to interaction design patterns to entertain; care was alsotaken to ensure historic authenticity. Feedback from user testingin both controlled (laboratory) and public (museum) environmentshas been extremely positive. REXband is part of the RegensburgExperience, an exhibition scheduled to open in July 2007 to showcase the rich history of Regensburg, Germany.</abstract>
    <keywords>interactive music exhibits, medieval music, augmented instru- ments, e-learning, education </keywords>
  </document>
  <document>
    <name>nime2007_178.pdf</name>
    <abstract>This paper describes work on a newly created large-scaleinteractive theater performance entitled Schwelle (Thresholds). The authors discuss an innovative approach towardsthe conception, development and implementation of dynamicand responsive audio scenography : a constantly evolving,multi-layered sound design generated by continuous inputfrom a series of distributed wireless sensors deployed bothon the body of a performer and placed within the physicalstage environment. The paper is divided into conceptualand technological parts. We first describe the project's dramaturgical and conceptual context in order to situate theartistic framework that has guided the technological systemdesign. Specifically, this framework discusses the team's approach in combining techniques from situated computing,theatrical sound design practice and dynamical systems inorder to create a new kind of adaptive audio scenographicenvironment augmented by wireless, distributed sensing foruse in live theatrical performance. The goal of this adaptive sound design is to move beyond both existing playbackmodels used in theatre sound as well as the purely humancentered, controller-instrument approach used in much current interactive performance practice.</abstract>
    <keywords>Interactive performance, dynamical systems, wireless sens- ing, adaptive audio scenography, audio dramaturgy, situated computing, sound design </keywords>
  </document>
  <document>
    <name>nime2007_185.pdf</name>
    <keywords>Architecture, installation, interaction, granular synthesis, adaptation, engagement. </keywords>
  </document>
  <document>
    <name>nime2007_191.pdf</name>
    <abstract>The distinctive features of interactive sound installations inpublic space are considered, with special attention to therich, if undoubtedly difficult, environments in which theyexist. It is argued that such environments, and the socialcontexts that they imply, are among the most valuable features of these works for the approach that we have adoptedto creation as research practice. The discussion is articulated through case studies drawn from two of our installations, Recycled Soundscapes (2004) and Skyhooks (2006).Implications for the broader design of new musical instruments are presented.</abstract>
  </document>
  <document>
    <name>nime2007_197.pdf</name>
    <abstract>In this paper we introduce a System conceived to serve as the "musical brain" of autonomous musical robots or agent-based software simulations of robotic systems. Our research goal is to provide robots with the ability to integrate with the musical culture of their surroundings. In a multi-agent configuration, the System can simulate an environment in which autonomous agents interact with each other as well as with external agents (e.g., robots, human beings or other systems). The main outcome of these interactions is the transformation and development of their musical styles as well as the musical style of the environment in which they live. </abstract>
  </document>
  <document>
    <name>nime2007_203.pdf</name>
    <abstract> WISEAR (Wireless Sensor Array)8, provides a robust andscalable platform for virtually limitless types of data input tosoftware synthesis engines. It is essentially a Linux based SBC(Single Board Computer) with 802.11a/b/g wireless capability.The device, with batteries, only weighs a few pounds and can beworn by a dancer or other live performer. Past work has focusedon connecting "conventional" sensors (eg., bend sensors,accelerometers, FSRs, etc...) to the board and using it as a datarelay, sending the data as real time control messages to synthesisengines like Max/MSP and RTcmix1. Current research hasextended the abilities of the device to take real-time audio andvideo data from USB cameras and audio devices, as well asrunning synthesis engines on board the device itself. Given itsgeneric network ability (eg., being an 802.11a/b/g device) there istheoretically no limit to the number of WISEAR boxes that canbe used simultaneously in a performance, facilitating multiperformer compositions. This paper will present the basic design philosophy behindWISEAR, explain some of the basic concepts and methods, aswell as provide a live demonstration of the running device, wornby the author.</abstract>
    <keywords>Wireless, sensors, embedded devices, linux, real-time audio, real- time video </keywords>
  </document>
  <document>
    <name>nime2007_205.pdf</name>
    <keywords>Inertial Measurement Unit, IMU, Position Tracking, Interactive Dance Performance, Graphical Object, Mapping. </keywords>
  </document>
  <document>
    <name>nime2007_209.pdf</name>
    <abstract>This paper presents an approach to audio-haptic integration that utilizes Open Sound Control, an increasingly wellsupported standard for audio communication, to initializeand communicate with dynamic virtual environments thatwork with off-the-shelf force-feedback devices.</abstract>
    <keywords>Haptics, control, multi-modal, audio, force-feedback </keywords>
  </document>
  <document>
    <name>nime2007_213.pdf</name>
    <abstract>Chroma based representations of acoustic phenomenon are representations of sound as pitched acoustic energy. A framewise chroma distribution over an entire musical piece is a useful and straightforward representation of its musical pitch over time. This paper examines a method of condensing the block-wise chroma information of a musical piece into a two dimensional embedding. Such an embedding is a representation or map of the different pitched energies in a song, and how these energies relate to each other in the context of the song. The paper presents an interactive version of this representation as an exploratory analytical tool or instrument for granular synthesis. Pointing and clicking on the interactive map recreates the acoustical energy present in the chroma blocks at that location, providing an effective way of both exploring the relationships between sounds in the original piece, and recreating a synthesized approximation of these sounds in an instrumental fashion. </abstract>
    <keywords>Chroma, granular synthesis, dimensionality reduction  </keywords>
  </document>
  <document>
    <name>nime2007_220.pdf</name>
  </document>
  <document>
    <name>nime2007_224.pdf</name>
    <abstract>This report presents the design and construct ion of Rage in Conjunction with the Machine, a simple but novel pairing of musical interface and sound sculpture. The authors discuss the design and creation of this instrument , focusing on the unique aspects of it, including the use of physical systems, large gestural input, scale, and the electronic coupling of a physical input to a physical output.</abstract>
  </document>
  <document>
    <name>nime2007_234.pdf</name>
    <abstract>This paper describes the development of B-Keeper, a reatime beat tracking system implemented in Java and Max/MSP,which is capable of maintaining synchronisation between anelectronic sequencer and a drummer. This enables musicians to interact with electronic parts which are triggeredautomatically by the computer from performance information. We describe an implementation which functions withthe sequencer Ableton Live.</abstract>
    <keywords>Human-Computer Interaction, Automatic Accompaniment, Performance </keywords>
  </document>
  <document>
    <name>nime2007_238.pdf</name>
    <abstract>This paper describes a system enabling a human to perform music with a robot in real-time, in the context of North Indian classical music. We modify a traditional acoustic sitar into a hyperinstrument in order to capture performance gestures for musical analysis. A custom built four-armed robotic Indian drummer was built using a microchip, solenoids, aluminum and folk frame drums. Algorithms written towards "intelligent" machine musicianship are described. The final goal of this research is to have a robotic drummer accompany a professional human sitar player live in performance. </abstract>
    <keywords>Musical Robotics, Electronic Sitar, Hyperinstruments, Music Information Retrieval (MIR).  </keywords>
  </document>
  <document>
    <name>nime2007_242.pdf</name>
    <abstract>The starting point for this project is the want to produce amusic controller that could be employed in such a manner thateven lay public could enjoy the possibilities of mobile art. Allof the works that are discussed here are in relation to a newGPS-based controller, the Wrist-Conductor. The works aretechnically based around the synchronizing possibilitiesusing the GPS Time Mark and are aesthetically rooted in worksthat function in an open public space such as a city or a forest.One of the works intended for the controller, China Gates, i sdiscussed here in detail in order to describe how the GPSWrist-Controller is actually used in a public art context. Theother works, CitySonics, The Enchanted Forest and Get a Pot&amp; a Spoon are described briefly in order to demonstrate thateven a simple controller can be used to create a body of works.This paper also addresses the breaking of the media bubblevia the concept of the "open audience", or how mobile art canengage pedestrians as viewers or listeners within public spaceand not remain an isolated experience for performers only.</abstract>
    <keywords>Mobile Music, GPS, Controller, Collaborative Performance </keywords>
  </document>
  <document>
    <name>nime2007_246.pdf</name>
    <abstract>This paper presents an electronic piano keyboard and computer mouse designed for use in a magnetic resonance imaging scanner. The interface allows neuroscientists studying motor learning of musical tasks to perform functional scans of a subject's brain while synchronizing the scanner, auditory and visual stimuli, and auditory feedback with the onset, offset, and velocity of the piano keys. The design of the initial prototype and environment-specific issues are described, as well as prior work in the field. Preliminary results are positive and were unable to show the existence of image artifacts caused by the interface. Recommendations to improve the optical assembly are provided in order to increase the robustness of the design. </abstract>
    <keywords>Input device, MRI-compatible, fMRI, motor learning, optical sensing.  </keywords>
  </document>
  <document>
    <name>nime2007_254.pdf</name>
    <abstract>This study proposes new possibilities for interaction design pertaining to music piece creation. Specifically, the study created an environment wherein a wide range of users are able to easily experience new musical expressions via a combination of newly developed software and the Nintendo Wii Remote controller. </abstract>
    <keywords>Interactive systems, improvisation, gesture, composition  INTRODUCTION Though music related research focusing on the interaction between people and computers is currently experiencing wide range development, the history of approaches wherein the creation of new musical expression is made possible via the active </keywords>
  </document>
  <document>
    <name>nime2007_256.pdf</name>
    <abstract>Almost all traditional musical instruments have a one-to-one correspondence between a given fingering and the pitch that sounds for that fingering. The Samchillian Tip Tip Tip Cheeepeeeee does not - it is a keyboard MIDI controller that is based on intervals rather than fixed pitches. That is, a given keypress will sound a pitch a number of steps away from the last note sounded (within the key signature and scale selected) according to the 'delta' value assigned to that key. The advantages of such a system are convenience, speed, and the ability to play difficult, unusual and/or unintended passages extemporaneously. </abstract>
    <keywords>samchillian, keyboard, MIDI controller, relative, interval, microtonal, computer keyboard, pitch, musical instrument </keywords>
  </document>
  <document>
    <name>nime2007_260.pdf</name>
    <abstract>Graph Theory links the creative music-making activities of web site visitors to the dynamic generation of an instrumental score for solo violin. Participants use a web-based interface to navigate among short, looping musical fragments to create their own unique path through the open-form composition. Before each concert performance, the violinist prints out a new copy of the score that orders the fragments based on the decisions made by web visitors. </abstract>
    <keywords>Music, Composition, Residency, Audience Interaction, Collaboration, Violin, Graph, Flash, Internet, Traveling Salesman.  </keywords>
  </document>
  <document>
    <name>nime2007_264.pdf</name>
    <abstract>This paper describes the design and implementation of a new interface prototype for live music mixing. The ColorDex system employs a completely new operational metaphor which allows the mix DJ to prepare up to six tracks at once, and perform mixes between up to three of those at a time. The basic premises of the design are: 1) Build a performance tool that multiplies the possible choices a DJ has in respect in how and when tracks are prepared and mixed; 2) Design the system in such a way that the tool does not overload the performer with unnecessary complexity, and 3) Make use of novel technology to make the performance of live music mixing more engaging for both the performer and the audience. The core components of the system are: A software program to load, visualize and playback digitally encoded tracks; the HDDJ device (built chiefly out of a repurposed hard disk drive), which provides tactile manipulation of the playback speed and position of tracks; and the Cubic Crossfader, a wireless sensor cube that controls of the volume of individual tracks, and allows the DJ to mix these in interesting ways. </abstract>
    <keywords>Novel interfaces, live music-mixing, cube-based interfaces, crossfading, repurposing HDDs, accelerometer-based cubic control  </keywords>
  </document>
  <document>
    <name>nime2007_270.pdf</name>
    <keywords>Musical controller, Puredata, scanned synthesis, flex sensors.  </keywords>
  </document>
  <document>
    <name>nime2007_273.pdf</name>
    <abstract>This paper proposes that the physicality of an instrument be considered an important aspect in the design of new interfaces for musical expression. The use of Laban's theory of effort in the design of new effortful interfaces, in particular looking at effortspace modulation, is investigated, and a platform for effortful interface development (named the DAMPER) is described. Finally, future work is described and further areas of research are highlighted. </abstract>
    <keywords>Effortful Interaction. Haptics. Laban Analysis. Physicality. HCI.  </keywords>
  </document>
  <document>
    <name>nime2007_277.pdf</name>
    <abstract>This paper describes the design of Mimi, a multi-modal interactive musical improvisation system that explores the potential and powerful impact of visual feedback in performermachine interaction. Mimi is a performer-centric tool designed for use in performance and teaching. Its key andnovel component is its visual interface, designed to providethe performer with instantaneous and continuous information on the state of the system. For human improvisation,in which context and planning are paramount, the relevantstate of the system extends to the near future and recentpast. Mimi's visual interface allows for a peculiar blendof raw reflex typically associated with improvisation, andpreparation and timing more closely affiliated with scorebased reading. Mimi is not only an effective improvisationpartner, it has also proven itself to be an invaluable platformthrough which to interrogate the mental models necessaryfor successful improvisation.</abstract>
    <keywords>Performer-machine interaction, visualization design, machine improvisation </keywords>
  </document>
  <document>
    <name>nime2007_281.pdf</name>
    <abstract>Many fascinating new developments in the area bowed stringed instruments have been developed in recent years. However, the majority of these new applications are either not well known, used orconsidered in a broader context by their target users. The necessaryexchange between the world of developers and the players is ratherlimited. A group of performers, researchers, instrument developersand composers was founded in order to share expertise and experiences and to give each other feedback on the work done to developnew instruments. Instruments incorporating new interfaces, synthesis methods, sensor technology, new materials like carbon fiber andwood composites as well as composite materials and research outcome are presented and discussed in the group. This paper gives anintroduction to the group and reports about activities and outcomesin the last two years.</abstract>
    <keywords>Interdisciplinary user group, electronic bowed string instrument, evaluation of computer based musical instruments </keywords>
  </document>
  <document>
    <name>nime2007_289.pdf</name>
    <abstract>In this paper, we describe the composition of a piece for choir and Integral Music Controller. We focus more on the aesthetic, conceptual, and practical aspects of the interface and less on the technological details. We especially stress the influence that the designed interface poses on the compositional process and how we approach the expressive organisation of musical materials during the composition of the piece, as well as the addition of nuances (personal real-time expression) by the musicians at performance time. </abstract>
    <keywords>Composition, Integral Music Controller, Emotion measurement, Physiological Measurement, Spatialisation.  </keywords>
  </document>
  <document>
    <name>nime2007_293.pdf</name>
    <abstract>We present an interactive sound spatialization and synthesis system based on Interaural Time Difference (ITD) model and Evolutionary Computation. We define a Sonic Localization Field using sound attenuation and ITD azimuth angle parameters and, in order to control an adaptive algorithm, we used pairs of these parameters as Spatial Sound Genotypes (SSG). They are extracted from waveforms which are considered individuals of a Population Set. A user-interface receives input from a generic gesture interface (such as a NIME device) and interprets them as ITD cues. Trajectories provided by these signals are used as Target Sets of an evolutionary algorithm. A Fitness procedure optimizes locally the distance between the Target Set and the SSG pairs. Through a parametric score the user controls dynamic changes in the sound output. </abstract>
    <keywords>interactive, sound, spatialization, evolutionary, adaptation.  </keywords>
  </document>
  <document>
    <name>nime2007_299.pdf</name>
    <abstract>In this project, eye tracking researchers and computer music composers collaborate to create musical compositions that are played with the eyes. A commercial eye tracker (LC Technologies Eyegaze) is connected to a music and multimedia authoring environment (Max/MSP/Jitter). The project addresses issues of both noise and control: How will the performance benefit from the noise inherent in eye trackers and eye movements, and to what extent should the composition encourage the performer to try to control a specific musical outcome? Providing one set of answers to these two questions, the authors create an eye-controlled composition, EyeMusic v1.0, which was selected by juries for live performance at computer music conferences.Author KeywordsComputer music, eye tracking, new media art, performance.ACM Classification </abstract>
    <keywords>H.5.2 [Information Interfaces and Presentation] User Interfaces - input devices and strategies, interaction styles.  J.5 [Arts and Humanities] Fine arts, performing arts.  </keywords>
  </document>
  <document>
    <name>nime2007_301.pdf</name>
    <abstract>The FrankenPipe project is an attempt to convert a traditionalHighland Bagpipe into a controller capable of driving both realtime synthesis on a laptop as well as a radio-controlled (RC) car.Doing so engages musical creativity while enabling novel, oftenhumorous, performance art. The chanter is outfitted withphotoresistors (CdS photoconductive cells) underneath each hole,allowing a full range of MIDI values to be produced with eachfinger and giving the player a natural feel. An air-pressure sensoris also deployed in the bag to provide another element of controlwhile capturing a fundamental element of bagpipe performance.The final product navigates the realm of both musical instrumentand toy, allowing the performer to create a novel yet richperformance experience for the audience.</abstract>
    <keywords>FrankenPipe, alternate controller, MIDI, bagpipe, photoresistor, chanter. </keywords>
  </document>
  <document>
    <name>nime2007_305.pdf</name>
    <abstract>EyesWeb XMI (for eXtended Multimodal Interaction) is the new version of the well-known EyesWeb platform. It has a main focus on multimodality and the main design target of this new release has been to improve the ability to process and correlate several streams of data. It has been used extensively to build a set of interactive systems for performing arts applications for Festival della Scienza 2006, Genoa, Italy. The purpose of this paper is to describe the developed installations as well as the new EyesWeb features that helped in their development.</abstract>
    <keywords>EyesWeb, multimodal interactive systems, performing arts. </keywords>
  </document>
  <document>
    <name>nime2007_309.pdf</name>
    <abstract>A crucial set of decisions in digital musical instrument design deals with choosing mappings between parameters controlled by the performer and the synthesis algorithms that actually generate sound. Feature-based synthesis offers a way to parameterize audio synthesis in terms of the quantifiable perceptual characteristics, or features, the performer wishes the sound to take on. Techniques for accomplishing such mappings and enabling feature-based synthesis to be performed in real time are discussed. An example is given of how a real-time performance system might be designed to take advantage of feature-based synthesis's ability to provide perceptually meaningful control over a large number of synthesis parameters. </abstract>
    <keywords>Feature, Synthesis, Analysis, Mapping, Real-time.  </keywords>
  </document>
  <document>
    <name>nime2007_313.pdf</name>
    <abstract>This paper introduces jPop-E (java-based PolyPhrase Ensemble), an assistant system for the Pop-E performancerendering system. Using this assistant system, MIDI dataincluding expressive tempo changes or velocity control canbe created based on the user's musical intention. Pop-E(PolyPhrase Ensemble) is one of the few machine systemsdevoted to creating expressive musical performances thatcan deal with the structure of polyphonic music and theuser's interpretation of the music. A well-designed graphical user interface is required to make full use of the potential ability of Pop-E. In this paper, we discuss the necessaryelements of the user interface for Pop-E, and describe theimplemented system, jPop-E.</abstract>
    <keywords>Performance Rendering, User Interface, Ensemble Music Ex- pression </keywords>
  </document>
  <document>
    <name>nime2007_317.pdf</name>
    <abstract>Playing music over the Internet, whether for real-time jamming, network performance or distance education, is constrained by the speed of light which introduces, over long distances, time delays unsuitable for musical applications. Current musical collaboration systems generally transmit compressed audio streams over low-latency and high-bandwidthnetworks to optimize musician synchronization. This paperproposes an alternative approach based on pattern recognition and music prediction. Trained for a particular typeof music, here the Indian tabla drum, the system calledTablaNet identifies rhythmic patterns by recognizing individual strokes played by a musician and mapping them dynamically to known musical constructs. Symbols representing these musical structures are sent over the network toa corresponding computer system. The computer at thereceiving end anticipates incoming events by analyzing previous phrases and synthesizes an estimated audio output.Although such a system may introduce variants due to prediction approximations, resulting in a slightly different musical experience at both ends, we find that it demonstratesa high level of playability with an immediacy not present inother systems, and functions well as an educational tool.</abstract>
    <keywords>network music performance, real-time online musical collab- oration, Indian percussions, tabla bols, strokes recognition, music prediction </keywords>
  </document>
  <document>
    <name>nime2007_321.pdf</name>
    <keywords>JamiOki, PureJoy, collaborative performance, structured im- provisation, electronically-mediated performance, found sound </keywords>
  </document>
  <document>
    <name>nime2007_330.pdf</name>
    <abstract>The guitar pick has traditionally been used to strike or rakethe strings of a guitar or bass, and in rarer instances, ashamisen, lute, or other stringed instrument. The pressure exerted on it, however, has until now been ignored.The MIDI Pick, an enhanced guitar pick, embraces this dimension, acting as a trigger for serial data, audio samples,MIDI messages 1, Max/MSP patches, and on/off messages.This added scope expands greatly the stringed instrumentplayer's musical dynamic in the studio or on stage.</abstract>
    <keywords>guitar, MIDI, pick, plectrum, wireless, bluetooth, ZigBee, Arduino, NIME, ITP </keywords>
  </document>
  <document>
    <name>nime2007_334.pdf</name>
    <abstract>This paper describes the design and experimentation of a Kalman Filter used to improve position tracking of a 3-D gesture-based musical controller known as the Radiodrum. The Singer dynamic model for target tracking is used to describe the evolution of a Radiodrum's stick position in time. The autocorrelation time constant of a gesture's acceleration and the variance of the gesture acceleration are used to tune the model to various performance modes. Multiple Kalman Filters tuned to each gesture type are run in parallel and an Interacting Multiple Model (IMM) is implemented to decide on the best combination of filter outputs to track the current gesture. Our goal is to accurately track Radiodrum gestures through noisy measurement signals. </abstract>
    <keywords>Kalman Filtering, Radiodrum, Gesture Tracking, Interacting Multiple Model  INTRODUCTION Intention is a key aspect of traditional music performance. The ability for an artist to reliably reproduce sound, pitch, rhythms, and emotion is paramount to the design of any instrument. With the </keywords>
  </document>
  <document>
    <name>nime2007_338.pdf</name>
    <abstract>In this paper, design scenarios made possible by the use of an interactive illuminated floor as the basis of an audiovisual environment are presented. By interfacing a network of pressure sensitive, light-emitting tiles with a 7.1 channel speaker system and requisite audio software, many avenues for collaborative expression emerge, as do heretofore unexplored modes of multiplayer music and dance gaming. By giving users light and sound cues that both guide and respond to their movement, a rich environment is created that playfully integrates the auditory, the visual, and the kinesthetic into a unified interactive experience.</abstract>
    <keywords>Responsive Environments, Audiovisual Play, Kinetic Games, Movement Rich Game Play, Immersive Dance, Smart Floor </keywords>
  </document>
  <document>
    <name>nime2007_344.pdf</name>
    <abstract>We present a new group of audio effects that use beat tracking, the detection of beats in an audio signal, to relate effectparameters to the beats in an input signal. Conventional audio effects are augmented so that their operation is related tothe output of a beat tracking system. We present a temposynchronous delay effect and a set of beat synchronous lowfrequency oscillator effects including tremolo, vibrato andauto-wah. All effects are implemented in real-time as VSTplug-ins to allow for their use in live performance.</abstract>
  </document>
  <document>
    <name>nime2007_346.pdf</name>
    <abstract>This article presents various custom software tools called Automatic Notation Generators (ANG's) developed by the authors to aid in the creation of algorithmic instrumental compositions. The unique possibilities afforded by ANG software are described, along with relevant examples of their compositional output. These avenues of exploration include: mappings of spectral data directly into notated music, the creation of software transcribers that enable users to generate multiple realizations of algorithmic compositions, and new types of spontaneous performance with live generated screen-based music notation. The authors present their existing software tools along with suggestions for future research and artistic inquiry. </abstract>
  </document>
  <document>
    <name>nime2007_352.pdf</name>
    <abstract>This paper presents a newly created database containingcalibrated gesture and audio data corresponding to variousviolin bowstrokes, as well as video and motion capture datain some cases. The database is web-accessible and searchable by keywords and subject. It also has several importantfeatures designed to improve accessibility to the data and tofoster collaboration between researchers in fields related tobowed string synthesis, acoustics, and gesture.</abstract>
    <keywords>violin, bowed string, bowstroke, bowing, bowing parame- ters, technique, gesture, audio </keywords>
  </document>
  <document>
    <name>nime2007_358.pdf</name>
    <abstract>This paper presents a methodology and a set of tools for gesture control of sources in 3D surround sound. The techniques for rendering acoustic events on multi-speaker or headphone-based surround systems have evolved considerably, making it possible to use them in real-time performances on light equipment. Controlling the placement of sound sources is usually done in idiosyncratic ways and has not yet been fully explored and formalized. This issue is addressed here with the proposition of a methodical approach. The mapping of gestures to source motion is implemented by giving the sources physical object properties and manipulating these characteristics with standard geometrical transforms through hierarchical or emergent relationships. </abstract>
    <keywords>Gesture, Surround Sound, Mapping, Trajectory, Transform Matrix, Tree Hierarchy, Emergent Structures.  </keywords>
  </document>
  <document>
    <name>nime2007_363.pdf</name>
    <abstract>This work presents an interactive device to control an adaptive tuning and synthesis system. The gestural controller is based on the theremin concept in which only an antenna is used as a proximity sensor. This interactive process is guided by sensorial consonance curves and adaptive tuning related to psychoacoustical studies. We used an algorithm to calculate the dissonance values according to amplitudes and frequencies of a given sound spectrum. The theoretical background is presented followed by interactive composition strategies and sound results. </abstract>
    <keywords>Interaction, adaptive tuning, theremin, sensorial dissonance, synthesis.  </keywords>
  </document>
  <document>
    <name>nime2007_367.pdf</name>
    <abstract>In previous publications (see for example [2] and [3]), we described an interactive music system, designed to improvise with saxophonist John Butcher; our system analyzes timbral and gestural features in real-time, and uses this information to guide response generation. This paper overviews our recent work with the system's interaction management component (IMC). We explore several options for characterizing improvisation at a higher level, and managing decisions for interactive performance in a rich timbral environment. We developed a simple, efficient framework using a small number of features suggested by recent work in mood modeling in music. We describe and evaluate the first version of the IMC, which was used in performance at the Live Algorithms for Music (LAM) conference in December 2006. We touch on developments on the system since LAM, and discuss future plans to address perceived shortcomings in responsiveness, and the ability of the system to make long-term adaptations. </abstract>
    <keywords>Interactive music systems, timbral analysis, free improvisation.  </keywords>
  </document>
  <document>
    <name>nime2007_371.pdf</name>
    <abstract>Until recently, the sonification of Virtual Environments had often been reduced to its simplest expression. Too often soundscapes and background music are predetermined, repetitive and somewhat predictable. Yet, there is room for more complex and interesting sonification schemes that can improve the sensation of presence in a Virtual Environment. In this paper we propose a system that automatically generates original background music in real-time called VR-RoBoser. As a test case we present the application of VR-RoBoser to a dynamic avatar that explores its environment. We show that the musical events are directly and continuously generated and influenced by the behavior of the avatar in three-dimensional virtual space, generating a context dependent sonification. </abstract>
    <keywords>Real-time Composition, Interactive Sonification, Real-time  Neural Processing, Multimedia, Virtual Environment, Avatar.  </keywords>
  </document>
  <document>
    <name>nime2007_379.pdf</name>
    <abstract>This paper describes musical experiments aimed at designing control structures for navigating complex and continuous sonic spaces. The focus is on sound processing techniques which contain a high number of control parameters,and which exhibit subtle and interesting micro-variationsand textural qualities when controlled properly. The examples all use a simple low-dimensional controller - a standardgraphics tablet - and the task of initimate and subtle textural manipulations is left to the design of proper mappings,created using a custom toolbox of mapping functions. Thiswork further acts to contextualize past theoretical results bythe given musical presentations, and arrives at some conclusions about the interplay between musical intention, controlstrategies and the process of their design.</abstract>
    <keywords>Mapping, Control, Sound Texture, Musical Gestures </keywords>
  </document>
  <document>
    <name>nime2007_384.pdf</name>
    <abstract>In this paper we present the concept and prototype of a new musical interface that utilizes the close relationship between gestural expression in the act of painting and that of playing a musical instrument in order to provide non-musicians the opportunity to create musical expression. A physical brush on a canvas acts as the instrument. The characteristics of its stroke are intuitively mapped to a conductor program, defining expressive parameters of the tone in real-time. Two different interaction modes highlight the importance of bodily expression in making music as well as the value of a metaphorical visual representation.</abstract>
    <keywords>musical interface, musical expression, expressive gesture, musical education, natural interface </keywords>
  </document>
  <document>
    <name>nime2007_386.pdf</name>
    <abstract>Freqtric Drums is a new musical, corporal electronic instrument that allows us not only to recover face-to-face communication, but also makes possible body-to-body communication so that a self image based on the sense of being a separate body can be signicant altered through an openness toand even a sense of becoming part of another body. FreqtricDrums is a device that turns audiences surrounding a performer into drums so that the performer, as a drummer, cancommunicate with audience members as if they were a setof drums. We describe our concept and the implementationand process of evolution of Freqtric Drums.</abstract>
    <keywords>interpersonal communication, musical instrument, interac- tion design, skin contact, touch </keywords>
  </document>
  <document>
    <name>nime2007_390.pdf</name>
    <abstract>In this paper we describe a system which allows users to use their full-body for controlling in real-time the generation of an expressive audio-visual feedback. The system extracts expressive motion features from the user's full-body movements and gestures. The values of these motion features are mapped both onto acoustic parameters for the real-time expressive rendering of a piece of music, and onto real-time generated visual feedback projected on a screen in front of the user. </abstract>
    <keywords>Expressive interaction; multimodal environments; interactive music systems  </keywords>
  </document>
  <document>
    <name>nime2007_394.pdf</name>
    <abstract>This paper reports our experiments on using a dual-coreDSP processor in the construction of a user-programmablemusical instrument and controller called the TouchBox.</abstract>
    <keywords>dual-core, DSP, touch-screen, synthesizer, controller </keywords>
  </document>
  <document>
    <name>nime2007_396.pdf</name>
    <keywords>Turntable, D.J. ,Phenakistoscope, multimedia performance </keywords>
  </document>
  <document>
    <name>nime2007_401.pdf</name>
    <abstract>Eowave and Ircam have been deeply involved into gestureanalysis and sensing for a few years by now, as severalartistic projects demonstrate (1). In 2004, Eowave has beenworking with Ircam on the development of the Eobodysensor system, and since that, Eowave's range of sensors hasbeen increased with new sensors sometimes developed innarrow collaboration with artists for custom sensor systemsfor installations and performances. This demo-paperdescribes the recent design of a new USB/MIDI-to-sensorinterface called Eobody2.</abstract>
    <keywords>Gestural controller, Sensor, MIDI, USB, Computer music, Relays, Motors, Robots, Wireless. </keywords>
  </document>
  <document>
    <name>nime2007_403.pdf</name>
    <abstract>The WISP is a novel wireless sensor that uses 3 axis magnetometers, accelerometers, and rate gyroscopes to provide a real-time measurement of its own orientation in space. Orientation data are transmitted via the Open Sound Control protocol (OSC) to a synthesis engine for interactive live dance performance. </abstract>
    <keywords>Music Controller, Human-Computer Interaction, Wireless Sensing, Inertial Sensing.  </keywords>
  </document>
  <document>
    <name>nime2007_407.pdf</name>
    <abstract>This paper introduces a system for improvisational musicalexpression that enables all users, novice and experienced, toperform intuitively and expressively. Users can generate musically consistent results through intuitive action, inputtingrhythm in a decent tempo. We demonstrate novel mappingways that reect user's input information more interactivelyand eectively in generating the music. We also present various input devices that allow users more creative liberty.</abstract>
    <keywords>Improvisation, interactive music, a sense of tempo </keywords>
  </document>
  <document>
    <name>nime2007_409.pdf</name>
    <abstract>We proposed a circle canon system for enjoying a musical ensemble supported by a computer and network. Using the song "Frog round", which is a popular circle canon chorus originated from a German folk song, we produced a singing ensemble opportunity where everyone plays the music together at the same time. The aim of our system is that anyone can experience the joyful feeling of actually playing the music as well as sharing it with others. </abstract>
    <keywords>Circle canon, Chorus, Song, Frog round, Ensemble, Internet, Max/MSP, MySQL database.  </keywords>
  </document>
  <document>
    <name>nime2007_411.pdf</name>
    <abstract>Loop-R is a real-time video performance tool, based in the exploration of low-tech, used technology and human engineering research. With this tool its author is giving a shout to industry, using existing and mistreated technology in innovative ways, combining concepts and interfaces: blending segregated interfaces (GUI and Physical) into one. After graspable interfaces and the "end" of WIMP interfaces, hardware and software blend themselves in a new genre providing free control of video-loops in an expressive hybrid tool. </abstract>
    <keywords>Real-time; video; interface; live-visuals; loop;  </keywords>
  </document>
  <document>
    <name>nime2007_415.pdf</name>
    <abstract>The Music Cre8tor is an interactive music composition systemcontrolled by motion sensors specifically designed forchildren with disabilities although not exclusively for thispopulation. The player(s) of the Music Cre8tor can either holdor attach accelerometer sensors to trigger a variety ofcomputer-generated sounds, MIDI instruments and/or prerecorded sound files. The sensitivity of the sensors can bemodified for each unique individual so that even the smallestmovement can control a sound. The flexibility of the systemis such that either four people can play simultaneously and/orone or more players can use up to four sensors. The originalgoal of this program was to empower students with disabilitiesto create music and encourage them to perform with othermusicians, however this same goal has expanded to includeother populations.</abstract>
    <keywords>Music Education, disabilities, special education, motion sensors, music composition, interactive performance. </keywords>
  </document>
  <document>
    <name>nime2007_417.pdf</name>
    <abstract>In this demonstration, I exemplify how a musical channel ofcommunication can be established in computer-mediatedinteraction between musicians and dancers in real time. Thischannel of communication uses a software libraryimplemented as a library of external objects for Max/MSP[1],that processes data from an object or library that performsframe-differencing analysis of a video stream in real time inthis programming environment.</abstract>
  </document>
  <document>
    <name>nime2008_003.pdf</name>
    <abstract>The rapid development of network communicationtechnologies has allowed composers to create new ways inwhich to directly engage participants in the exploration of newmusical environments. A number of distinctive aestheticapproaches to the musical application of networks will beoutlined in this paper each of which is mediated andconditioned by the technical and aesthetic foundations of thenetwork technologies themselves. Recent work in the field byartists such as Atau Tanaka and Metraform will be examined, aswill some of the earlier pioneering work in the genre by MaxNeuhaus. While recognizing the historical context ofcollaborative work, the author will examine how the strategiesemployed in the work of these artists have helped redefine anew aesthetics of engagement in which play, spatial andtemporal dislocation are amongst the genre's definingcharacteristics.</abstract>
    <keywords>Networks, collaborative, open-form, play, interface. </keywords>
  </document>
  <document>
    <name>nime2008_009.pdf</name>
    <abstract>This paper presents the latest developments of the Public Sound Objects (PSOs) system, an experimental framework to implement and test new concepts for Networked Music. The project of a Public interactive installation using the PSOs system was commissioned in 2007 by Casa da Musica, the main concert hall space in Porto. It resulted in a distributed musical structure with up to ten interactive performance terminals distributed along the Casa da Musica's hallways, collectively controlling a shared acoustic piano. The installation allows the visitors to collaborate remotely with each other, within the building, using a software interface custom developed to facilitate collaborative music practices and with no requirements in terms previous knowledge of musical performance. </abstract>
  </document>
  <document>
    <name>nime2008_013.pdf</name>
    <abstract>New application spaces and artistic forms can emerge whenusers are freed from constraints. In the general case ofhuman-computer interfaces, users are often confined to afixed location, severely limiting mobility. To overcome thisconstraint in the context of musical interaction, we presenta system to manage large-scale collaborative mobile audioenvironments, driven by user movement. Multiple participants navigate through physical space while sharing overlaid virtual elements. Each user is equipped with a mobilecomputing device, GPS receiver, orientation sensor, microphone, headphones, or various combinations of these technologies. We investigate methods of location tracking, wireless audio streaming, and state management between mobiledevices and centralized servers. The result is a system thatallows mobile users, with subjective 3-D audio rendering,to share virtual scenes. The audio elements of these scenescan be organized into large-scale spatial audio interfaces,thus allowing for immersive mobile performance, locativeaudio installations, and many new forms of collaborativesonic activity.</abstract>
    <keywords>sonic navigation, mobile music, spatial interaction, wireless audio streaming, locative media, collaborative interfaces </keywords>
  </document>
  <document>
    <name>nime2008_019.pdf</name>
    <abstract>Open Sound Control (OSC) is being used successfully as amessaging protocol among many computers, gesturalcontrollers and multimedia systems. Although OSC hasaddressed some of the shortcomings of MIDI, OSC cannotdeliver on its promises as a real-time communication protocolfor constrained embedded systems. This paper will examinesome of the advantages but also dispel some of the mythsconcerning OSC. The paper will also describe how some of thebest features of OSC can be used to develop a lightweightprotocol that is microcontroller friendly.</abstract>
  </document>
  <document>
    <name>nime2008_024.pdf</name>
    <abstract>The continuous evolutions in the human-computer interfaces field have allowed the development of control devicesthat let have a more and more intuitive, gestural and noninvasive interaction.Such devices find a natural employment also in the musicapplied informatics and in particular in the electronic music,always searching for new expressive means.This paper presents a prototype of a system for the realtime control of sound spatialization in a multichannel configuration with a multimodal interaction interface. The spatializer, called SMuSIM, employs interaction devices thatrange from the simple and well-established mouse and keyboard to a classical gaming used joystick (gamepad), finallyexploiting more advanced and innovative typologies basedon image analysis (as a webcam).</abstract>
    <keywords>Sound spatialization, multimodal interaction, interaction interfaces, EyesWeb, Pure data. </keywords>
  </document>
  <document>
    <name>nime2008_028.pdf</name>
    <abstract>Over the last century, composers have made increasingly ambitious experiments with musical time, but have been impeded in expressing more temporally-complex musical processes by the limitations of both music notations and human performers. In this paper, we describe a computer-based notation and gestural control system for independently manipulating the tempi of musical parts within a piece, at performance time. We describe how the problem was approached, drawing upon feedback and suggestions from consultations across multiple disciplines, seeking analogous problems in other fields. Throughout, our approach is guided and, ultimately, assessed by an established professional composer, who was able to interact with a working prototype of the system. </abstract>
  </document>
  <document>
    <name>nime2008_034.pdf</name>
    <keywords>synthesis control, expressive timing, playing styles </keywords>
  </document>
  <document>
    <name>nime2008_038.pdf</name>
    <abstract>A new interface for visualizing and analyzing percussion gestures is presented, proposing enhancements of existing motion capture analysis tools. This is achieved by offering apercussion gesture analysis protocol using motion capture.A virtual character dynamic model is then designed in order to take advantage of gesture characteristics, yielding toimprove gesture analysis with visualization and interactioncues of different types.</abstract>
    <keywords>Gesture and sound, interface, percussion gesture, virtual character, interaction. </keywords>
  </document>
  <document>
    <name>nime2008_044.pdf</name>
    <keywords>bowing, gesture, playing technique, principal component anal- ysis, classification </keywords>
  </document>
  <document>
    <name>nime2008_049.pdf</name>
    <abstract>This article discusses a virtual slide guitar instrument, recently introduced in [7]. The instrument consists of a novelphysics-based synthesis model and a gestural user interface.The synthesis engine uses energy-compensated time-varyingdigital waveguides. The string algorithm also contains aparametric model for synthesizing the tube-string contactsounds. The real-time virtual slide guitar user interface employs optical gesture recognition, so that the user can playthis virtual instrument simply by making slide guitar playing gestures in front of a camera.</abstract>
    <keywords>Sound synthesis, slide guitar, gesture control, physical mod- eling </keywords>
  </document>
  <document>
    <name>nime2008_053.pdf</name>
    <keywords>Augmented instrument, electric guitar, gesture-sound relationship </keywords>
  </document>
  <document>
    <name>nime2008_057.pdf</name>
    <abstract>This paper describes the Sormina, a new virtual and tangibleinstrument, which has its origins in both virtual technology andthe heritage of traditional instrument design. The motivationbehind the project is presented, as well as hardware andsoftware design. Insights gained through collaboration withacoustic musicians are presented, as well as comparison tohistorical instrument design.</abstract>
    <keywords>Gestural controller, digital musical instrument, usability, music history, design. </keywords>
  </document>
  <document>
    <name>nime2008_061.pdf</name>
    <abstract>The music community has long had a strong interest in haptic technology. Recently, more effort has been put into making it more and more accessible to instrument designers.This paper covers some of these technologies with the aimof helping instrument designers add haptic feedback to theirinstruments. We begin by giving a brief overview of practicalactuators. Next, we compare and contrast using embeddedmicrocontrollers versus general purpose computers as controllers. Along the way, we mention some common softwareenvironments for implementing control algorithms. Then wediscuss the fundamental haptic control algorithms as well assome more complex ones. Finally, we present two practicaland effective haptic musical instruments: the haptic drumand the Cellomobo.</abstract>
    <keywords>haptic, actuator, practical, immersion, embedded, sampling rate, woofer, haptic drum, Cellomobo </keywords>
  </document>
  <document>
    <name>nime2008_071.pdf</name>
    <abstract>Phya is an open source C++ library originally designed foradding physically modeled contact sounds into computergame environments equipped with physics engines. We review some aspects of this system, and also consider it fromthe purely aesthetic perspective of musical expression.</abstract>
    <keywords>NIME, musical expression, virtual reality, physical model- ing, audio synthesis </keywords>
  </document>
  <document>
    <name>nime2008_077.pdf</name>
    <abstract>In this paper I discuss the importance of and need forpedagogical materials to support the development of newinterfaces and new instruments for electronic music. I describemy method for creating a graduated series of pedagogicaletudes composed using Max/MSP. The etudes will helpperformers and instrument designers learn the most commonlyused basic skills necessary to perform with interactiveelectronic music instruments. My intention is that the finalseries will guide a beginner from these initial steps through agraduated method, eventually incorporating some of the moreadvanced techniques regularly used by electronic musiccomposers.I describe the order of the series, and discuss the benefits (bothto performers and to composers) of having a logical sequence ofskill-based etudes. I also connect the significance of skilledperformers to the development of two essential areas that Iperceive are still just emerging in this field: the creation of acomposed repertoire and an increase in musical expressionduring performance.</abstract>
  </document>
  <document>
    <name>nime2008_081.pdf</name>
    <abstract>The expressive and creative affordances of an interface aredifficult to evaluate, particularly with quantitative methods.However, rigorous qualitative methods do exist and can beused to investigate such topics. We present a methodologybased around user studies involving Discourse Analysis ofspeech. We also present an example of the methodologyin use: we evaluate a musical interface which utilises vocaltimbre, with a user group of beatboxers.</abstract>
  </document>
  <document>
    <name>nime2008_087.pdf</name>
    <abstract>There is small but useful body of research concerning theevaluation of musical interfaces with HCI techniques. Inthis paper, we present a case study in implementing thesetechniques; we describe a usability experiment which evaluated the Nintendo Wiimote as a musical controller, andreflect on the effectiveness of our choice of HCI methodologies in this context. The study offered some valuable results,but our picture of the Wiimote was incomplete as we lackeddata concerning the participants' instantaneous musical experience. Recent trends in HCI are leading researchers totackle this problem of evaluating user experience; we reviewsome of their work and suggest that with some adaptation itcould provide useful new tools and methodologies for computer musicians.</abstract>
    <keywords>HCI Methodology, Wiimote, Evaluating Musical Interac- tion </keywords>
  </document>
  <document>
    <name>nime2008_091.pdf</name>
    <abstract>We combine two concepts, the musical instrument as metaphorand technology probes, to explore how tangible interfaces canexploit the semantic richness of sound. Using participatorydesign methods from Human-Computer Interaction (HCI), wedesigned and tested the A20, a polyhedron-shaped, multichannel audio input/output device. The software maps soundaround the edges and responds to the user's gestural input,allowing both aural and haptic modes of interaction as well asdirect manipulation of media content. The software is designedto be very flexible and can be adapted to a wide range ofshapes. Our tests of the A20's perceptual and interactionproperties showed that users can successfully detect soundplacement, movement and haptic effects on this device. Ourparticipatory design workshops explored the possibilities of theA20 as a generative tool for the design of an extended,collaborative personal music player. The A20 helped users toenact scenarios of everyday mobile music player use and togenerate new design ideas.</abstract>
    <keywords>Generative design tools, Instrument building, Multi-faceted audio, Personal music devices, Tangible user interfaces, Technology probes </keywords>
  </document>
  <document>
    <name>nime2008_097.pdf</name>
    <abstract>The described project is a new approach to use highly sensitive low force pressure sensor matrices for malposition, cramping and tension of hands and fingers, gesture and keystroke analysis and for new musical expression. In the latter, sensors are used as additional touch sensitive switches and keys. In pedagogical issues, new ways of technology enhanced teaching, self teaching and exercising are described. The used sensors are custom made in collaboration with the ReactiveS Sensorlab. </abstract>
    <keywords>Pressure Measurement, Force, Sensor, Finger, Violin, Strings, Piano, Left Hand, Right Hand, Time Line, Cramping, Gesture and Posture Analysis.  </keywords>
  </document>
  <document>
    <name>nime2008_103.pdf</name>
    <abstract>In this paper, we describe an algorithm for the numericalevaluation of the orientation of an object to which a clusterof accelerometers, gyroscopes and magnetometers has beenattached. The algorithm is implemented through a set ofMax/Msp and pd new externals. Through the successfulimplementation of the algorithm, we introduce Pointingat, a new gesture device for the control of sound in a 3Denvironment. This work has been at the core of the Celeritas Project, an interdisciplinary research project on motiontracking technology and multimedia live performances between the Tyndall Institute of Cork and the InteractionDesign Centre of Limerick.</abstract>
  </document>
  <document>
    <name>nime2008_107.pdf</name>
    <abstract>The paper introduces new fiber and malleable materials,including piezoresistive fabric and conductive heat-shrinktubing, and shows techniques and examples of how they maybe used for rapid prototyping and agile development of musicalinstrument controllers. New implementations of well-knowndesigns are covered as well as enhancements of existingcontrollers. Finally, two new controllers are introduced that aremade possible by these recently available materials andconstruction techniques.</abstract>
    <keywords>Agile Development, Rapid Prototyping, Conductive fabric, Piezoresistive fabric, conductive heatshrink tubing, augmented instruments. </keywords>
  </document>
  <document>
    <name>nime2008_113.pdf</name>
    <abstract>In this paper, we describe a set of hardware and software tools for creating musical controllers with any flat surface or simple object, such as tables, walls, metallic plates, wood boards, etc. The system makes possible to transform such physical objects and surfaces into virtual control interfaces, by using computer vision technologies to track the interaction made by the musician, either with the hands, mallets or sticks. These new musical interfaces, freely reconfigurable, can be used to control standard sound modules or effect processors, by defining zones on their surface and assigning them musical commands, such as the triggering of notes or the modulation of parameters.</abstract>
    <keywords>Computer Vision, Multi-touch Interaction, Musical Interfaces.  </keywords>
  </document>
  <document>
    <name>nime2008_117.pdf</name>
    <abstract>This paper presents a comparison of the movement styles of two theremin players based on observation and analysis of video recordings. The premise behind this research is that a consideration of musicians' movements could form the basis for a new framework for the design of new instruments. Laban Movement Analysis is used to qualitatively analyse the movement styles of the musicians and to argue that the Recuperation phase of their phrasing is essential to achieve satisfactory performance. </abstract>
    <keywords>Effort Phrasing, Recuperation, Laban Movement Analysis, Theremin </keywords>
  </document>
  <document>
    <name>nime2008_122.pdf</name>
    <keywords>affective computing, interactive performance, HMM, gesture recognition, intelligent mapping, affective interface </keywords>
  </document>
  <document>
    <name>nime2008_128.pdf</name>
    <abstract>It started with an idea to create an empty space in which you activated music and light as you moved around. In responding to the music and lighting you would activate more or different sounds and thereby communicate with the space through your body. This led to an artistic research project in which children's spontaneous movement was observed, a choreography made based on the children's movements and music written and recorded for the choreography. This music was then decomposed and choreographed into an empty space at Botkyrka konsthall creating an interactive dance installation. It was realized using an interactive sound and light system in which 5 video cameras were detecting the motion in the room connected to a 4-channel sound system and a set of 14 light modules. During five weeks people of all ages came to dance and move around in the installation. The installation attracted a wide range of people of all ages and the tentative evaluation indicates that it was very positively received and that it encouraged free movement in the intended way. Besides observing the activity in the installation interviews were made with schoolchildren age 7 who had participated in the installation. </abstract>
    <keywords>Installation, dance, video recognition, children's movement, interactive multimedia  </keywords>
  </document>
  <document>
    <name>nime2008_134.pdf</name>
    <keywords>Active listening of music, expressive interfaces, full-body motion analysis and expressive gesture processing, multimodal interactive systems for music and performing arts applications, collaborative environments, social interaction.  </keywords>
  </document>
  <document>
    <name>nime2008_140.pdf</name>
    <abstract>Musical open works can be often thought like sequences of musical structures, which can be arranged by anyone who had access to them and who wished to realize the work. This paper proposes an innovative agent-based system to model the information and organize it in structured knowledge; to create effective, graph-centric browsing perspectives and views for the user; to use authoring tools for the performance of open work of electro-acoustic music. </abstract>
    <keywords>Musical Open Work, Multimedia Information Systems,  Software Agents, zz-structures.  </keywords>
  </document>
  <document>
    <name>nime2008_144.pdf</name>
    <abstract>This paper presents an agent-based architecture for robotic musical instruments that generate polyphonic rhythmic patterns that continuously evolve and develop in a musically "intelligent" manner. Agent-based software offers a new method for real-time composition that allows for complex interactions between individual voices while requiring very little user interaction or supervision. The system described, Kinetic Engine, is an environment in which individual software agents, emulate drummers improvising within a percussion ensemble. Player agents assume roles and personalities within the ensemble, and communicate with one another to create complex rhythmic interactions. In this project, the ensemble is comprised of a 12-armed musical robot, MahaDeviBot, in which each limb has its own software agent controlling what it performs. </abstract>
    <keywords>Robotic Musical Instruments, Agents, Machine Musicianship. </keywords>
  </document>
  <document>
    <name>nime2008_150.pdf</name>
    <abstract>In this paper, we investigate the relationships between gesture and sound by means of an elementary gesture sonification. This work takes inspiration from Bauhaus' ideals and Paul Klee's investigation into forms and pictorial representation. In line with these ideas, the main aim of this work is to reduce gesture to a combination of a small number of elementary components (gestalts) used to control a corresponding small set of sounds. By means of a demonstrative tool, we introduce here a line of research that is at its initial stage. The envisaged goal of future developments is a novel system that could be a composing/improvising tool as well as an interface for interactive dance and performance. </abstract>
    <keywords>Bauhaus, Klee, gesture analysis, sonification.  </keywords>
  </document>
  <document>
    <name>nime2008_154.pdf</name>
    <abstract>We present our work with augmented everyday objectstransformed into sound sources for music generation. The idea isto give voice to objects through technology. More specifically, theparadigm of the birth of musical instruments as a sonification ofobjects used in domestic or work everyday environments is hereconsidered and transposed into the technologically augmentedscenarios of our contemporary world.</abstract>
    <keywords>Rag-time washboard, sounding objects, physics-based sound synthesis, interactivity, sonification, augmented everyday objects. </keywords>
  </document>
  <document>
    <name>nime2008_158.pdf</name>
    <abstract>This paper describes a generalized motion-based framework forthe generation of large musical control fields from imaging data.The framework is general in the sense that it does not depend ona particular source of sensing data. Real-time images of stageperformers, pre-recorded and live video, as well as more exoticdata from imaging systems such as thermography, pressuresensor arrays, etc. can be used as a source of control. Featurepoints are extracted from the candidate images, from whichmotion vector fields are calculated. After some processing, thesemotion vectors are mapped individually to sound synthesisparameters. Suitable synthesis techniques include granular andmicrosonic algorithms, additive synthesis and micro-polyphonicorchestration. Implementation details of this framework isdiscussed, as well as suitable creative and artistic uses andapproaches.</abstract>
    <keywords>Computer vision, control field, image analysis, imaging, mapping, microsound, motion flow, sonification, synthesis </keywords>
  </document>
  <document>
    <name>nime2008_164.pdf</name>
    <keywords>Poetry, language sonification, psychoanalysis, linguistics, Freud, realtime poetry.  </keywords>
  </document>
  <document>
    <name>nime2008_168.pdf</name>
    <abstract>Moving out of doors with digital tools and electronic music and creating musically rich experiences is made possible by the increased availability of ever smaller and more powerful mobile computers. Composing music for and in a landscape instead of for a closed architectural space offers new perspectives but also raises questions about interaction and composition of electronic music. The work we present here was commissioned by a festival and ran on a daily basis over a period of three months. A GPS-enabled embedded Linux system is assembled to serve as a location-aware sound platform. Several challenges have to be overcome both technically and artistically to achieve a seamless experience and provide a simple device to be handed to the public. By building this interactive experience, which relies as much on the user's willingness to explore the invisible sonic landscape as on the ability to deploy the technology, a number of new avenues for exploring electronic music and interactivity in location-based media open up. New ways of composing music for and in a landscape and for creating audience interaction are explored. </abstract>
    <keywords>Location-based, electronic music, composition, embedded Linux, GPS, Pure Data, interaction, mapping, soundscape </keywords>
  </document>
  <document>
    <name>nime2008_175.pdf</name>
    <abstract>A general-purpose firmware for a low cost microcontroller is described that employs the Open Sound Control protocol over USB. The firmware is designed with considerations for integration in new musical interfaces and embedded devices. Features of note include stateless design, efficient floating-point support, temporally correct data handling, and protocol completeness. A timing performance analysis is conducted.</abstract>
  </document>
  <document>
    <name>nime2008_181.pdf</name>
    <abstract>An approach for creating structured Open Sound Control(OSC) messages by separating the addressing of node valuesand node properties is suggested. This includes a methodfor querying values and properties. As a result, it is possibleto address complex nodes as classes inside of more complextree structures using an OSC namespace. This is particularly useful for creating flexible communication in modularsystems. A prototype implementation is presented and discussed.</abstract>
  </document>
  <document>
    <name>nime2008_185.pdf</name>
    <abstract>Many mobile devices, specifically mobile phones, come equipped with a microphone. Microphones are high-fidelity sensors that can pick up sounds relating to a range of physical phenomena. Using simple feature extraction methods,parameters can be found that sensibly map to synthesis algorithms to allow expressive and interactive performance.For example blowing noise can be used as a wind instrument excitation source. Also other types of interactionscan be detected via microphones, such as striking. Hencethe microphone, in addition to allowing literal recording,serves as an additional source of input to the developingfield of mobile phone performance.</abstract>
    <keywords>mobile music making, microphone, mobile-stk </keywords>
  </document>
  <document>
    <name>nime2008_193.pdf</name>
    <abstract />
    <keywords />
  </document>
  <document>
    <name>nime2008_197.pdf</name>
  </document>
  <document>
    <name>nime2008_203.pdf</name>
  </document>
  <document>
    <name>nime2008_207.pdf</name>
    <abstract>This paper describes a project started for implementing DJscratching techniques on the reactable. By interacting withobjects representing scratch patterns commonly performedon the turntable and the crossfader, the musician can playwith DJ techniques and manipulate how they are executedin a performance. This is a novel approach to the digital DJapplications and hardware. Two expert musicians practisedand performed on the reactable in order to both evaluate theplayability and improve the design of the DJ techniques.</abstract>
  </document>
  <document>
    <name>nime2008_211.pdf</name>
    <abstract>This paper reports on a Short-Term Scientific Mission (STSM)sponsored by the Sonic Interaction Design (SID) EuropeanCOST Action IC601.Prototypes of objects for the novel instrument Reactablewere developed, with the goal of studying sonification ofmovements on this platform using physical models. A physical model of frictional interactions between rubbed dry surfaces was used as an audio generation engine, which alloweddevelopment in two directions - a set of objects that affordsmotions similar to sliding, and a single object aiming tosonify contact friction sound. Informal evaluation was obtained from a Reactable expert user, regarding these sets ofobjects. Experiments with the objects were also performed- related to both audio filtering, and interfacing with otherobjects for the Reactable.</abstract>
    <keywords>Reactable, physical model, motion sonification, contact fric- tion </keywords>
  </document>
  <document>
    <name>nime2008_215.pdf</name>
    <abstract>This research focuses on real-time gesture learning and recognition. Events arrive in a continuous stream without explicitly given boundaries. To obtain temporal accuracy, weneed to consider the lag between the detection of an eventand any effects we wish to trigger with it. Two methodsfor real time gesture recognition using a Nintendo Wii controller are presented. The first detects gestures similar to agiven template using either a Euclidean distance or a cosinesimilarity measure. The second method uses novel information theoretic methods to detect and categorize gestures inan unsupervised way. The role of supervision, detection lagand the importance of haptic feedback are discussed.</abstract>
    <keywords>Gesture recognition, supervised and unsupervised learning, interaction, haptic feedback, information dynamics, HMMs </keywords>
  </document>
  <document>
    <name>nime2008_219.pdf</name>
    <abstract>This paper describes the compositional process for creatingthe interactive work for violin entitled VITESSIMO using theAugmented Violin [1].</abstract>
    <keywords>Augmented Violin, gesture tracking, interactive performance </keywords>
  </document>
  <document>
    <name>nime2008_221.pdf</name>
    <abstract>Sound libraries for music synthesizers easily comprise onethousand or more programs ("patches"). Thus, there areenough raw data to apply data mining to reveal typicalsettings and to extract dependencies. Intelligent user interfaces for music synthesizers can be based on such statistics.This paper proposes two approaches: First, the user setsany number of parameters and then lets the system find thenearest sounds in the database, a kind of patch autocompletion. Second, all parameters are "live" as usual, but turningone knob or setting a switch will also change the settingsof other, statistically related controls. Both approaches canbe used with the standard interface of the synthesizer. Ontop of that, this paper introduces alternative or additionalinterfaces based on data visualization.</abstract>
    <keywords>Information visualization, mutual information, intelligent user interfaces </keywords>
  </document>
  <document>
    <name>nime2008_225.pdf</name>
    <abstract>This paper presents a project called i-Maestro (www.i-maestro.org) which develops interactive multimedia environments for technology enhanced music education. The project explores novel solutions for music training in both theory and performance, building on recent innovations resulting from the development of computer and information technologies, by exploiting new pedagogical paradigms with cooperative and interactive self-learning environments, gesture interfaces, and augmented instruments. This paper discusses the general context along with the background and current developments of the project, together with an overview of the framework and discussions on a number of selected tools to support technology-enhanced music learning and teaching. </abstract>
  </document>
  <document>
    <name>nime2008_229.pdf</name>
    <abstract>This paper describes the HOP system. It consists of a wireless module built up by multiple nodes and a base station. The nodes detect acceleration of e.g. human movement. At a rate of 100 Hertz the base station collects the acceleration samples. The data can be acquired in real-time software like Pure Data and Max/MSP. The data can be used to analyze and/or sonify movement. </abstract>
    <keywords>Digital Musical Instrument, Wireless Sensors, Inertial Sensing, Hop Sensor  </keywords>
  </document>
  <document>
    <name>nime2008_233.pdf</name>
    <keywords>Ubiquitous computing, context -awareness, networking, embedded systems, chairs, digital artefacts, emotional state sensing, affective computing, biosignals. </keywords>
  </document>
  <document>
    <name>nime2008_237.pdf</name>
    <abstract>We present examples of a wireless sensor network as applied to wearable digital music controllers. Recent advances in wireless Personal Area Networks (PANs) have precipitated the IEEE 802.15.4 standard for low-power, low-cost wireless sensor networks. We have applied this new technology to create a fully wireless, wearable network of accelerometers which are small enough to be hidden under clothing. Various motion analysis and machine learning techniques are applied to the raw accelerometer data in real-time to generate and control music on the fly. </abstract>
    <keywords>Wearable computing, personal area networks, accelerometers, 802.15.4, motion analysis, human-computer interaction, live performance, digital musical controllers, gestural control  </keywords>
  </document>
  <document>
    <name>nime2008_241.pdf</name>
    <abstract>This research aims to develop a wearable musical interfacewhich enables to control audio and video signals by usinghand gestures and human body motions. We have beendeveloping an audio-visual manipulation system that realizes tracks control, time-based operations and searching fortracks from massive music library. It aims to build an emotional and affecting musical interaction, and will providea better method of music listening to people. A sophisticated glove-like device with an acceleration sensor and several strain sensors has been developed. A realtime signalprocessing and musical control are executed as a result ofgesture recognition. We also developed a stand-alone device that performs as a musical controller and player at thesame time. In this paper, we describe the development of acompact and sophisticated sensor device, and demonstrateits performance of audio and video signals control.</abstract>
    <keywords>Embodied Sound Media, Music Controller, Gestures, Body Motion, Musical Interface </keywords>
  </document>
  <document>
    <name>nime2008_245.pdf</name>
    <abstract>This paper proposes the creation of a method book for tabletbased instruments, evaluating pedagogical materials fortraditional instruments as well as research in human-computerinteraction and tablet interfaces.</abstract>
    <keywords>Wacom tablet, digitizing tablet, expressivity, gesture, mapping, pedagogy, practice </keywords>
  </document>
  <document>
    <name>nime2008_249.pdf</name>
    <abstract>We present an audio waveform editor that can be operated in real time through a tabletop interface. The systemcombines multi-touch and tangible interaction techniques inorder to implement the metaphor of a toolkit that allows direct manipulation of a sound sample. The resulting instrument is well suited for live performance based on evolvingloops.</abstract>
    <keywords>tangible interface, tabletop interface, musical performance, interaction techniques </keywords>
  </document>
  <document>
    <name>nime2008_253.pdf</name>
  </document>
  <document>
    <name>nime2008_257.pdf</name>
    <abstract>This paper is about GeoGraphy, a graph-based system forthe control of both musical composition and interactive performance and its implementation in a real-time, interactiveapplication. The implementation includes a flexible userinterface system.</abstract>
  </document>
  <document>
    <name>nime2008_261.pdf</name>
    <abstract>In this paper, we describe the development of multi-platform tools for Audiovisual and Kinetic installations. These involve the connection of three development environments: Python, SuperCollider and Processing, in order to drive kinetic art installations and to combine these with digital synthesis of sound and image in real time. By connecting these three platforms via the OSC protocol, we enable the control in real time of analog physical media (a device that draws figures on sand), sound synthesis and image synthesis. We worked on the development of algorithms for drawing figures and synthesizing images and sound on all three platforms and experimented with various mechanisms for coordinating synthesis and rendering in different media. Several problems were addressed: How to coordinate the timing between different platforms? What configuration to use? Clientserver (who is the client who the server?), equal partners, mixed configurations. A library was developed in SuperCollider to enable the packaging of algorithms into modules with automatic generation of GUI from specifications, and the saving of configurations of modules into session files as scripts in SuperCollider code. The application of this library as a framework for both driving graphic synthesis in Processing and receiving control data from it resulted in an environment for experimentation that is also being used successfully in teaching interactive audiovisual media. </abstract>
    <keywords>kinetic art, audiovisual installations, python, SuperCollider, Processing, algorithmic art, tools for multi-platform development  </keywords>
  </document>
  <document>
    <name>nime2008_265.pdf</name>
    <abstract>Through the developing of tools for analyzing the performerssonic and movement-based gestures, research into the systemperformer interaction has focused on the computer's ability torespond to the performer. Where as such work shows interestwithin the community in developing an interaction paradigmmodeled on the player, by focusing on the perception andreasoning of the system, this research assumes that theperformer's manner of interaction is in agreement with thiscomputational model. My study presents an alternative model ofinteraction designed for improvisatory performance centered onthe perception of the performer as understood by theories takenfrom performance practices and cognitive science.</abstract>
    <keywords>Interactive performance, Perception, HCI </keywords>
  </document>
  <document>
    <name>nime2008_277.pdf</name>
    <abstract>One of the advantages of case-based systems is that theycan generate expressions even if the user doesn't know howthe system applies expression rules. However, the systemscannot avoid the problem of data sparseness and do notpermit a user to improve the expression of a certain part ofa melody directly. After discussing the functions requiredfor user-oriented interface for performance rendering systems, this paper proposes a directable case-based performance rendering system, called Itopul. Itopul is characterized by 1) a combination of the phrasing model and thepulse model, 2) the use of a hierarchical music structure foravoiding from the data sparseness problem, 3) visualizationof the processing progress, and 4) music structures directlymodifiable by the user.</abstract>
    <keywords>Performance Rendering, User Interface, Case-based Approach </keywords>
  </document>
  <document>
    <name>nime2008_281.pdf</name>
    <abstract>In this work we describe our initial explorations in building a musical instrument specifically for providing listenerswith simple, but useful, ambient information. The termAmbient Musical Information Systems (AMIS) is proposedto describe this kind of research. Instruments like these differ from standard musical instruments in that they are tobe perceived indirectly from outside one's primary focus ofattention. We describe our rationale for creating such a device, a discussion on the appropriate qualities of sound fordelivering ambient information, and a description of an instrument created for use in a series of experiments that wewill use to test out ideas. We conclude with a discussion ofour initial findings, and some further directions we wish toexplore.</abstract>
    <keywords>Ambient Musical Information Systems, musical instruments, human computer interaction, Markov chain, probability, al- gorithmic composition </keywords>
  </document>
  <document>
    <name>nime2008_285.pdf</name>
    <abstract>The Elbow Piano distinguishes two types of piano touch: a touchwith movement in the elbow joint and a touch without. A playednote is first mapped to the left or right hand by visual tracking.Custom-built goniometers attached to the player's arms are usedto detect the type of touch. The two different types of touchesare sonified by different instrument sounds. This gives theplayer an increased awareness of his elbow movements, which isconsidered valuable for piano education. We have implementedthe system and evaluated it with a group of music students.</abstract>
    <keywords>Piano, education, sonification, feedback, gesture. </keywords>
  </document>
  <document>
    <name>nime2008_289.pdf</name>
    <abstract>Musical keyboard instruments have a long history, whichresulted in many kinds of keyboards (claviers) today. Sincethe hardware of conventional musical keyboards cannot bechanged, such as the number of keys, musicians have tocarry these large keyboards for playing music that requiresonly a small diapason. To solve this problem, the goal ofour study is to construct UnitKeyboard, which has only 12keys (7 white keys and 5 black keys) and connectors fordocking with other UnitKeyboards. We can build variouskinds of musical keyboard configurations by connecting oneUnitKeyboard to others, since they have automatic settingsfor multiple keyboard instruments. We discuss the usabilityof the UnitKeyboard from reviews by several amateur andprofessional pianists who used the UnitKeyboard.</abstract>
    <keywords>Portable keyboard instruments, block interface, Automatic settings </keywords>
  </document>
  <document>
    <name>nime2008_293.pdf</name>
    <abstract>After eight years of practice on the first hyper-flute prototype (a flute extended with sensors), this article presentsa retrospective of its instrumental practice and the newdevelopments planned from both technological and musical perspectives. Design, performance skills, and mappingstrategies are discussed, as well as interactive compositionand improvisation.</abstract>
  </document>
  <document>
    <name>nime2008_299.pdf</name>
    <abstract>We introduce physically motivated interfaces for playing virtual musical instruments, and we suggest that they lie somewhere in between commonplace interfaces and haptic interfaces in terms of their complexity. Next, we review guitarlike interfaces, and we design an interface to a virtual string.The excitation signal and pitch are sensed separately usingtwo independent string segments. These parameters controla two-axis digital waveguide virtual string, which modelsvibrations in the horizontal and vertical transverse axes aswell as the coupling between them. Finally, we consider theadvantages of using a multi-axis pickup for measuring theexcitation signal.</abstract>
    <keywords>physically motivated, physical, models, modeling, vibrating string, guitar, pitch detection, interface, excitation, coupled strings, haptic </keywords>
  </document>
  <document>
    <name>nime2008_303.pdf</name>
    <abstract>Being one of the earliest electronic instruments the basic principles of the Theremin have often been used to design new musical interfaces. We present the structured design and evaluation of a set of 3D interfaces for a virtual Theremin, the VRemin. The variants differ in the size of the interaction space, the interface complexity, and the applied IO devices. We conducted a formal evaluation based on the well-known AttrakDiff questionnaire for evaluating the hedonic and pragmatic quality of interactive products. The presented work is a first approach towards a participatory design process for musical interfaces that includes user evaluation at early design phases. </abstract>
  </document>
  <document>
    <name>nime2008_311.pdf</name>
    <keywords>Mapping, database, audiovisual, radio, installation art.  </keywords>
  </document>
  <document>
    <name>nime2008_315.pdf</name>
    <abstract>Monalisa is a software platform that enables to "see the sound, hear the image". It consists of three software: Monalisa Application, Monalisa-Audio Unit, and Monalisa-Image Unit, and an installation: Monalisa "shadow of the sound". In this paper, we describe the implementation of each software and installation with the explanation of the basic algorithms to treat the image data and the sound data transparently.</abstract>
    <keywords>Sound and Image Processing Software, Plug-in, Installation </keywords>
  </document>
  <document>
    <name>nime2008_319.pdf</name>
    <keywords>Automatic Accompaniment, Beat Tracking, Human-Computer Interaction, Musical Interface Evaluation </keywords>
  </document>
  <document>
    <name>nime2008_325.pdf</name>
    <abstract>In this paper, we present a pitch space based musical interface approach. A pitch space arranges tones in a way that meaningful tone combinations can be easily generated. Using a touch sensitive surface or a 3D-Joystick a player can move through the pitch space and create the desired sound by selecting tones. The more optimal the tones are geometrically arranged, the less control parameters are required to move through the space and to select the desired pitches. For this the quality of pitch space based musical interfaces depends on two factors: 1. the way how the tones are organized within the pitch space and 2. the way how the parameters of a given controller are used to move through the space and to select pitches. This paper presents a musical interface based on a tonal pitch space derived from a four dimensional model found by the music psychologists [11], [2]. The proposed pitch space particularly eases the creation of tonal harmonic music. Simultaneously it outlines music psychological and theoretical principles of music. </abstract>
    <keywords>Pitch space, musical interface, Carol L. Krumhansl, music psychology, music theory, western tonal music, 3D tonality model, spiral of thirds, 3D, Hardware controller, Symmetry model  </keywords>
  </document>
  <document>
    <name>nime2008_331.pdf</name>
    <abstract>We describe a system that can listen to a performance of Indian music and recognize the raag, the fundamental melodicframework that Indian classical musicians improvise within.In addition to determining the most likely raag being performed, the system displays the estimated the likelihoodof each of the other possible raags, visualizing the changesover time. The system computes the pitch-class distributionand uses a Bayesian decision rule to classify the resultingtwelve dimensional feature vector, where each feature represents the relative use of each pitch class. We show that thesystem achieves high performance on a variety of sources,making it a viable tool for interactive performance.</abstract>
  </document>
  <document>
    <name>nime2008_335.pdf</name>
    <abstract>A general CAC1-environment charged with physical-modelling capabilities is described. It combines CommonMusic,ODE and Fluxus in a modular way, making a powerful andflexible environment for experimenting with physical modelsin composition.Composition in this respect refers to the generation andmanipulation of structure typically on or above a note, phrase or voice-level. Compared to efforts in synthesisand performance little work has gone into applying physicalmodels to composition. Potentials in composition-applications are presumably large.The implementation of the physically equipped CAC-environment is described in detail.</abstract>
    <keywords>Physical Models in composition, CommonMusic, Musical mapping </keywords>
  </document>
  <document>
    <name>nime2008_339.pdf</name>
    <abstract>The Color of Waiting is an interactive theater workwith music, dance, and video which was developed atSTEIM in Amsterdam and further refined at CMMASin Morelia Mexico with funding from Meet theComposer. Using Max/MSP/ Jitter a cellist is able tocontrol sound and video during the performancewhile performing a structured improvisation inresponse to the dancer's movement. In order toensure. repeated performances of The Color o fWaiting , Kinesthetech Sense created the scorecontained in this paper. Performance is essential tothe practice of time-based art as a living form, buthas been complicated by the unique challenges ininterpretation and re-creation posed by worksincorporating technology. Creating a detailed scoreis one of the ways artists working with technologycan combat obsolescence.</abstract>
  </document>
  <document>
    <name>nime2008_345.pdf</name>
    <abstract>We developed a rhythmic instruments ensemble simulator generating animation using game controllers. The motion of a player is transformed into musical expression data of MIDI to generate sounds, and MIDI data are transformed into animation control parameters to generate movies. These animations and music are shown as the reflection of player performance. Multiple players can perform a musical ensemble to make more varied patterns of animation. Our system is so easy that everyone can enjoy performing a fusion of music and animation. </abstract>
    <keywords>Wii Remote, Wireless game controller, MIDI, Max/MSP, Flash movie, Gesture music and animation.  </keywords>
  </document>
  <document>
    <name>nime2008_347.pdf</name>
    <abstract>The demonstration of a series of properly weighted and balanced Bluetooth sensor bows for violin, viola, cello and bass. </abstract>
    <keywords>Sensor bow, stringed instruments, bluetooth  </keywords>
  </document>
  <document>
    <name>nime2008_349.pdf</name>
    <abstract>Plink Jet is a robotic musical instrument made from scavenged inkjet printers and guitar parts. We investigate the expressive capabilities of everyday machine technology by recontextualizing the relatively high-tech mechanisms of typical office debris into an electro-acoustic musical instrument. We also explore the performative relationship between human and machine.</abstract>
    <keywords>Interaction Design, Repurposing of Consumer Technology, DIY, Performing Technology, Robotics, Automation, Infra-Instrument  </keywords>
  </document>
  <document>
    <name>nime2008_352.pdf</name>
    <keywords>umbrella, musical expression, sound generating device, 3D sound system, sound-field arrangement. </keywords>
  </document>
  <document>
    <name>nime2008_360.pdf</name>
    <abstract>This research aims to develop a novel instrument for sociomusical interaction where a number of participants can produce sounds by feet in collaboration with each other. Thedeveloped instrument, beacon, is regarded as embodied soundmedia product that will provide an interactive environmentaround it. The beacon produces laser beams lying on theground and rotating. Audio sounds are then produced whenthe beams pass individual performer's foot. As the performers are able to control the pitch and sound length accordingto the foot location and angles facing the instrument, theperformer's body motion and foot behavior can be translated into sound and music in an intuitive manner.</abstract>
    <keywords>Embodied sound media, Hyper-instrument, Laser beams </keywords>
  </document>
  <document>
    <name>nime2008_362.pdf</name>
    <abstract>This paper describes the development of a wireless wearablecontroller, GO, for both sound processing and interactionwith wearable lights. Pure Data is used for sound processing.The GO prototype is built using a PIC microcontroller usingvarious sensors for receiving information from physicalmovements.</abstract>
    <keywords>Wireless controller, Pure Data, Gestural interface, Interactive Lights. </keywords>
  </document>
  <document>
    <name>nime2008_364.pdf</name>
    <keywords>Graphical Interface, Computer Game, MIDI Display </keywords>
  </document>
  <document>
    <name>nime2008_366.pdf</name>
    <abstract>This demonstration presents three new augmented and metasaxophone interface/instruments, built by the Bent LeatherBand. The instruments are designed for virtuosic liveperformance and make use of Sukandar Kartadinata's Gluion[OSC] interfaces. The project rationale and research outcomesfor the first twelve months is discussed. Instruments/interfacesdescribed include the Gluisop, Gluialto and Leathersop.</abstract>
    <keywords>Augmented saxophone, Gluion, OSC, virtuosic performance systems </keywords>
  </document>
  <document>
    <name>nime2008_372.pdf</name>
    <abstract>The Musical Synchrotron is a software interface that connects wireless motion sensors to a real-time interactive environment (Pure Data, Max/MSP). In addition to the measurement of movement, the system provides audio playback and visual feedback. The Musical Synchrotron outputs a score with the degree in which synchronization with the presented music is successful. The interface has been used to measure how people move in response to music. The system was used for experiments at public events. </abstract>
    <keywords>Wireless sensors, tempo perception, social interaction, music and movement, embodied music cognition </keywords>
  </document>
  <document>
    <name>nime2009_001.pdf</name>
    <abstract>This paper presents an evaluation and comparison of four input devices for percussion tasks: a standard tom drum, Roland V-Drum, and two established examples of gestural controllers: the Buchla Lightning II, and the Radio Baton. The primary goal of this study was to determine how players' actions changed when moving from an acoustic instrument like the tom drum, to a gestural controller like the Buchla Lightning, which bears little resemblance to an acoustic percussion instrument. Motion capture data was analyzed by comparing a subject's hand height variability and timing accuracy across the four instruments as they performed simple musical tasks. Results suggest that certain gestures such as hand height amplitude can be adapted to these gestural controllers with little change and that in general subjects' timing variability is significantly affected when playing on the Lightning and Radio Baton when compared to the more familiar tom drum and VDrum. Possible explanations and other observations are also presented. </abstract>
    <keywords>Evaluation of Input Devices, Motion Capture, Buchla Lightning II, Radio Baton.  </keywords>
  </document>
  <document>
    <name>nime2009_007.pdf</name>
    <abstract>Measurement of pianists' arm movement provides a signal,which is composed of controlled movements and noise. Thenoise is composed of uncontrolled movement generated bythe interaction of the arm with the piano action and measurement error. We propose a probabilistic model for armtouch movements, which allows to estimate the amount ofnoise in a joint. This estimation helps to interpret the movement signal, which is of interest for augmented piano andpiano pedagogy applications.</abstract>
    <keywords>Piano, arm movement, gesture, classification, augmented instrument, inertial sensing. </keywords>
  </document>
  <document>
    <name>nime2009_013.pdf</name>
    <abstract>This paper presents a HCI inspired evaluation of simple physical interfaces used to control physical models. Specifically knobs and sliders are compared in a creative and exploratory framework, which simulates the natural environment in which an electronic musician would normally explore a new instrument. No significant difference was measured between using knobs and sliders for controlling parameters of a physical modeling electronic instrument. Thereported difference between the tested instruments were mostlydue to the sound synthesis models.</abstract>
    <keywords>Evaluation, Interfaces, Sliders, Knobs, Physi- cal Modeling, Electronic Musicians, Exploration, Creativ- ity, Affordances. </keywords>
  </document>
  <document>
    <name>nime2009_019.pdf</name>
    <abstract>Haptic feedback is an important element that needs to be carefully designed in computer music interfaces. This paper presents an evaluation of several force renderings for target acquisition in space when used to support a music related task. The study presented here addresses only one musical aspect: the need to repeat elements accurately in time and in content. Several force scenarios will be rendered over a simple 3D target acquisition task and users' performance will be quantitatively and qualitatively evaluated. The results show how the users' subjective preference for a particular kind of force support does not always correlate to a quantitative measurement of performance enhancement. We describe a way in which a control mapping for a musical interface could be achieved without contradicting the users' preferences as obtained from the study. </abstract>
    <keywords>music interfaces, force feedback, tempo, comfort, target acquisition.  </keywords>
  </document>
  <document>
    <name>nime2009_025.pdf</name>
    <abstract>In this paper, we discuss a number of issues related to the design of evaluation tests for comparing interactive music systems for improvisation. Our testing procedure covers rehearsal and performance environments, and captures the experiences of a musician/participant as well as an audience member/observer. We attempt to isolate salient components of system behavior, and test whether the musician or audience are able to discern between systems with significantly different behavioral components. We report on our experiences with our testing methodology, in comparative studies of our London and ARHS improvisation systems [1]. </abstract>
    <keywords>Interactive music systems, human computer interaction, evaluation tests.  </keywords>
  </document>
  <document>
    <name>nime2009_031.pdf</name>
    <abstract>Motivated by previous work aimed at developing mathematical models to describe expressive timing in music, and specifically the final ritardandi, using measured kinematic data, we further investigate the linkage of locomotion and timing in music. The natural running behavior of four subjects is measured with a wearable sensor prototype and analyzed to create normalized tempo curves. The resulting curves are then used to modulate the final ritard of MIDI scores, which are also performed by an expert musician. A Turing-inspired listening test is conducted to observe a human listener's ability to determine the nature of the performer. </abstract>
    <keywords>Musical kinematics, expressive tempo,  machine music.  </keywords>
  </document>
  <document>
    <name>nime2009_033.pdf</name>
    <abstract>Vibetone is a musical input device which was build to explore tactile feedback in gesture based interaction. It is a prototype aimed to allow the performer to play both continuously and discrete pitched sounds in the same space. Our primary focus is on tactile feedback to guide the artist's movements during his performance. Thus, also untrained users are enabled to musical expression through bodily actions and precisely arm movements, guided through tactile feedback signals. </abstract>
    <keywords>tactile feedback, intuitive interaction, gestural interaction, MIDI controller  </keywords>
  </document>
  <document>
    <name>nime2009_035.pdf</name>
    <keywords>Sonification, Interactive Sonification, Auditory Display.  </keywords>
  </document>
  <document>
    <name>nime2009_037.pdf</name>
    <abstract>In this project we have developed reactive instruments for performance. Reactive instruments provide feedback for the performer thereby providing a more dynamic experience. This is achieved through the use of haptics and robotics. Haptics provide a feedback system to the control surface. Robotics provides a way to actuate the instruments and their control surfaces. This allows a highly coordinated "dance" between performer and the instrument. An application for this idea is presented as a linear slide interface. Reactive interfaces represent a dynamic way for music to be portrayed in performance. </abstract>
    <keywords>haptics, robotics, dynamic interfaces  </keywords>
  </document>
  <document>
    <name>nime2009_039.pdf</name>
    <abstract>Hands On Stage, designed from a percussionist's perspective, is a new performance interface designed for audiovisual improvisation. It comprises a custom-built table interface and a performance system programmed in two environments, SuperCollider 3 and Isadora. This paper traces the interface's evolution over matters of relevant technology, concept, construction, system design, and its creative outcomes. </abstract>
    <keywords>audiovisual, interface design, performance.  </keywords>
  </document>
  <document>
    <name>nime2009_041.pdf</name>
    <abstract>The Vibrobyte is a wireless haptic interface specialized forco-located musical performance. The hardware is designedaround the open source Arduino platform, with haptic control data encapsulated in OSC messages, and OSC/hardwarecommunications handled by Processing. The Vibrobyte wasfeatured at the International Computer Music Conference2008 (ICMC) in a telematic performance between ensembles in Belfast, Palo Alto (California, USA), and Troy (NewYork, USA).</abstract>
  </document>
  <document>
    <name>nime2009_043.pdf</name>
    <abstract>This paper describes a cost-effective, modular, open source framework for a laser interface design that is open to community development, interaction and user modification. The following paper highlights ways in which we are implementing the multi-laser gestural interface in musical, visual, and robotic contexts. </abstract>
    <keywords>Lasers, photocell sensor, UltraSound, Open Source controller design, digital gamelan, digital tanpura  </keywords>
  </document>
  <document>
    <name>nime2009_045.pdf</name>
    <keywords>Interaction, audience, performer, visualize, sen- sor, physical, gesture. </keywords>
  </document>
  <document>
    <name>nime2009_048.pdf</name>
    <abstract>This is intended to introduce the system, which combines BodySuit, especially Powered Suit, and Second Life, as well as its possibilities and its uses in a musical performance application. The system which we propose contains both a gesture controller and robots at the same time. In this system, the Data Suit, BodySuit controls the avatar in Second Life and Second Life controls the exoskeleton, Powered Suit in real time. These are related with each other in conjunction with Second Life in Internet. BodySuit doesn't contain a hand-held controller. A performer, for example a dancer, wears a suit. Gestures are transformed into electronic signals by sensors. Powered Suit is another suit that a dancer wears, but gestures are generated by motors. This is a sort of wearable robot. Second Life is software that is developed by Linden Lab. It allows creating a virtual world and a virtual human (avatar) in Internet. Working together with BodySuit, Powered Suit, and Second Life the idea behind the system is that a human body is augmented by electronic signals and is reflected in a virtual world in order to be able to perform interactively. </abstract>
  </document>
  <document>
    <name>nime2009_050.pdf</name>
    <abstract>We developed a system called Life Game Orchestra that generates music by translating cellular patterns of Conway's Game of Life into musical scales. A performer can compose music by controlling varying cell patterns and sounds with visual and auditory fun. A performer assigns the elements of tone to two-dimensional cell patterns in the matrix of the Game of Life. Our system searches defined cell patterns in the varying matrix dynamically. If the patterns are matched, corresponding tones are generated. A performer can make cells in the matrix by moving in front of a camera and interactively influencing the generation of music. The progress of the Game of Life is controlled with a clock defined by the performer to configure the groove of the music. By running multiple matrices with different pattern mapping, clock timing, and instruments, we can perform an ensemble. The Life Game Orchestra is a fusion system of the design of a performer and the emergence of cellular automata as a complex system. </abstract>
    <keywords>Conway's Game of Life, Cellular automata, Cell pattern, scale, Interactive composition, performance.  </keywords>
  </document>
  <document>
    <name>nime2009_052.pdf</name>
    <abstract>This article describes the implications of design and materials of computer controllers used in the context of interactive dance performance. Size, shape, and layout all influence audience perception of the performer, and materials imply context for further interpretation of the interactive performance work. It describes the construction of the "Control/Recorder" and the "VideoLyre," two custom computer control surfaces made for Leonardo's Chimes, a work by Toenjes, Marchant and Smith, and how these controllers contribute to theatrical aesthetic intent. </abstract>
    <keywords>control surface, interface, tactile, natural, organic, interactive dance.  </keywords>
  </document>
  <document>
    <name>nime2009_054.pdf</name>
    <abstract>Deviate generates multiple streams of melodic and rhythmic output in real-time, according to user-specified control parameters. This performance system has been implemented using Max 5 [1] within the genre of popular contemporary electronic music, incorporating techno, IDM, and related forms. The aim of this project is not musical style synthesis, but to construct an environment in which a range of creative and musical goals may be achieved. A key aspect is control over generative processes, as well as consistent yet varied output. An approach is described which frees the user from determining note-level output while allowing control to be maintained over larger structural details, focusing specifically on the melodic aspect of this system. Audio examples are located online at http://www.cetenbaath.com/cb/about-deviate/. </abstract>
    <keywords>generative, performance, laptop, popular music  </keywords>
  </document>
  <document>
    <name>nime2009_056.pdf</name>
    <abstract>SpiralSet is a sound toy incorporating game enginesoftware used in conjunction with a spectral synthesissound engine constructed in Max/MSP/Jitter. SpiralSetwas presented as an interactive installation piece at theSonic Arts Expo 2008, in Brighton, UK. A custom madesensor-based interface is used for control of the system.The user interactions are designed to be quickly accessiblein an installation context, yet allowing the potential forsonic depth and variation.</abstract>
    <keywords>Sound Toys, Game Engines, Animated Interfaces, Spectral Synthesis, Open Work, Max/MSP. </keywords>
  </document>
  <document>
    <name>nime2009_058.pdf</name>
    <abstract>This paper explores a rapidly developed, new musical interface involving a touch-screen, 32 pressure sensitive button pads, infrared sensor, 8 knobs and cross-fader. We provide a versatile platform for computer-based music performance and production using a human computer interface that has strong visual and tactile feedback as well as robust software that exploits the strengths of each individual system component. </abstract>
  </document>
  <document>
    <name>nime2009_060.pdf</name>
    <abstract>This paper presents the SARC EyesWeb Catalog (SEC), agroup of blocks designed for real-time gesture recognitionthat have been developed for the open source program EyesWeb. We describe how the recognition of real-time bodymovements can be used for musician-computer-interaction.</abstract>
    <keywords>SARC EyesWeb Catalog, gesture recognition </keywords>
  </document>
  <document>
    <name>nime2009_062.pdf</name>
    <abstract>We describe a new method for 2D fiducial tracking. We use region adjacency information together with angles between regions to encode IDs inside fiducials, whereas previous research by Kaltenbrunner and Bencina utilize region adjacency tree. Our method supports a wide ID range and is fast enough to accommodate real-time video. It is also very robust against false positive detection. </abstract>
    <keywords>fiducial tracking, computer vision, tangible user interface, interaction techniques.  </keywords>
  </document>
  <document>
    <name>nime2009_064.pdf</name>
    <abstract>During several decades, the research at Waseda University has been focused on developing anthropomorphic robots capable performing musical instruments. As a result of our research efforts, the Waseda Flutist Robot WF-4RIV and the Waseda Saxophonist Robot WAS-1 have been designed to reproduce the human player performance. As a long-term goal, we are proposing to enable the interaction between musical performance robots as well as with human players. In general the communication of humans within a band is a special case of conventional human social behavior. Rhythm, harmony and timbre of the music played represent the emotional states of the musicians. So the development of an artificial entity that participates in such an interaction may contribute to the better understanding of some of the mechanisms that enable the communication of humans in musical terms. Therefore, we are not considering a musical performance robot (MPR) just as a mere sophisticated MIDI instrument. Instead, its human-like design and the integration of perceptual capabilities may enable to act on its own autonomous initiative based on models which consider its own physical constrains. In this paper, we present an overview of our research approaches towards enabling the interaction between musical performance robots as well as with musicians. </abstract>
  </document>
  <document>
    <name>nime2009_070.pdf</name>
    <abstract>This paper presents an interactive and improvisational jam session, including human players and two robotic musicians. The project was developed in an effort to create novel and inspiring music through human-robot collaboration. The jam session incorporates Shimon, a newly-developed socially-interactive robotic marimba player, and Haile, a perceptual robotic percussionist developed in previous work. The paper gives an overview of the musical perception modules, adaptive improvisation modes and human-robot musical interaction models that were developed for the session. The paper also addresses the musical output that can be created from increased interconnections in an expanded multiple-robot multiplehuman ensemble, and suggests directions for future work. </abstract>
    <keywords>Robotic musicianship, Shimon, Haile.  </keywords>
  </document>
  <document>
    <name>nime2009_074.pdf</name>
    <abstract>In this project, we have developed a real-time writing instrument for music control. The controller, MusicGrip, can capture the subtle dynamics of the user's grip while writing or drawing and map this to musical control signals and sonic outputs. This paper discusses this conversion of the common motor motion of handwriting into an innovative form of music expression. The presented example instrument can be used to integrate the composing aspect of music with painting and writing, creating a new art form from the resultant aural and visual representation of the collaborative performing process. </abstract>
    <keywords>Interactive music control, writing instrument, pen controller, MIDI, group performing activity.  </keywords>
  </document>
  <document>
    <name>nime2009_078.pdf</name>
    <keywords>Tabletop computers, collaborative instruments, collaborative composition, group improvisation, spatial au- dio interfaces, customizable instruments. </keywords>
  </document>
  <document>
    <name>nime2009_082.pdf</name>
    <abstract>It is surely not difficult for anyone with experience in thesubject known as Music Theory to realize that there is avery definite and precise relationship between music andmathematics. This paper describes the SoriSu, a newelectronic musical instrument based on Sudoku puzzles,which probe the expressive possibilities of mathematicalconcepts in music. The concept proposes a new way ofmapping numbers to sound. This interface was designed toprovide easy and pleasing access to music for users whoare unfamiliar or uncomfortable with current musicaldevices. The motivation behind the project is presented, aswell as hardware and software design.</abstract>
    <keywords>Numbers, Game Interfaces, Mathematics and Sound, Mathematics in Music, Puzzles, Tangible User Interfaces. </keywords>
  </document>
  <document>
    <name>nime2009_090.pdf</name>
    <abstract>This paper presents a method for using a runner's pacefor real-time control of the time-scaling facility of a phasevocoder, resulting in the automated synchronization of anaudio track tempo to the generated control signal. The increase in usage of portable music players during exercisehas given rise to the development of new personal exerciseaids, most notably the Nike+iPod system, which relies onembedded sensor technologies to provide kinematic workout statistics. There are also systems that select songs basedon the measured step frequency of a runner. The proposedsystem also uses the pace of a runner, but this information isused to change the tempo of the music.</abstract>
    <keywords>NIME, synchronization, exercise, time-scaling. </keywords>
  </document>
  <document>
    <name>nime2009_098.pdf</name>
    <abstract> We present the Kalichord: a small, handheld electro/acoustic instrument in which the player's right hand plucks virtual strings while his left hand uses buttons to play independent bass lines. The Kalichord uses the analog signal from plucked acoustic tines to excite a physical string model, allowing a nuanced and intuitive plucking experience. First, we catalog instruments related to the Kalichord. Then we examine the use of analog signals to excite a physical string model and discuss the expressiveness and form factors that this technique affords. We then describe the overall construction of the Kalichord and possible playing styles, and finally we consider ways we hope to improve upon the current prototype. </abstract>
    <keywords>Kalichord, physical model, tine, piezo, plucked string, electro-acoustic instruments, kalimba, accordion </keywords>
  </document>
  <document>
    <name>nime2009_102.pdf</name>
    <abstract>In this paper we describe an approach for introducing newelectronic percussive sound possibilities for stringinstruments by "listening" to the sounds of the instrument'sbody and extracting audio and data from the wood'sacoustic vibrations. A method for capturing, localizing andanalyzing the percussive hits on the instrument's body ispresented, in connection with an audio-driven electronicpercussive sound module. The system introduces a newgesture-sound relationship in the electric string instrumentplaying environment, namely the use of percussivetechniques on the instrument's body which are null inregular circumstances due to selective and exclusivemicrophone use for the strings. Instrument bodypercussions are widely used in the acoustic instrumentalpraxis. They yield a strong potential for providing anextended soundscape via instrument augmentation, directlycontrolled by the musician through haptic manipulation ofthe instrument itself. The research work was carried out onthe electric guitar, but the method used can apply to anystring instrument with a resonating body.</abstract>
  </document>
  <document>
    <name>nime2009_106.pdf</name>
    <abstract>Large vibrating plates are used as thunder sheets in orchestras. We have extended the use of flat plates by cementing aflat panel electroacoustic transducer on a large brass sheet.Because of the thickness of the panel, the output is subject tononlinear distortion. When combined with a real-time inputand signal processing algorithm, the active brass plate canbecome an effective musical instrument for performance ofnew music.</abstract>
    <keywords>Electroacoustics, flat panel </keywords>
  </document>
  <document>
    <name>nime2009_110.pdf</name>
    <abstract>This paper gives a historical overview of the development of alternative sonic display systems at Princeton University; in particular, the design, construction, and use in live performance of a series of spherical and hemispherical speaker systems. We also provide a DIY guide to constructing the latest series of loudspeakers that we are currently using in our research and music making. </abstract>
    <keywords>loudspeakers, hemispherical speakers, sonic display systems, laptop orchestras.  </keywords>
  </document>
  <document>
    <name>nime2009_116.pdf</name>
    <abstract>The history and future of Open Sound Control (OSC) is discussed and the next iteration of the OSC specification is introduced with discussion of new features to support NIME community activities. The roadmap to a major revision of OSC is developed. </abstract>
    <keywords>Open Sound Control, Time Tag, OSC, Reservation Protocols.  </keywords>
  </document>
  <document>
    <name>nime2009_121.pdf</name>
    <abstract>An on-the-fly reconfigurable low-level embedded servicearchitecture is presented as a means to improve scalability, improve conceptual comprehensibility, reduce humanerror and reduce development time when designing newsensor-based electronic musical instruments with real-timeresponsiveness. The implementation of the concept ina project called micro-OSC is described. Other sensorinterfacing products are evaluated in the context of DIYprototyping of musical instruments. The capabilities ofthe micro-OSC platform are demonstrated through a set ofexamples including resistive sensing, mixed digital-analogsystems, many-channel sensor interfaces and time-basedmeasurement methods.</abstract>
    <keywords>real-time musical interface, DIY design, em- bedded web services, rapid prototyping, reconfigurable firmware </keywords>
  </document>
  <document>
    <name>nime2009_125.pdf</name>
    <abstract>Firmata is a generic protocol for communicating with microcontrollers from software on a host computer. The central goal is to make the microcontroller an extension of theprogramming environment on the host computer in a manner that feels natural in that programming environment. Itwas designed to be open and flexible so that any programming environment can support it, and simple to implementboth on the microcontroller and the host computer to ensurea wide range of implementations. The current reference implementation is a library for Arduino/Wiring and is includedwith Arduino software package since version 0012. Thereare matching software modules for a number of languages,like Pd, OpenFrameworks, Max/MSP, and Processing.</abstract>
  </document>
  <document>
    <name>nime2009_131.pdf</name>
    <keywords>Data exchange, collaborative performance, in- teractive performance, interactive art works, sensor data, Open- SoundControl, SuperCollider, Max/MSP Introduction and Background The SenseWorld Data Network addresses one of the major challenges in the research/creation of interactive live per- formance work the sharing and manipulation of raw and/or conditioned sensor data among different media systems (real time audio and video, lighting, mechatronics, show control, etc). While the </keywords>
  </document>
  <document>
    <name>nime2009_135.pdf</name>
    <abstract>Low-latency streaming of high-quality audio has the potential to dramatically transform the world of interactive musical applications. We provide methods for accurately measuring the end-to-end latency and audio quality of a delivered audio stream and apply these methods to an empirical evaluation of several streaming engines. In anticipationof future demands for emerging applications involving audio interaction, we also review key features of streamingengines and discuss potential challenges that remain to beovercome.</abstract>
    <keywords>Networked Musical Performance, high-fidelity audio streaming, glitch detection, latency measurement </keywords>
  </document>
  <document>
    <name>nime2009_141.pdf</name>
    <abstract>"Extension du corps sonore" is long-term project initiatedby Musiques Nouvelles [4], a contemporary music ensemble in Mons. It aims at giving instrumental music performers an extended control over the sound of their instrument byextending the understanding of the sound body from the instrument only to the combination of the instrument and thewhole body of the performer. The development started atARTeM and got the benefit of a three month numediartresearch project [1] that focused on three axes of research:pre-processing of sensor data, gesture recognition and mapping through interpolation. The objectives were the development of computing methods and flexible Max/MSP externals to be later integrated in the ARTeM software framework for the concerts with viola player Dominica Eyckmans. They could be used in a variety of other artistic worksand will be made available on the numediart website [1],where more detailed information can be found in the Quarterly Progress Scientific Report #4.</abstract>
    <keywords>Sensor data pre-processing, gesture recognition, mapping, interpolation, extension du corps sonore </keywords>
  </document>
  <document>
    <name>nime2009_147.pdf</name>
    <abstract>We describe initial prototypes and a design strategy for new, user-customized audio-manipulation and editing tools. These tools are designed to enable intuitive control of audio-processing tasks while anthropomorphically matching the target user. </abstract>
    <keywords>user modeling, user customization  </keywords>
  </document>
  <document>
    <name>nime2009_151.pdf</name>
    <abstract>In this poster we present the early prototype of the augmented Psychophone - a saxophone with various applied sensors, allowing the saxophone player to attach effects like pitch shifting, wah-wah and ring modulation to the saxophone, simply by moving the saxophone as one would do when really being enthusiastic and involved in the performance. The possibility of scratching on the previously recorded sound is also possible directly on the saxophone.  </abstract>
    <keywords>Augmented saxophone, Physical computing, hyper instruments, mapping. </keywords>
  </document>
  <document>
    <name>nime2009_153.pdf</name>
    <abstract>Catch Your Breath is an interactive audiovisual bio-feedbacksystem adapted from a project designed to reduce respiratory irregularity in patients undergoing 4D CT scans for oncological diagnosis. The system is currently implementedand assessed as a potential means to reduce motion-induceddistortion in CT images.A museum installation based on the same principle wascreated in which an inexpensive wall-mounted web camera tracks an IR sensor embedded into a pendant worn bythe user. The motion of the subjects breathing is trackedand interpreted as a real-time variable tempo adjustment toa stored musical file. The subject can then adjust his/herbreathing to synchronize with a separate accompanimentline. When the breathing is regular and is at the desiredtempo, the audible result sounds synchronous and harmonious. The accompaniment's tempo progresses and gradually decrease which causes the breathing to synchronize andslow down, thus increasing relaxation.</abstract>
    <keywords>sensor, music, auditory display. </keywords>
  </document>
  <document>
    <name>nime2009_155.pdf</name>
    <abstract>With the increase of sales of Wii game consoles, it is becoming commonplace for the Wii remote to be used as analternative input device for other computer systems. In thispaper, we present a system which makes use of the infraredcamera within the Wii remote to capture the gestures of aconductor using a baton with an infrared LED and battery.Our system then performs data analysis with gesture classification and following, and finally displays the gestures using visual baton trajectories and audio feedback. Gesturetrajectories are displayed in real time and can be comparedto the corresponding diagram shown in a textbook. In addition, since a conductor normally does not look at a screenwhile conducting, tones are played to represent a certainbeat in a conducting gesture. Further, the system can be controlled entirely with the baton, removing the need to switchfrom baton to mouse. The interface is intended to be usedfor pedagogy purposes.</abstract>
    <keywords>Conducting, Gesture, Infrared, Learning, Wii. </keywords>
  </document>
  <document>
    <name>nime2009_157.pdf</name>
    <abstract>"Music for 32 Chess Pieces" is a software system that supports composing, performing and improvising music by playing a chess game. A game server stores a representation of the state of a game, validates proposed moves by players, updates game state, and extracts a graph of piece-to-piece relationships. It also loads a plugin code module that acts as a composition. A plugin maps pieces and relationships on the board, such as support or attack relationships, to a timed sequence of notes and accents. The server transmits notes in a sequence to an audio renderer process via network datagrams. Two players can perform a composition by playing chess, and a player can improvise by adjusting a plugin's music mapping parameters via a graphical user interface. A composer can create a new composition by writing a new plugin that uses a distinct algorithm for mapping game rules and states to music. A composer can also write a new note-to-sound mapping program in the audio renderer language. This software is available at http://faculty.kutztown.edu/parson/music/ParsonMusic.html. </abstract>
    <keywords>algorithmic composition, chess, ChucK, improvisation, Max/MSP, SuperCollider.  </keywords>
  </document>
  <document>
    <name>nime2009_159.pdf</name>
    <abstract>This paper reports on work in progress on the creativeproject MagNular, part of a wider practical study of thepotential collaborative compositional applications of gameengine technologies. MagNular is a sound toy utilizingcomputer game and physics engine technologies to createan animated interface used in conjunction with an externalsound engine developed within Max/MSP. The playercontrols virtual magnets that attract or repel numerousparticle objects, moving them freely around the virtualspace. Particle object collision data is mapped to controlsound onsets and synthesis/DSP (Digital SignalProcessing) parameters. The user "composes" bycontrolling and influencing the simulated physicalbehaviors of the particle objects within the animatedinterface.</abstract>
    <keywords>Sound Toys, Open Work, Game Engines, Animated Interfaces, Max/MSP. </keywords>
  </document>
  <document>
    <name>nime2009_161.pdf</name>
    <abstract>AUDIO ORIENTEERING is a collaborative performance environment in which physical tokens are used to navigate an invisible sonic landscape. In this paper, I describe the hardware and software used to implement a prototype audio terrain with multiple interaction modes and sonic behaviors mapped onto three-dimensional space. </abstract>
    <keywords>wii, 3-d positioning, audio terrain,  collaborative performance.  </keywords>
  </document>
  <document>
    <name>nime2009_163.pdf</name>
    <abstract>This paper presents developments in the technology underlying the cyclotactor, a finger-based tactile I/O device for musical interaction. These include significant improvements both in the basic characteristics of tactile interaction and in the related (vibro)tactile sample rates, latencies, and timing precision. After presenting the new prototype's tactile output force landscape, some of the new possibilities for interaction are discussed, especially those for musical interaction with zero audio/tactile latency.</abstract>
    <keywords>Musical controller, tactile interface. </keywords>
  </document>
  <document>
    <name>nime2009_165.pdf</name>
    <abstract>A MIDI-to-OSC converter is implemented on a commercially available embedded linux system, tighly integratedwith a microcontroller. A layered method is developed whichpermits the conversion of serial data such as MIDI to OSCformatted network packets with an overall system latencybelow 5 milliseconds for common MIDI messages.The Gumstix embedded computer provide an interesting and modular platform for the development of such anembedded applications. The project shows great potentialto evolve into a generic sensors-to-OSC ethernet converterwhich should be very useful for artistic purposes and couldbe used as a fast prototyping interface for gesture acquisitiondevices.</abstract>
    <keywords>MIDI, Open Sound Control, converter, gumstix </keywords>
  </document>
  <document>
    <name>nime2009_169.pdf</name>
    <abstract>This is a technical and experimental report of parallel processing, using the "Propeller" chip. Its eight 32 bits processors (cogs) can operate simultaneously, either independently or cooperatively, sharing common resources through a central hub. I introduce this unique processor and discuss about the possibility to develop interactive systems and smart interfaces in media arts, because we need many kinds of tasks at a same time with NIMErelated systems and installations. I will report about (1) Propeller chip and its powerful IDE, (2) external interfaces for analog/digital inputs/outputs, (3) VGA/NTSC/PAL video generation, (4) audio signal processing, and (5) originally-developed MIDI input/output method. I also introduce three experimental prototype systems.</abstract>
    <keywords>Propeller, parallel processing, MIDI, sensor, interfaces. </keywords>
  </document>
  <document>
    <name>nime2009_171.pdf</name>
    <abstract>The development of new interfaces for musical expressionhas created a need to study how spectators comprehend newperformance technologies and practices. As part of a largerproject examining how interactions with technology can becommunicated with the spectator, we relate our model ofspectator understanding of error to the NIME discourse surrounding transparency, mapping, skill and success.</abstract>
    <keywords>performance, skill, transparency, design, HCI </keywords>
  </document>
  <document>
    <name>nime2009_173.pdf</name>
    <abstract>In this paper we present new issues and challenges relatedto the vertical tablet playing. The approach is based on apreviously presented instrument, the HANDSKETCH. Thisinstrument has now been played regularly for more than twoyears by several performers. Therefore this is an opportunityto propose a better understanding of the performing strategy.We present the behavior of the whole body as an underlyingaspect in the manipulation of the instrument.</abstract>
    <keywords>graphic tablet, playing position, techniques </keywords>
  </document>
  <document>
    <name>nime2009_177.pdf</name>
    <abstract>Haptic technology, providing force cues and creating a programmable physical instrument interface, can assist musicians in making gestures. The finite reaction time of thehuman motor control system implies that the execution of abrief musical gesture does not rely on immediate feedbackfrom the senses, rather it is preprogrammed to some degree.Consequently, we suggest designing relatively simple anddeterministic interfaces for providing haptic assistance.In this paper, we consider the specific problem of assisting a musician in selecting pitches from a continuous range.We build on a prior study by O'Modhrain of the accuracyof pitches selected by musicians on a Theremin-like hapticinterface. To improve the assistance, we augment the interface with programmed detents so that the musician can feelthe locations of equal tempered pitches. Nevertheless, themusician can still perform arbitrary pitch inflections such asglissandi, falls, and scoops. We investigate various formsof haptic detents, including fixed detent levels and forcesensitive detent levels. Preliminary results from a subjecttest confirm improved accuracy in pitch selection broughtabout by detents.</abstract>
    <keywords>Haptic, detent, pitch selection, human motor system, feedback control, response time, gravity well </keywords>
  </document>
  <document>
    <name>nime2009_183.pdf</name>
    <abstract>A haptic musical instrument is an electronic musical instrument that provides the musician not only with audio feedback but also with force feedback. By programming feedback controllers to emulate the laws of physics, many haptic musical instruments have been previously designed thatmimic real acoustic musical instruments. The controllerprograms have been implemented using finite difference and(approximate) hybrid digital waveguide models. We presenta novel method for constructing haptic musical instrumentsin which a haptic device is directly interfaced with a conventional digital waveguide model by way of a junction element, improving the quality of the musician's interactionwith the virtual instrument. We introduce both the explicitdigital waveguide control junction and the implicit digitalwaveguide control junction.</abstract>
    <keywords>haptic musical instrument, digital waveguide, control junction, explicit, implicit, teleoperation </keywords>
  </document>
  <document>
    <name>nime2009_187.pdf</name>
    <abstract>The carillon is one of the few instruments that elicit sophisticated haptic interaction from amateur and professional players alike. Like the piano keyboard, the velocity of a player's impact on each carillon key, or baton, affects the quality of the resultant tone; unlike the piano, each carillon baton returns a different forcefeedback. Force-feedback varies widely from one baton to the next across the entire range of the instrument and with further idiosyncratic variation from one instrument to another. This makes the carillon an ideal candidate for haptic simulation. The application of synthesized forcefeedback based on an analysis of forces operating in a typical carillon mechanism offers a blueprint for the design of an electronic practice clavier and with it the solution to a problem that has vexed carillonists for centuries, namely the inability to rehearse repertoire in private. This paper will focus on design and implementation of a haptic carillon clavier derived from an analysis of the Australian National Carillon in Canberra. </abstract>
    <keywords>Haptics, force-feedback, mechanical analysis.  </keywords>
  </document>
  <document>
    <name>nime2009_193.pdf</name>
    <abstract>The Electrumpet is an enhancement of a normal trumpet with a variety of electronic sensors and buttons. It is a new hybrid instrument that facilitates simultaneous acoustic and electronic playing. The normal playing skills of a trumpet player apply to the new instrument. The placing of the buttons and sensors is not a hindrance to acoustic use of the instrument and they are conveniently located. The device can be easily attached to and detached from a normal Bb-trumpet. The device has a wireless connection with the computer through Bluetooth-serial (Arduino). Audio and data processing in the computer is effected by three separate instances of MAX/MSP connected through OSC (controller data) and Soundflower (sound data). The current prototype consists of 7 analogue sensors (4 valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" controller) and 9 digital switches. An LCD screen that is controlled by a separate Arduino (mini) is attached to the trumpet and displays the current controller settings that are sent through a serial connection. </abstract>
    <keywords>Trumpet, multiple Arduinos, Bluetooth, LCD, low latency, OSC, MAX/MSP.  </keywords>
  </document>
  <document>
    <name>nime2009_199.pdf</name>
    <keywords>Controller, Sensor, MIDI, USB, Computer Music, ribbon controllers, ribbon cello.  </keywords>
  </document>
  <document>
    <name>nime2009_203.pdf</name>
    <keywords>sensor, gestural, technology, performance, piano, motors, interactive  </keywords>
  </document>
  <document>
    <name>nime2009_207.pdf</name>
    <abstract>In this paper we describe an interaction framework whichclassifies musicians' interactions with virtual musical instruments into three modes: instrumental, ornamental andconversational. We argue that conversational interactionsare the most difficult to design for, but also the most interesting. To illustrate our approach to designing for conversational interactions we describe the performance workPartial Reflections 3 for two clarinets and interactive software. This software uses simulated physical models to create a virtual sound sculpture which both responds to andproduces sounds and visuals.</abstract>
    <keywords>Music, instruments, interaction. </keywords>
  </document>
  <document>
    <name>nime2009_213.pdf</name>
    <abstract>In this paper we discuss the concept of style, focusing in particular on methods of designing new instruments that facilitate the cultivation and recognition of style. We distinguishbetween style and structure of an interaction and discuss thesignificance of this formulation within the context of NIME.Two workshops that were conducted to explore style in interaction design are described, from which we identify elements of style that can inform and influence the design process. From these, we suggest steps toward designing forstyle in new musical interactions.</abstract>
    <keywords>expression, style, structure, skill, virtuosity </keywords>
  </document>
  <document>
    <name>nime2009_222.pdf</name>
    <abstract>This paper reports on initial stages of research leading to the development of an intermedia performance Counterlines - a duet for Disklavier and Wacom Cintiq, in which both performers generate audiovisual gestures that relate to each other contrapuntally. The pianist generates graphic elements while playing music and the graphic performer generates piano notes by drawing lines. The paper focuses on interfacing sounds and images performed by the pianist. It provides rationale for the choice of materials of great simplicity and describes our approach to mapping. </abstract>
    <keywords>intermedia, Disklavier, piano, Wacom Cintiq, mapping, visual music  </keywords>
  </document>
  <document>
    <name>nime2009_226.pdf</name>
    <abstract>Music composition on computer is a challenging task, involving a range of data types to be managed within a single software tool. A composition typically comprises a complex arrangement of material, with many internal relationships between data in different locations repetition, inversion, retrograde, reversal and more sophisticated transformations. The creation of such complex artefacts is labour intensive, and current systems typically place a significant cognitive burden on the composer in terms of maintaining a work as a coherent whole. FrameWorks 3D is an attempt to improve support for composition tasks within a Digital Audio Workstation (DAW) style environment via a novel three-dimensional (3D) user-interface. In addition to the standard paradigm of tracks, regions and tape recording analogy, FrameWorks displays hierarchical and transformational information in a single, fully navigable workspace. The implementation combines Java with Max/MSP to create a cross-platform, user-extensible package and will be used to assess the viability of such a tool and to develop the ideas further. </abstract>
    <keywords>Digital Audio Workstation, graphical user- interfaces, 3D graphics, Max/MSP, Java.  </keywords>
  </document>
  <document>
    <name>nime2009_230.pdf</name>
    <abstract>A compendium of foundational circuits for interfacing resistive pressure and position sensors is presented with example applications for music controllers and tangible interfaces. </abstract>
    <keywords>Piezoresistive Touch Sensor Pressure Sensing Current Steering Multitouch.  </keywords>
  </document>
  <document>
    <name>nime2009_236.pdf</name>
    <abstract>This paper presents a new force-sensitive surface designedfor playing music. A prototype system has been implemented using a passive capacitive sensor, a commodity multichannel audio interface, and decoding software running ona laptop computer. This setup has been a successful, lowcost route to a number of experiments in intimate musicalcontrol.</abstract>
    <keywords>Multitouch, sensors, tactile, capacitive, percus- sion controllers. </keywords>
  </document>
  <document>
    <name>nime2009_242.pdf</name>
    <abstract>This paper introduces a flexible mapping editor, which transforms multi-touch devices into musical instruments. The editor enables users to create interfaces by dragging and dropping components onto the interface and attaching actions to them, which will be executed when certain userdefined conditions obtain. The editor receives touch information via the non-proprietary communication protocol, TUIO [9], and can, therefore, be used together with a variety of different multi-touch input devices. </abstract>
    <keywords>NIME, multi-touch, multi-modal interface, sonic interaction design.  </keywords>
  </document>
  <document>
    <name>nime2009_250.pdf</name>
    <abstract>The UBS Virtual Maestro is an interactive conducting system designed by Immersion Music to simulate the experience of orchestral conducting for the general public attending a classical music concert. The system utilizes the Wii Remote, which users hold and move like a conducting baton to affect the tempo and dynamics of an orchestral video/audio recording. The accelerometer data from the Wii Remote is used to control playback speed and volume in real-time. The system is housed in a UBSbranded kiosk that has toured classical performing arts venues throughout the United States and Europe in 2007 and 2008. In this paper we share our experiences in designing this standalone system for thousands of users, and lessons that we learned from the project. </abstract>
    <keywords>conducting, gesture, interactive installations, Wii Remote  </keywords>
  </document>
  <document>
    <name>nime2009_256.pdf</name>
    <abstract>This paper describes The Vocal Augmentation and Manipulation Prosthesis (VAMP) a gesture-based wearable controller for live-time vocal performance. This controller allows a singer to capture and manipulate single notes that he or she sings, using a gestural vocabulary developed from that of choral conducting. By drawing from a familiar gestural vocabulary, this controller and the associated mappings can be more intuitive and expressive for both performer and audience. </abstract>
    <keywords>musical expressivity, vocal performance,  gestural control, conducting.  </keywords>
  </document>
  <document>
    <name>nime2009_264.pdf</name>
    <abstract>This paper introduces the new audiovisual sequencing system "Versum" that allows users to compose in three dimensions. In the present paper the conceptual soil from which this system has sprung is discussed first. Secondly, the basic concepts with which Versum operates are explained, providing a general idea of what is meant by sequencing in three dimensions and explaining what compositions made in Versum can look and sound like. Thirdly, the practical ways in which a composer can use Versum to make his own audiovisual compositions are presented by means of a more detailed description of the different graphical user interface elements. Fourthly, a short description is given of the modular structure of the software underlying Versum. Finally, several foresights regarding the directions in which Versum will continue to develop in the near future are presented. </abstract>
    <keywords>audiovisual, sequencing, collaboration.  </keywords>
  </document>
  <document>
    <name>nime2009_266.pdf</name>
    <abstract>In this paper we describe findings related to user interfacerequirements for live electronic music arising from researchconducted as part of the first three-year phase of the EUfunded Integra project. A number of graphical user interface(GUI) prototypes developed during the Integra project initial phase are described and conclusions drawn about theirdesign and implementation.</abstract>
    <keywords>Integra, User Interface, Usability, Design, Live Electronics, Music Technology </keywords>
  </document>
  <document>
    <name>nime2009_268.pdf</name>
    <keywords>Interactive music instruments, visual interfaces, visual feedback, tangible interfaces, augmented reality, collaborative music, networked musical instruments, real-time musical systems, musical sequencer.  </keywords>
  </document>
  <document>
    <name>nime2009_270.pdf</name>
  </document>
  <document>
    <name>nime2009_276.pdf</name>
    <abstract>Phonetic symbols describe movements of the vocal tract,tongue and lips, and are combined into complex movementsforming the words of language. In music, vocables are wordsthat describe musical sounds, by relating vocal movementsto articulations of a musical instrument. We posit that vocable words allow the composers and listeners to engageclosely with dimensions of timbre, and that vocables couldsee greater use in electronic music interfaces. A preliminarysystem for controlling percussive physical modelling synthesis with textual words is introduced, with particular application in expressive specification of timbre during computer music performances.</abstract>
  </document>
  <document>
    <name>nime2009_280.pdf</name>
    <abstract>Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance. </abstract>
    <keywords>Machine learning, mapping, tools.  </keywords>
  </document>
  <document>
    <name>nime2009_286.pdf</name>
    <abstract>In this paper mappings and adaptation in the context of interactive sound installations are discussed. Starting from an ecological perspective on non-expert audience interaction a brief overview and discussion of mapping strategies with a special focus on adaptive systems using machine learning algorithms is given. An audio-visual interactive installation is analyzed and its implementation used to illustrate the issues of audience engagement and to discuss the efficiency of adaptive mappings. </abstract>
    <keywords>Interaction, adaptive mapping, machine learning, audience engagement </keywords>
  </document>
  <document>
    <name>nime2009_297.pdf</name>
    <abstract>The Fragmented Orchestra is a distributed musical instrument which combines live audio streams from geographically disparate sites, and granulates each according to thespike timings of an artificial spiking neural network. Thispaper introduces the work, outlining its historical context,technical architecture, neuronal model and network infrastructure, making specific reference to modes of interactionwith the public.</abstract>
  </document>
  <document>
    <name>nime2009_303.pdf</name>
    <abstract>The Smule Ocarina is a wind instrument designed for the iPhone, fully leveraging its wide array of technologies: microphone input (for breath input), multitouch (for fingering), accelerometer, real-time sound synthesis, highperformance graphics, GPS/location, and persistent data connection. In this mobile musical artifact, the interactions of the ancient flute-like instrument are both preserved and transformed via breath-control and multitouch finger-holes, while the onboard global positioning and persistent data connection provide the opportunity to create a new social experience, allowing the users of Ocarina to listen to one another. In this way, Ocarina is also a type of social instrument that enables a different, perhaps even magical, sense of global connectivity. </abstract>
  </document>
  <document>
    <name>nime2009_308.pdf</name>
    <abstract>This paper presents "Scratch-Off", a new musical multiplayer DJ game that has been designed for a mobile phone. We describe how the game is used as a test platform for experimenting with various types of multimodal feedback. The game uses movement gestures made by the players to scratch a record and control crossfades between tracks, with the objective of the game to make the correct scratch at the correct time in relation to the music. Gestures are detected using the devices built-in tri-axis accelerometer and multi-touch screen display. The players receive visual, audio and various types of vibrotactile feedback to help them make the correct scratch on the beat of the music track. We also discuss the results of a pilot study using this interface. </abstract>
    <keywords>Mobile devices, gesture, audio games.  </keywords>
  </document>
  <document>
    <name>nime2009_312.pdf</name>
    <abstract>ZooZBeat is a gesture-based mobile music studio. It is designed to provide users with expressive and creative access to music making on the go. ZooZBeat users shake the phone or tap the screen to enter notes. The result is quantized, mapped onto a musical scale, and looped. Users can then use tilt and shake movements to manipulate and share their creation in a group. Emphasis is placed on finding intuitive metaphors for mobile music creation and maintaining a balance between control and ease-of-use that allows non-musicians to begin creating music with the application immediately. </abstract>
    <keywords>mobile music, gestural control  </keywords>
  </document>
  <document>
    <name>nime2009_316.pdf</name>
    <abstract>It has been shown that collaborative musical interfaces encourage novice users to explore the sound space and promote their participation as music performers. Nevertheless, such interfaces are generally physically situated and can limit the possibility of movements on the stage, a critical factor in live music performance. In this paper we introduce the Drummer, a networked digital musical interface that allows multiple performers to design and play drum kits simultaneously while, at the same time, keeping their ability to freely move on the stage. The system consists of multiple Nintendo DS clients with an intuitive, user-configurable interface and a server computer which plays drum sounds. The Drummer Machine, a small piece of hardware to augment the performance of the Drummer, is also introduced. </abstract>
  </document>
  <document>
    <name>nime2010_001.pdf</name>
    <abstract>The aim of this paper is to define the process of iterative interface design as it pertains to musical performance. Embodying this design approach, the Monome OSC/MIDI USB controller represents a minimalist, open-source hardware device. The open-source nature of the device has allowed for a small group of Monome users to modify the hardware, firmware, and software associated with the interface. These user driven modifications have allowed the re-imagining of the interface for new and novel purposes, beyond even that of the device's original intentions. With development being driven by a community of users, a device can become several related but unique generations of musical controllers, each one focused on a specific set of needs. </abstract>
    <keywords>Iterative Design, Monome, Arduinome, Arduino.  </keywords>
  </document>
  <document>
    <name>nime2010_007.pdf</name>
    <keywords>Musical instruments, Script language </keywords>
  </document>
  <document>
    <name>nime2010_013.pdf</name>
  </document>
  <document>
    <name>nime2010_019.pdf</name>
    <abstract>This paper, describes the second phase of an ongoing research project dealing with the implementation of an interactive interface. It is a "hands free" instrument, utilizing a non-contact tactile feedback method based on airborne ultrasound. The three main elements/components of the interface that will be discussed in this paper are: 1. Generation of audible sound by self-demodulation of an ultrasound signal during its propagation through air; 2. The condensation of the ultrasound energy in one spatial point generating a precise tactile reproduction of the audible sound; and 3. The feed-forward method enabling a real-time intervention of the musician, by shaping the tactile (ultra)sound directly with his hands.</abstract>
    <keywords>haptics, vibro-tactility, feedback, ultrasound, hands-free interface, nonlinear acoustics, parametric array. </keywords>
  </document>
  <document>
    <name>nime2010_023.pdf</name>
    <abstract>The Neurohedron is a multi-modal interface for a nonlinear sequencer software model, embodied physically in a dodecahedron. The faces of the dodecahedron are both inputs and outputs, allowing the device to visualize the activity of the software model as well as convey input to it. The software model maps MIDI notes to the faces of the device, and defines and controls the behavior of the sequencer's progression around its surface, resulting in a unique instrument for computer-based performance and composition. </abstract>
  </document>
  <document>
    <name>nime2010_026.pdf</name>
    <abstract>We introduce an interactive interface for the custom designof metallophones. The shape of each plate must be determined in the design process so that the metallophone willproduce the proper tone when struck with a mallet. Unfortunately, the relationship between plate shape and tone iscomplex, which makes it difficult to design plates with arbitrary shapes. Our system addresses this problem by runninga concurrent numerical eigenanalysis during interactive geometry editing. It continuously presents a predicted tone tothe user with both visual and audio feedback, thus makingit possible to design a plate with any desired shape and tone.We developed this system to demonstrate the effectivenessof integrating real-time finite element method analysis intogeometric editing to facilitate the design of custom-mademusical instruments. An informal study demonstrated theability of technically unsophisticated user to apply the system to complex metallophone design.</abstract>
    <keywords>Modeling - Modeling Interfaces, Modeling - Geometric Mod- eling, Modeling - CAD, Methods and Applications - Edu- cation, Real-time FEM </keywords>
  </document>
  <document>
    <name>nime2010_031.pdf</name>
    <abstract>The field of mixed-reality interface design is relatively young and in regards to music, has not been explored in great depth. Using computer vision and collision detection techniques, Freepad further explores the development of mixed-reality interfaces for music. The result is an accessible user-definable MIDI interface for anyone with a webcam, pen and paper, which outputs MIDI notes with velocity values based on the speed of the strikes on drawn pads. </abstract>
    <keywords>Computer vision, form recognition, collision detection, mixed- reality, custom interface, MIDI  </keywords>
  </document>
  <document>
    <name>nime2010_037.pdf</name>
    <abstract>Our team realized that a need existed for a music programming interface in the Minim audio library of the Processingprogramming environment. The audience for this new interface would be the novice programmer interested in usingmusic as part of the learning experience, though the interface should also be complex enough to benefit experiencedartist-programmers. We collected many ideas from currently available music programming languages and librariesto design and create the new capabilities in Minim. Thebasic mechanisms include chained unit generators, instruments, and notes. In general, one "patches" unit generators(for example, oscillators, delays, and envelopes) together inorder to create synthesis algorithms. These algorithms canthen either create continuous sound, or be used in instruments to play notes with specific start time and duration.We have written a base set of unit generators to enablea wide variety of synthesis options, and the capabilities ofthe unit generators, instruments, and Processing allow fora wide range of composition techniques.</abstract>
    <keywords>Minim, music programming, audio library, Processing, mu- sic software </keywords>
  </document>
  <document>
    <name>nime2010_043.pdf</name>
    <abstract>The analysis of digital music systems has traditionally been characterized by an approach that can be defined as phenomenological. The focus has been on the body and its relationship to the machine, often neglecting the system's conceptual design. This paper brings into focus the epistemic features of digital systems, which implies emphasizing the cognitive, conceptual and music theoretical side of our musical instruments. An epistemic dimension space for the analysis of musical devices is proposed. </abstract>
    <keywords>Epistemic tools, music theory, dimension space, analysis.  </keywords>
  </document>
  <document>
    <name>nime2010_047.pdf</name>
    <abstract>Human agency, our capacity for action, has been at the hub of discussions centring upon philosophical enquiry for a long period of time. Sensory supplementation devices can provide us with unique opportunities to investigate the different aspects of our agency by enabling new modes of perception and facilitating the emergence of novel interactions, all of which is impossible without the aforesaid devices. Our preliminary study investigates the non-verbal strategies employed for negotiation of our capacity for action with other bodies and the surrounding space through body-to-body and body-to-space couplings enabled by sensory supplementation devices. We employed a lowfi rapid prototyping approach to build this device, enabling distal perception by sonic and haptic feedback. Further, we conducted a workshop in which participants equipped with this device engaged in game-like activities. </abstract>
    <keywords>Human agency, sensory supplementation, distal perception,  sonic feedback, tactile feedback, enactive interfaces  </keywords>
  </document>
  <document>
    <name>nime2010_051.pdf</name>
  </document>
  <document>
    <name>nime2010_057.pdf</name>
    <keywords>AlloSphere, mapping, performance, HCI, interactivity, Vir- tual Reality, OSC, multi-user, network </keywords>
  </document>
  <document>
    <name>nime2010_063.pdf</name>
  </document>
  <document>
    <name>nime2010_069.pdf</name>
    <abstract>This paper articulates an interest in a kind of interactive musical instrument and artwork that defines the mechanisms for instrumental interactivity from the iconic morphologies of "ready-mades", casting historical utilitarian objects as the basis for performed musical experiences by spectators. The interactive repertoires are therefore partially pre-determined through enculturated behaviors that are associated with particular objects, but more importantly, inextricably linked to the thematic and meaningful assemblage of the work itself. Our new work epi-thet gathers data from individual interactions with common microscopes placed on platforms within a large space. This data is correlated with public domain genetic datasets obtained from micro-array analysis. A sonification algorithm generates unique compositions associated with the spectator "as measured" through their individual specification in performing an iconic measurement action. The apparatus is a receptacle for unique compositions in sound, and invites a participatory choreography of stillness that is available for reception as a live musical performance. </abstract>
    <keywords>Sonification installation spectator-choreography micro-array ready-mades morphology stillness  </keywords>
  </document>
  <document>
    <name>nime2010_072.pdf</name>
    <abstract>The propriety of articulation, especially of notes that lackannotations, is influenced by the origin of the particularmusic. This paper presents a rule system for articulationderived from late Baroque and early Classic treatises on performance. Expressive articulation, in this respect, is understood as a combination of alterable tone features like duration, loudness, and timbre. The model differentiates globalcharacteristics and local particularities, provides a generalframework for human-like music performances, and, therefore, serves as a basis for further and more complex rulesystems.</abstract>
    <keywords>Articulation, Historically Informed Performance, Expres- sive Performance, Synthetic Performance </keywords>
  </document>
  <document>
    <name>nime2010_076.pdf</name>
    <abstract>We discuss how the environment urMus was designed to allow creation of mobile musical instruments on multi-touch smartphones. The design of a mobile musical instrument consists of connecting sensory capabilities to output modalities through various means of processing. We describe how the default mapping interface was designed which allows to set up such a pipeline and how visual and interactive multi-touch UIs for musical instruments can be designed within the system. </abstract>
    <keywords>Mobile music making, meta-environment, design, mapping, user interface  </keywords>
  </document>
  <document>
    <name>nime2010_082.pdf</name>
    <abstract>In this paper, we describe the development of the Stanford Mobile Phone Orchestra (MoPhO) since its inceptionin 2007. As a newly structured ensemble of musicians withiPhones and wearable speakers, MoPhO takes advantageof the ubiquity and mobility of smartphones as well asthe unique interaction techniques offered by such devices.MoPhO offers a new platform for research, instrument design, composition, and performance that can be juxtaposedto that of a laptop orchestra. We trace the origins of MoPhO,describe the motivations behind the current hardware andsoftware design in relation to the backdrop of current trendsin mobile music making, detail key interaction conceptsaround new repertoire, and conclude with an analysis onthe development of MoPhO thus far.</abstract>
    <keywords>mobile phone orchestra, live performance, iPhone, mobile music </keywords>
  </document>
  <document>
    <name>nime2010_088.pdf</name>
    <abstract>This paper reviews and extends questions of the scope of an interactive musical instrument and mapping strategies for expressive performance. We apply notions of embodiment and affordance to characterize gestural instruments. We note that the democratization of sensor technology in consumer devices has extended the cultural contexts for interaction. We revisit questions of mapping drawing upon the theory of affordances to consider mapping and instrument together. This is applied to recent work by the author and his collaborators in the development of instruments based on mobile devices designed for specific performance situations. </abstract>
    <keywords>Musical affordance, NIME, mapping, instrument definition, mobile, multimodal interaction.  </keywords>
  </document>
  <document>
    <name>nime2010_094.pdf</name>
    <abstract>This paper describes a novel method for composing andimprovisation with real-time chaotic oscillators. Recentlydiscovered algebraically simple nonlinear third-order differential equations are solved and acoustical descriptors relating to their frequency spectrums are determined accordingto the MPEG-7 specification. A second nonlinearity is thenadded to these equations: a real-time audio signal. Descriptive properties of the complex behaviour of these equationsare then determined as a function of difference tones derived from a Just Intonation scale and the amplitude ofthe audio signal. By using only the real-time audio signalfrom live performer/s as an input the causal relationshipbetween acoustic performance gestures and computer output, including any visual or performer-instruction output,is deterministic even if the chaotic behaviours are not.</abstract>
  </document>
  <document>
    <name>nime2010_100.pdf</name>
    <keywords>Interactive music interface, real-time, percussion, machine learning, Markov models, MIDI. </keywords>
  </document>
  <document>
    <name>nime2010_106.pdf</name>
    <abstract>A qualitative study to investigate the development of stylein performance with a highly constrained musical instrument is described. A new one-button instrument was designed, with which several musicians were each asked topractice and develop a solo performance. Observations oftrends in attributes of these performances are detailed in relation to participants' statements in structured interviews.Participants were observed to develop stylistic variationsboth within the domain of activities suggested by the constraint, and by discovering non-obvious techniques througha variety of strategies. Data suggest that stylistic variationsoccurred in spite of perceived constraint, but also becauseof perceived constraint. Furthermore, participants tendedto draw on unique experiences, approaches and perspectivesthat shaped individual performances.</abstract>
    <keywords>design, interaction, performance, persuasive technology </keywords>
  </document>
  <document>
    <name>nime2010_112.pdf</name>
    <abstract>We propose an environment that allows users to create music by leveraging playful visualization and organic interaction. Our attempt to improve ideas drawn from traditional sequencer paradigm has been made in terms of extemporizing music and associating with visualization in real-time. In order to offer different user experience and musical possibility, this system incorporates many techniques, including; flocking simulation, nondeterministic finite automata (NFA), score file analysis, vector calculation, OpenGL animation, and networking. We transform a sequencer into an audiovisual platform for composition and performance, which is furnished with artistry and ease of use. Thus we believe that it is suitable for not only artists such as algorithmic composers or audiovisual performers, but also anyone who wants to play music and imagery in a different way.	 </abstract>
  </document>
  <document>
    <name>nime2010_116.pdf</name>
    <abstract>In this paper, we introduce a wireless musical interface driven by grasping forces and human motion. The sounds generated by the traditional digital musical instruments are dependent on the physical shape of the musical instruments. The freedom of the musical performance is restricted by its structure. Therefore, the sounds cannot be generated with the body expression like the dance. We developed a ball-shaped interface, TwinkleBall, to achieve the free-style performance. A photo sensor is embedded in the translucent rubber ball to detect the grasping force of the performer. The grasping force is translated into the luminance intensity for processing. Moreover, an accelerometer is also embedded in the interface for motion sensing. By using these sensors, a performer can control the note and volume by varying grasping force and motion respectively. The features of the proposed interface are ball-shaped, wireless, and handheld size. As a result, the proposed interface is able to generate the sound from the body expression such as dance. </abstract>
    <keywords>Musical Interface, Embodied Sound Media, Dance Performance.  </keywords>
  </document>
  <document>
    <name>nime2010_125.pdf</name>
    <keywords>contrary, beat tracking, stream analysis, musical agent </keywords>
  </document>
  <document>
    <name>nime2010_130.pdf</name>
    <abstract>The tools for spatial composition typically model just a small subset of the spatial audio cues known to researchers. As composers explore this medium it has become evident that the nature of spatial sound perception is complex. Yet interfaces for spatial composition are often simplistic and the end results can be disappointing. This paper presents an interface that is designed to liberate the composer from thinking of spatialised sound as points in space. Instead, visual images are used to define sound in terms of shape, size and location. Images can be sequenced into video, thereby creating rich and complex temporal soundscapes. The interface offers both the ability to craft soundscapes and also compose their evolution in time. </abstract>
    <keywords>Spatial audio, surround sound, ambisonics, granular synthesis, decorrelation, diffusion.  </keywords>
  </document>
  <document>
    <name>nime2010_136.pdf</name>
    <abstract>Multi-point devices are rapidly becoming a practical interface choice for electronic musicians. Interfaces that generate multiple simultaneous streams of point data present a unique mapping challenge. This paper describes an analysis system for point relationships that acts as a bridge between raw streams of multi-point data and the instruments they control, using a multipoint trackpad to test various configurations. The aim is to provide a practical approach for instrument programmers working with multi-point tools, while highlighting the difference between mapping systems based on point coordinate streams, grid evaluations, or object interaction and mapping systems based on multi-point data relationships. </abstract>
    <keywords>Multi-point, multi-touch interface, instrument mapping, multi- point data analysis, trackpad instrument  </keywords>
  </document>
  <document>
    <name>nime2010_140.pdf</name>
    <abstract>An important part of building interactive sound models is designing the interface and control strategy. The multidimensional structure of the gestures natural for a musical or physical interface may have little obvious relationship to the parameters that a sound synthesis algorithm exposes for control. A common situation arises when there is a nonlinear synthesis technique for which a traditional instrumental interface with quasi-independent control of pitch and expression is desired. This paper presents a semi-automatic meta-modeling tool called the Instrumentalizer for embedding arbitrary synthesis algorithms in a control structure that exposes traditional instrument controls for pitch and expression. </abstract>
    <keywords>Musical interface, parameter mapping, expressive control.  </keywords>
  </document>
  <document>
    <name>nime2010_144.pdf</name>
    <abstract>scoreLight is a playful musical instrument capable of generating sound from the lines of drawings as well as from theedges of three-dimensional objects nearby (including everyday objects, sculptures and architectural details, but alsothe performer's hands or even the moving silhouettes ofdancers). There is no camera nor projector: a laser spotexplores shapes as a pick-up head would search for soundover the surface of a vinyl record - with the significant difference that the groove is generated by the contours of thedrawing itself.</abstract>
    <keywords>H5.2 [User Interfaces] interaction styles / H.5.5 [Sound and Music Computing] Methodologies and techniques / J.5 [Arts and Humanities] performing arts </keywords>
  </document>
  <document>
    <name>nime2010_150.pdf</name>
    <keywords>turntable, dial, encoder, re-purposed, hard drive, scratch- ing, inherent dynamics, DIY </keywords>
  </document>
  <document>
    <name>nime2010_156.pdf</name>
    <abstract>Since 2007, our research is related to the development of an anthropomorphic saxophonist robot, which it has been designed to imitate the saxophonist playing by mechanically reproducing the organs involved for playing a saxophone. Our research aims in understanding the motor control from an engineering point of view and enabling the communication. In this paper, the Waseda Saxophone Robot No. 2 (WAS-2) which is composed by 22-DOFs is detailed. The lip mechanism of WAS-2 has been designed with 3-DOFs to control the motion of the lower, upper and sideway lips. In addition, a human-like hand (16 DOF-s) has been designed to enable to play all the keys of the instrument. Regarding the improvement of the control system, a feed-forward control system with dead-time compensation has been implemented to assure the accurate control of the air pressure. In addition, the implementation of an auditory feedback control system has been proposed and implemented in order to adjust the positioning of the physical parameters of the components of the robot by providing a pitch feedback and defining a recovery position (off-line). A set of experiments were carried out to verify the mechanical design improvements and the dynamic response of the air pressure. As a result, the range of sound pressure has been increased and the proposed control system improved the dynamic response of the air pressure control. </abstract>
    <keywords>Humanoid Robot, Auditory Feedback, Music, Saxophone.  </keywords>
  </document>
  <document>
    <name>nime2010_162.pdf</name>
    <abstract>This paper describes the making of a class to teach the history and art of musical robotics. The details of the curriculum are described as well as designs for our custom schematics for robotic solenoid driven percussion. This paper also introduces four new robotic instruments that were built during the term of this course. This paper also introduces the Machine Orchestra, a laptop orchestra with ten human performers and our five robotic instruments.  </abstract>
  </document>
  <document>
    <name>nime2010_166.pdf</name>
    <abstract>This paper proposes a novel method to realize an initiativeexchange for robot. A humanoid robot plays vibraphone exchanging initiative with a human performer by perceivingmultimodal cues in real time. It understands the initiative exchange cues through vision and audio information.In order to achieve the natural initiative exchange betweena human and a robot in musical performance, we built thesystem and the software architecture and carried out the experiments for fundamental algorithms which are necessaryto the initiative exchange.</abstract>
    <keywords>Human-robot interaction, initiative exchange, prediction </keywords>
  </document>
  <document>
    <name>nime2010_174.pdf</name>
    <abstract>The Mobile Music (MoMu) toolkit is a new open-sourcesoftware development toolkit focusing on musical interaction design for mobile phones. The toolkit, currently implemented for iPhone OS, emphasizes usability and rapidprototyping with the end goal of aiding developers in creating real-time interactive audio applications. Simple andunified access to onboard sensors along with utilities forcommon tasks found in mobile music development are provided. The toolkit has been deployed and evaluated in theStanford Mobile Phone Orchestra (MoPhO) and serves asthe primary software platform in a new course exploringmobile music.</abstract>
    <keywords>instrument design, iPhone, mobile music, software develop- ment, toolkit </keywords>
  </document>
  <document>
    <name>nime2010_178.pdf</name>
    <abstract>The use of metaphor has a prominent role in HCI, both as a device to help users understand unfamiliar technologies, and as a tool to guide the design process. Creators of new computerbased instruments face similar design challenges as those in HCI. In the course of creating a new piece for Mobile Phone Orchestra we propose the metaphor of a sound as a ball and explore the interactions and sound mappings it suggests. These lead to the design of a gesture-controlled instrument that allows players to "bounce" sounds, "throw" them to other players, and compete in a game to "knock out" others' sounds. We composed the piece SoundBounce based on these interactions, and note that audiences seem to find performances of the piece accessible and engaging, perhaps due to the visibility of the metaphor. </abstract>
    <keywords>Mobile music, design, metaphor, performance, gameplay.  </keywords>
  </document>
  <document>
    <name>nime2010_182.pdf</name>
    <abstract>Impact force is an important dimension for percussive musical instruments such as the piano. We explore three possible mechanisms how to get impact forces on mobile multi-touch devices: using built-in accelerometers, the pressure sensing capability of Android phones, and external force sensing resistors. We find that accelerometers are difficult to control for this purpose. Android's pressure sensing shows some promise, especially when combined with augmented playing technique. Force sensing resistors can offer good dynamic resolution but this technology is not currently offered in commodity devices and proper coupling of the sensor with the applied impact is difficult. </abstract>
    <keywords>Force, impact, pressure, multi-touch, mobile phone, mobile music making.  </keywords>
  </document>
  <document>
    <name>nime2010_186.pdf</name>
    <abstract>The evolution of networked audio technologies has created unprecedented opportunities for musicians to improvise with instrumentalists from a diverse range of cultures and disciplines. As network speeds increase and latency is consigned to history, tele-musical collaboration, and in particular improvisation will be shaped by new methodologies that respond to this potential. While networked technologies eliminate distance in physical space, for the remote improviser, this creates a liminality of experience through which their performance is mediated. As a first step in understanding the conditions arising from collaboration in networked audio platforms, this paper will examine selected case studies of improvisation in a variety of networked interfaces. The author will examine how platform characteristics and network conditions influence the process of collective improvisation and the methodologies musicians are employing to negotiate their networked experiences. </abstract>
  </document>
  <document>
    <name>nime2010_192.pdf</name>
    <abstract>We present Drile, a multiprocess immersive instrument built uponthe hierarchical live-looping technique and aimed at musical performance. This technique consists in creating musical trees whosenodes are composed of sound effects applied to a musical content.In the leaves, this content is a one-shot sound, whereas in higherlevel nodes this content is composed of live-recorded sequencesof parameters of the children nodes. Drile allows musicians tointeract efficiently with these trees in an immersive environment.Nodes are represented as worms, which are 3D audiovisual objects. Worms can be manipulated using 3D interaction techniques,and several operations can be applied to the live-looping trees. Theenvironment is composed of several virtual rooms, i.e. group oftrees, corresponding to specific sounds and effects. Learning Drileis progressive since the musical control complexity varies according to the levels in live-looping trees. Thus beginners may havelimited control over only root worms while still obtaining musically interesting results. Advanced users may modify the trees andmanipulate each of the worms.</abstract>
    <keywords>Drile, immersive instrument, hierarchical live-looping, 3D interac- tion </keywords>
  </document>
  <document>
    <name>nime2010_198.pdf</name>
    <abstract>This research is concerned with issues of privacy, awareness and the emergence of roles in the process of digitallymediated collaborative music making. Specifically we areinterested in how providing collaborators with varying degrees of privacy and awareness of one another influencesthe group interaction. A study is presented whereby ninegroups of co-located musicians compose music together using three different interface designs. We use qualitative andquantitative data to study and characterise the musician'sinteraction with each other and the software. We show thatwhen made available to them, participants make extensiveuse of a private working area to develop musical contributions before they are introduced to the group. We also arguethat our awareness mechanisms change the perceived quality of the musical interaction, but have no impact on theway musicians interact with the software. We then reflecton implications for the design of new collaborative musicmaking tools which exploit the potential of digital technologies, while at the same time support creative musicalinteraction.</abstract>
    <keywords>Awareness, Privacy, Collaboration, Music, Interaction, En- gagement, Group Music Making, Design, Evaluation. </keywords>
  </document>
  <document>
    <name>nime2010_204.pdf</name>
    <abstract>In 2009 the cross artform group, Last Man to Die, presenteda series of performances using new interfaces and networkedperformance to integrate the three artforms of its members(actor, Hanna Cormick, visual artist, Benjamin Forster andpercussionist, Charles Martin). This paper explains ourartistic motivations and design for a computer vision surfaceand networked heartbeat sensor as well as the experience ofmounting our first major work, Vital LMTD.</abstract>
    <keywords>cross-artform performance, networked performance, physi- cal computing </keywords>
  </document>
  <document>
    <name>nime2010_208.pdf</name>
    <abstract>We report on a study of perceptual and acoustic featuresrelated to the placement of microphones around a custommade glass instrument. Different microphone setups weretested: above, inside and outside the instrument and at different distances. The sounds were evaluated by an expertperformer, and further qualitative and quantitative analyses have been carried out. Preference was given to therecordings from microphones placed close to the rim of theinstrument, either from the inside or the outside.</abstract>
    <keywords>glass instruments, microphone placement, sound analysis </keywords>
  </document>
  <document>
    <name>nime2010_212.pdf</name>
    <keywords>Hyper-Instruments, Glitch Music, Interactive Systems, Electronic Music Performance.  </keywords>
  </document>
  <document>
    <name>nime2010_217.pdf</name>
    <abstract>This paper presents the magnetic resonator piano, an augmented instrument enhancing the capabilities of the acoustic grand piano. Electromagnetic actuators induce the stringsto vibration, allowing each note to be continuously controlled in amplitude, frequency, and timbre without external loudspeakers. Feedback from a single pickup on thepiano soundboard allows the actuator waveforms to remainlocked in phase with the natural motion of each string. Wealso present an augmented piano keyboard which reportsthe continuous position of every key. Time and spatial resolution are sufficient to capture detailed data about keypress, release, pretouch, aftertouch, and other extended gestures. The system, which is designed with cost and setupconstraints in mind, seeks to give pianists continuous control over the musical sound of their instrument. The instrument has been used in concert performances, with theelectronically-actuated sounds blending with acoustic instruments naturally and without amplification.</abstract>
    <keywords>Augmented instruments, piano, interfaces, electromagnetic actuation, gesture measurement </keywords>
  </document>
  <document>
    <name>nime2010_223.pdf</name>
    <abstract>In this paper I describe aspects that have been involved in my experience of developing a hybrid instrument. The process of transformation and extension of the instrument is informed by ideas concerning the intrinsic communication aspects of musical activities. Decisions taken for designing the instrument and performing with it take into account the hypothesis that there are ontological levels of human reception in music that are related to the intercorporeal. Arguing that it is necessary to encounter resistances for achieving expression, it is suggested that new instrumental development ought to reflect on the concern for keeping the natural connections of live performances. </abstract>
  </document>
  <document>
    <name>nime2010_229.pdf</name>
    <abstract>This paper presents a virtual violin for real-time performances consisting of two modules: a violin spectral modeland a control interface. The interface is composed by asensing bow and a tube with drawn strings in substitutionof a real violin. The spectral model is driven by the bowingcontrols captured with the control interface and it is ableto predict spectral envelopes of the sound corresponding tothose controls. The envelopes are filled with harmonic andnoisy content and given to an additive synthesizer in orderto produce violin sounds. The sensing system is based ontwo motion trackers with 6 degrees of freedom. One trackeris attached to the bow and the other to the tube. Bowingcontrols are computed after a calibration process where theposition of virtual strings and the hair-ribbon of the bowis obtained. A real time implementation was developed asa MAX/MSP patch with external objects for each of themodules.</abstract>
    <keywords>violin, synthesis, control, spectral, virtual </keywords>
  </document>
  <document>
    <name>nime2010_233.pdf</name>
    <abstract>This research is an initial effort in showing how a multimodal approach can improve systems for gaining insight into a musician's practice and technique. Embedding a variety of sensors inside musical instruments and synchronously recording the sensors' data along with audio, we gather a database of gestural information from multiple performers, then use machine-learning techniques to recognize which musician is performing. Our multimodal approach (using both audio and sensor data) yields promising performer classification results, which we see as a first step in a larger effort to gain insight into musicians' practice and technique. </abstract>
    <keywords>Performer Recognition, Multimodal, HCI, Machine Learning, Hyperinstrument, eSitar  </keywords>
  </document>
  <document>
    <name>nime2010_238.pdf</name>
    <abstract>In this paper, we present our research on the acquisitionof gesture information for the study of the expressivenessin guitar performances. For that purpose, we design a sensor system which is able to gather the movements from lefthand fingers. Our effort is focused on a design that is (1)non-intrusive to the performer and (2) able to detect fromstrong movements of the left hand to subtle movements ofthe fingers. The proposed system is based on capacitive sensors mounted on the fingerboard of the guitar. We presentthe setup of the sensor system and analyze its response toseveral finger movements.</abstract>
    <keywords>Guitar; Gesture acquisition; Capacitive sensors </keywords>
  </document>
  <document>
    <name>nime2010_244.pdf</name>
    <abstract>The design of an unusually simple fabric-based touchlocation and pressure sensor is introduced. An analysisof the raw sensor data is shown to have significant nonlinearities and non-uniform noise. Using support vectormachine learning and a state-dependent adaptive filter itis demonstrated that these problems can be overcome.The method is evaluated quantitatively using a statisticalestimate of the instantaneous rate of information transfer.The SVM regression alone is shown to improve the gesturesignal information rate by up to 20% with zero addedlatency, and in combination with filtering by 40% subjectto a constant latency bound of 10 milliseconds.</abstract>
  </document>
  <document>
    <name>nime2010_250.pdf</name>
    <abstract>Mapping in interactive dance performance poses a number of questions related to the perception and expression of gestures in contrast to pure motion-detection and analysis. A specific interactive dance project is discussed, in which two complementary sensing modes are integrated to obtain higherlevel expressive gestures. These are applied to a modular nonlinear composition, in which the exploratory dance performance assumes the role of instrumentalist and conductor. The development strategies and methods for each of the involved artists are discussed and the software tools and wearable devices that were developed for this project are presented. </abstract>
    <keywords>Mapping, motion sensing, computer vision, artistic strategies, wearable sensors, mapping tools, splines, delaunay tessellation.  </keywords>
  </document>
  <document>
    <name>nime2010_255.pdf</name>
    <abstract>GIIMP addresses the criticism that in many interactive music systems the machine simply reacts. Interaction is addressed by extending Winkler's [18] model toward adapting Paine's [10] conversational model of interaction. Realized using commercial tools, GIIMP implements a machine/human generative improvisation system using human gesture input, machine gesture capture, and a gesture mutation module in conjunction with a flocking patch, mapped through microtonal/spectral techniques to sound. The intention is to meld some established and current practices, and combine aspects of symbolic and sub-symbolic approaches, toward musical outcomes. </abstract>
    <keywords>Interaction, gesture, genetic algorithm, flocking, improvisation.  </keywords>
  </document>
  <document>
    <name>nime2010_263.pdf</name>
    <abstract>"VirtualPhilharmony" (V.P.) is a conducting interface that enables users to perform expressive music with conducting action. Several previously developed conducting interfaces do not satisfy users who have conducting experience because the feedback from the conducting action does not always correspond with a natural performance. The tempo scheduler, which is the main engine of a conducting system, must be improved. V.P. solves this problem by introducing heuristics of conducting an orchestra in detecting beats, applying rules regarding the tempo expression in a bar, etc. We confirmed with users that the system realized a high "following" performance and had musical persuasiveness. </abstract>
    <keywords>Conducting system, heuristics, sensor, template.  </keywords>
  </document>
  <document>
    <name>nime2010_271.pdf</name>
    <abstract>Pressure, motion, and gesture are important parameters inmusical instrument playing. Pressure sensing allows to interpret complex hidden forces, which appear during playinga musical instrument. The combination of our new sensorsetup with pattern recognition techniques like the lately developed ordered means models allows fast and precise recognition of highly skilled playing techniques. This includes leftand right hand analysis as well as a combination of both. Inthis paper we show bow position recognition for string instruments by means of support vector regression machineson the right hand finger pressure, as well as bowing recognition and inaccurate playing detection with ordered meansmodels. We also introduce a new left hand and chin pressuresensing method for coordination and position change analysis. Our methods in combination with our audio, video,and gesture recording software can be used for teachingand exercising. Especially studies of complex movementsand finger force distribution changes can benefit from suchan approach. Practical applications include the recognitionof inaccuracy, cramping, or malposition, and, last but notleast, the development of augmented instruments and newplaying techniques.</abstract>
  </document>
  <document>
    <name>nime2010_277.pdf</name>
    <abstract>As one of the main expressive feature in music, articulationaffects a wide range of tone attributes. Based on experimental recordings we analyzed human articulation in the lateBaroque style. The results are useful for both the understanding of historically informed performance practices andfurther progress in synthetic performance generation. Thispaper reports of our findings and the implementation in aperformance system. Because of its flexibility and universality the system allows more than Baroque articulation.</abstract>
    <keywords>Expressive Performance, Articulation, Historically Informed Performance </keywords>
  </document>
  <document>
    <name>nime2010_283.pdf</name>
    <abstract>Generative music systems can be played by musicians who manipulate the values of algorithmic parameters, and their datacentric nature provides an opportunity for coordinated interaction amongst a group of systems linked over IP networks; a practice we call Network Jamming. This paper outlines the characteristics of this networked performance practice and discusses the types of mediated musical relationships and ensemble configurations that can arise. We have developed and tested the jam2jam network jamming software over recent years. We describe this system, draw from our experiences with it, and use it to illustrate some characteristics of Network Jamming.</abstract>
  </document>
  <document>
    <name>nime2010_287.pdf</name>
    <abstract>The paper reports on the development of prototypes of glassinstruments. The focus has been on developing acousticinstruments specifically designed for electronic treatment,and where timbral qualities have had priority over pitch.The paper starts with a brief historical overview of glassinstruments and their artistic use. Then follows an overviewof the glass blowing process. Finally the musical use of theinstruments is discussed.</abstract>
  </document>
  <document>
    <name>nime2010_291.pdf</name>
    <abstract>Input devices for controlling music software can benefit fromexploiting the use of perceptual-motor skill in interaction.The project described here is a new musical controller, designed with the aim of enabling intuitive and nuanced interaction through direct physical manipulation of malleablematerial.The controller is made from conductive foam. This foamchanges electrical resistance when deformed; the controllerworks by measuring resistance at multiple points in a single piece of foam in order to track its shape. These measurements are complex and interdependent so an echo statenetwork, a form of recurrent neural network, is employed totranslate the sensor readings into usable control data.A cube shaped controller was built and evaluated in thecontext of the haptic exploration of sound synthesis parameter spaces. Eight participants experimented with the controller and were interviewed about their experiences. Thecontroller achieves its aim of enabling intuitive interaction,but in terms of nuanced interaction, accuracy and repeatability were issues for some participants. It's not clear fromthe short evaluation study whether these issues would improve with practice, a longitudinal study that gives musicians time to practice and find the creative limitations ofthe controller would help to evaluate this fully.The evaluation highlighted interesting issues concerningthe high level nature of malleable control and different approaches to sonic exploration.</abstract>
    <keywords>Musical Controller, Reservoir Computing, Human Computer Interaction, Tangible User Interface, Evaluation </keywords>
  </document>
  <document>
    <name>nime2010_297.pdf</name>
    <abstract>The number of artists who express themselves through music in an unconventional way is constantly growing. Thistrend strongly depends on the high diffusion of laptops,which proved to be powerful and flexible musical devices.However laptops still lack in flexible interface, specificallydesigned for music creation in live and studio performances.To resolve this issue many controllers have been developed,taking into account not only the performer's needs andhabits during music creation, but also the audience desire tovisually understand how performer's gestures are linked tothe way music is made. According to the common need ofadaptable visual interface to manipulate music, in this paper we present a custom tridimensional controller, based onOpen Sound Control protocol and completely designed towork inside Virtual Reality: simple geometrical shapes canbe created to directly control loop triggering and parametermodification, just using free hand interaction.</abstract>
    <keywords>Glove device, Music controller, Virtual Reality, OSC, con- trol mapping </keywords>
  </document>
  <document>
    <name>nime2010_303.pdf</name>
    <abstract>The sound card anno 2010, is an ubiquitous part of almostany personal computing system; what was once considereda high-end, CD-quality audio fidelity, is today found in mostcommon sound cards. The increased presence of multichannel devices, along with the high sampling frequency, makesthe sound card desirable as a generic interface for acquisition of analog signals in prototyping of sensor-based musicinterfaces. However, due to the need for coupling capacitorsat a sound card's inputs and outputs, the use as a genericsignal interface of a sound card is limited to signals not carrying information in a constant DC component. Through arevisit of a card design for the (now defunct) ISA bus, thispaper proposes use of analog gates for bypassing the DCfiltering input sections, controllable from software - therebyallowing for arbitrary choice by the user, if a soundcardinput channel is to be used as a generic analog-to-digitalsensor interface. Issues regarding use of obsolete technology and educational aspects are discussed as well.</abstract>
  </document>
  <document>
    <name>nime2010_309.pdf</name>
    <abstract>Most new digital musical interfaces have evolved upon theintuitive idea that there is a causality between sonic outputand physical actions. Nevertheless, the advent of braincomputer interfaces (BCI) now allows us to directly accesssubjective mental states and express these in the physicalworld without bodily actions. In the context of an interactive and collaborative live performance, we propose to exploit novel brain-computer technologies to achieve unmediated brain control over music generation and expression.We introduce a general framework for the generation, synchronization and modulation of musical material from brainsignal and describe its use in the realization of Xmotion, amultimodal performance for a "brain quartet".</abstract>
    <keywords>Brain-computer Interface, Biosignals, Interactive Music Sys- tem, Collaborative Musical Performance </keywords>
  </document>
  <document>
    <name>nime2010_315.pdf</name>
    <abstract>This paper explores the evolution of collaborative, multi-user, musical interfaces developed for the Bricktable interactive surface. Two key types of applications are addressed: user interfaces for artistic installation and interfaces for musical performance. In describing our software, we provide insight on the methodologies and practicalities of designing interactive musical systems for tabletop surfaces. Additionally, subtleties of working with custom-designed tabletop hardware are addressed. </abstract>
    <keywords>Bricktable, Multi-touch Interface, Tangible Interface, Generative Music, Music Information Retrieval  </keywords>
  </document>
  <document>
    <name>nime2010_319.pdf</name>
    <abstract>This paper introduces the concept of composing expressive music using the principles of Fuzzy Logic. The paper provides a conceptual model of a musical work which follows compositional decision making processes. Significant features of this Fuzzy Logic framework are its inclusiveness through the consideration of all the many and varied musical details, while also incorporating the imprecision that characterises musical terminology and discourse. A significant attribute of my Fuzzy Logic method is that it traces the trajectory of all musical details, since it is both the individual elements and their combination over time which is significant to the effectiveness of a musical work in achieving its goals. The goal of this work is to find a set of elements and rules, which will ultimately enable the construction of a genralised algorithmic compositional system which can produce expressive music if so desired. </abstract>
  </document>
  <document>
    <name>nime2010_323.pdf</name>
    <abstract>In this paper we examine a wearable sonification and visualisation display that uses physical analogue visualisation and digital sonification to convey feedback about the wearer's activity and environment. Intended to bridge a gap between art aesthetics, fashionable technologies and informative physical computing, the user experience evaluation reveals the wearers' responses and understanding of a novel medium for wearable expression. The study reveals useful insights for wearable device design in general and future iterations of this sonification and visualisation display. </abstract>
    <keywords>Wearable display, sonification, visualisation, design aesthetics,  physical computing, multimodal expression, bimodal display  </keywords>
  </document>
  <document>
    <name>nime2010_327.pdf</name>
    <abstract>In this paper, we describe the shaping factors, which simplify and help us understand the multi-dimensional aspects of designing Wearable Expressions. These descriptive shaping factors contribute to both the design and user-experience evaluation of Wearable Expressions. </abstract>
    <keywords>Wearable expressions, body, user-centered design.  </keywords>
  </document>
  <document>
    <name>nime2010_331.pdf</name>
    <abstract>In this paper, we discuss the musical potential of COMPath - an online map based music-making tool - as a noveland unique interface for interactive music composition andperformance. COMPath provides an intuitive environmentfor creative music making by sonification of georeferenceddata. Users can generate musical events with simple andfamiliar actions on an online map interface; a set of local information is collected along the user-drawn route andthen interpreted as sounds of various musical instruments.We discuss the musical interpretation of routes on a map,review the design and implementation of COMPath, andpresent selected sonification results with focus on mappingstrategies for map-based composition.</abstract>
    <keywords>Musical sonification, map interface, online map service, geo- referenced data, composition, mashup </keywords>
  </document>
  <document>
    <name>nime2010_339.pdf</name>
    <abstract>This paper proposes a design concept for a tangible interface forcollaborative performances that incorporates two social factorspresent during performance, the individual creation andadaptation of technology and the sharing of it within acommunity. These factors are identified using the example of alaptop ensemble and then applied to three existing collaborativeperformance paradigms. Finally relevant technology, challengesand the current state of our implementation are discussed.</abstract>
    <keywords>Tangible User Interfaces, collaborative performances, social factors </keywords>
  </document>
  <document>
    <name>nime2010_343.pdf</name>
    <abstract>We present two complementary approaches for the visualization and interaction of dimensionally reduced data setsusing hybridization interfaces. Our implementations privilege syncretic systems allowing one to explore combinations(hybrids) of disparate elements of a data set through theirplacement in a 2-D space. The first approach allows for theplacement of data points anywhere on the plane accordingto an anticipated performance strategy. The contribution(weight) of each data point varies according to a power function of the distance from the control cursor. The secondapproach uses constrained vertex colored triangulations ofmanifolds with labels placed at the vertices of triangulartiles. Weights are computed by barycentric projection ofthe control cursor position.</abstract>
    <keywords>Interpolation, dimension reduction, radial basis functions, triangular mesh </keywords>
  </document>
  <document>
    <name>nime2010_348.pdf</name>
    <keywords>Generative music, mobile interfaces, multitouch interaction  </keywords>
  </document>
  <document>
    <name>nime2010_352.pdf</name>
    <keywords>Interactive music interface, calligraphy, graphical music composing, sonification  </keywords>
  </document>
  <document>
    <name>nime2010_356.pdf</name>
    <abstract>The sponge is an interface that allows a clear link to beestablished between gesture and sound in electroacousticmusic. The goals in developing the sponge were to reintroduce the pleasure of playing and to improve the interaction between the composer/performer and the audience. Ithas been argued that expenditure of effort or energy is required to obtain expressive interfaces. The sponge favors anenergy-sound relationship in two ways : 1) it senses acceleration, which is closely related to energy; and 2) it is madeout of a flexible material (foam) that requires effort to besqueezed or twisted. Some of the mapping strategies usedin a performance context with the sponge are discussed.</abstract>
    <keywords>Interface, electroacoustic music, performance, expressivity, mapping </keywords>
  </document>
  <document>
    <name>nime2010_360.pdf</name>
    <abstract>In this paper we discuss SurfaceMusic, a tabletop music system in which touch gestures are mapped to physical modelsof instruments. With physical models, parametric controlover the sound allows for a more natural interaction between gesture and sound. We discuss the design and implementation of a simple gestural interface for interactingwith virtual instruments and a messaging system that conveys gesture data to the audio system.</abstract>
    <keywords>Tabletop, multi-touch, gesture, physical model, Open Sound Control. </keywords>
  </document>
  <document>
    <name>nime2010_364.pdf</name>
    <abstract>Many musical instruments have interfaces which emphasisethe pitch of the sound produced over other perceptual characteristics, such as its timbre. This is at odds with the musical developments of the last century. In this paper, weintroduce a method for replacing the interface of musicalinstruments (both conventional and unconventional) witha more flexible interface which can present the intrument'savailable sounds according to variety of different perceptualcharacteristics, such as their brightness or roughness. Weapply this method to an instrument of our own design whichcomprises an electro-mechanically controlled electric guitarand amplifier configured to produce feedback tones.</abstract>
    <keywords>Concatenative Synthesis, Feedback, Guitar </keywords>
  </document>
  <document>
    <name>nime2010_368.pdf</name>
    <abstract>This paper presents a comparison of different configurationsof a wireless sensor system for capturing human motion.The systems consist of sensor elements which wirelesslytransfers motion data to a receiver element. The sensorelements consist of a microcontroller, accelerometer(s) anda radio transceiver. The receiver element consists of a radioreceiver connected through a microcontroller to a computerfor real time sound synthesis. The wireless transmission between the sensor elements and the receiver element is basedon the low rate IEEE 802.15.4/ZigBee standard.A configuration with several accelerometers connected bywire to a wireless sensor element is compared to using multiple wireless sensor elements with only one accelerometer ineach. The study shows that it would be feasable to connect5-6 accelerometers in the given setups.Sensor data processing can be done in either the receiverelement or in the sensor element. For various reasons it canbe reasonable to implement some sensor data processing inthe sensor element. The paper also looks at how much timethat typically would be needed for a simple pre-processingtask.</abstract>
    <keywords>wireless communication, ZigBee, microcontroller </keywords>
  </document>
  <document>
    <name>nime2010_372.pdf</name>
    <abstract>The past decade has seen an increase of low-cost technology for sensor data acquisition, which has been utilized for the expanding field of research in gesture measurement for music performance. Unfortunately, these devices are still far from being compatible with the audiovisual recording platforms which have been used to record synchronized streams of data. In this paper, we describe a practical solution for simultaneous recording of heterogeneous multimodal signals. The recording system presented uses MIDI Time Code to time-stamp sensor data and to synchronize with standard video and audio recording systems. We also present a set of tools for recording sensor data, as well as a set of analysis tools to evaluate in realtime the sample rate of different signals, and the overall synchronization status of the recording system. </abstract>
    <keywords>Synchronization, Multimodal Signals, Sensor Data Acquisition, Signal Recording.  </keywords>
  </document>
  <document>
    <name>nime2010_375.pdf</name>
    <abstract>This paper describes the development of an interactive 3Daudio/visual and network installation entitled POLLEN.Specifically designed for large computer Laboratories, theartwork explores the regeneration of those spaces throughthe creation of a fully immersive multimedia art experience.The paper describes the technical, aesthetic and educational development of the piece.</abstract>
    <keywords>Interactive, Installation, Network, 3D Physics Emulator, Educational Tools, Public Spaces, Computer Labs, Sound Design, Site-Specific Art </keywords>
  </document>
  <document>
    <name>nime2010_377.pdf</name>
    <abstract> Irregular Incurve is a MIDI controllable robotic string instrument. The twelve independent string-units compose the complete musical scale of 12 units. Each string can be plucked by a motor control guitar pick. A MIDI keyboard is attached to the instrument and serves as an interface for real-time interactions between the instrument and the audience. Irregular Incurve can also play preprogrammed music by itself. This paper presents the design concept and the technical solutions to realizing the functionality of Irregular Incurve. The future features are also discussed. </abstract>
    <keywords>NIME, Robotics, Acoustic, Interactive, MIDI, Real time Performance, String Instrument, Arduino, Servo, Motor Control   </keywords>
  </document>
  <document>
    <name>nime2010_380.pdf</name>
    <abstract>Peacock is a newly designed interface for improvisational performances. The interface is equipped with thirty-five proximity sensors arranged in five rows and seven columns. The sensors detect the movements of a performer's hands and arms in a three-dimensional space above them. The interface digitizes the output of the sensors into sets of high precision digital packets, and sends them to a patch running in Pdextended with a sufficiently high bandwidth for performances with almost no computational resource consumption in Pd. The precision, speed, and efficiency of the system enable the sonification of hand gestures in realtime without the need to attach any physical devices to the performer's body. This paper traces the interface's evolution, discussing relevant technologies, hardware construction, system design, and input monitoring. </abstract>
    <keywords>Musical interface, Sensor technologies, Computer music, Hardware and software design  </keywords>
  </document>
  <document>
    <name>nime2010_383.pdf</name>
    <abstract>Music recommendation systems can observe user's personal preferences and suggest new tracks from a large online catalog. In the case of context-aware recommenders, user's current emotional state plays an important role. One simple way to visualize emotions and moods is graphical emoticons. In this study, we researched a high-level mapping between genres, as descriptions of music, and emoticons, as descriptions of emotions and moods. An online questionnaire with 87 participants was arranged. Based on the results, we present a list of genres that could be used as a starting point for making recommendations fitting the current mood of the user. </abstract>
    <keywords>Music, music recommendation, context, facial expression, mood, emotion, emoticon, and musical genre.  </keywords>
  </document>
  <document>
    <name>nime2010_387.pdf</name>
    <abstract>This paper is a report on the development of a new musical instrument in which the main concept is "Untouchable". The key concept of this instrument is "sound generation by body gesture (both hands)" and "sound generation by kneading with hands". The new composition project had completed as the premiere of a new work "controllable untouchableness" with this new instrument in December 2009.</abstract>
    <keywords>Theremin, untouchable, distance sensor, Propeller processor </keywords>
  </document>
  <document>
    <name>nime2010_391.pdf</name>
    <abstract>This paper describes the design, implementation and outcome of Ground Me!, an interactive sound installation set up in the Sonic Lab of the Sonic Arts Research Centre. The site-specific interactive installation consists of multiple copper poles hanging from the Sonic Lab's ceiling panels, which trigger samples of electricity sounds when grounded through the visitor's' body to the space's metallic floor. </abstract>
    <keywords>Interactive sound installation, body impedance, skin conductivity, site-specific sound installation, human network, Sonic Lab, Arduino.  </keywords>
  </document>
  <document>
    <name>nime2010_395.pdf</name>
    <abstract>This paper presents Mmmmm; a Multimodal Mobile MusicMixer that provides DJs a new interface for mixing musicon the Nokia N900 phones. Mmmmm presents a novel wayfor DJ to become more interactive with their audience andvise versa. The software developed for the N900 mobilephone utilizes the phones built-in accelerometer sensor andBluetooth audio streaming capabilities to mix and apply effects to music using hand gestures and have the mixed audiostream to Bluetooth speakers, which allows the DJ to moveabout the environment and get familiarized with their audience, turning the experience of DJing into an interactiveand audience engaging process.Mmmmm is designed so that the DJ can utilize handgestures and haptic feedback to help them perform the various tasks involved in DJing (mixing, applying effects, andetc). This allows the DJ to focus on the crowd, thus providing the DJ a better intuition of what kind of music ormusical mixing style the audience is more likely to enjoyand engage with. Additionally, Mmmmm has an "Ambient Tempo Detection mode in which the phones camera isutilized to detect the amount of movement in the environment and suggest to the DJ the tempo of music that shouldbe played. This mode utilizes frame differencing and pixelchange overtime to get a sense of how fast the environmentis changing, loosely correlating to how fast the audience isdancing or the lights are flashing in the scene. By determining the ambient tempo of the environment the DJ canget a better sense for the type of music that would fit bestfor their venue.Mmmmm helps novice DJs achieve a better music repertoire by allowing them to interact with their audience andreceive direct feedback on their performance. The DJ canchoose to utilize these modes of interaction and performance or utilize traditional DJ controls using MmmmmsN900 touch screen based graphics user interface.</abstract>
    <keywords>Multi-modal, interaction, music, mixer, mobile, interactive, DJ, smart phones, Nokia, n900, touch screen, accelerome- ter, phone, audience </keywords>
  </document>
  <document>
    <name>nime2010_399.pdf</name>
    <abstract>With the decreasing audience of classical music performance, this research aims to develop a performance-enhancement system, called AIDA, to help classical performers better communicating with their audiences. With three procedures Input-Processing-Output, AIDA system can sense and analyze the body information of performers and further reflect it onto the responsive skin. Thus abstract and intangible emotional expressions of performers are transformed into tangible and concrete visual elements, which clearly facilitating the audiences' threshold for music appreciation. </abstract>
    <keywords>Interactive Performance, Ambient Environment, Responsive Skin, Music performance.  </keywords>
  </document>
  <document>
    <name>nime2010_403.pdf</name>
    <abstract>In this paper we outline the emerging field of Interactional Sound and Music which concerns itself with multi-person technologically mediated interactions primarily using audio. We present several examples of interactive systems in our group, and reflect on how they were designed and evaluated. Evaluation techniques for collective, performative, and task oriented activities are outlined and compared. We emphasise the importance of designing for awareness in these systems, and provide examples of different awareness mechanisms. </abstract>
    <keywords>Interactional, sound, music, mutual engagement, improvisation, composition, collaboration, awareness.  </keywords>
  </document>
  <document>
    <name>nime2010_411.pdf</name>
    <abstract>In this study artistic human-robot interaction design is introduced as a means for scientific research and artistic investigations. It serves as a methodology for situated cognitionintegrating empirical methodology and computational modeling, and is exemplified by the installation playing robot.Its artistic purpose is to aid to create and explore robots as anew medium for art and entertainment. We discuss the useof finite state machines to organize robots' behavioral reactions to sensor data, and give a brief outlook on structuredobservation as a potential method for data collection.</abstract>
  </document>
  <document>
    <name>nime2010_415.pdf</name>
    <abstract>This project aims at studying how recent interactive and interactions technologies would help extend how we play theguitar, thus defining the "multimodal guitar". Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed anddeveloped a freely-available toolbox for augmented guitarperformances, compliant with the PureData and Max/MSPenvironments, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing,modal synthesis, infinite sustain, rearranging looping and"smart" harmonizing.</abstract>
    <keywords>Augmented guitar, audio synthesis, digital audio effects, multimodal interaction, gestural sensing, polyphonic tran- scription, hexaphonic guitar </keywords>
  </document>
  <document>
    <name>nime2010_419.pdf</name>
    <abstract>This paper introduces my research in physical interactive design with my "GRIP MAESTRO" electroacoustic performance interface. It then discusses the considerations involved in creating intuitive software mappings of emotive performative gestures such that they are idiomatic not only of the sounds they create but also of the physical nature of the interface itself. </abstract>
  </document>
  <document>
    <name>nime2010_423.pdf</name>
    <abstract>In this paper, we present an interactive system that uses the body as a generative tool for creating music. We explore innovative ways to make music, create self-awareness, and provide the opportunity for unique, interactive social experiences. The system uses a multi-player game paradigm, where players work together to add layers to a soundscape of three distinct environments. Various sensors and hardware are attached to the body and transmit signals to a workstation, where they are processed using Max/MSP. The game is divided into three levels, each of a different soundscape. The underlying purpose of our system is to move the player's focus away from complexities of the modern urban world toward a more internalized meditative state. The system is currently viewed as an interactive installation piece, but future iterations have potential applications in music therapy, bio games, extended performance art, and as a prototype for new interfaces for musical expression. </abstract>
  </document>
  <document>
    <name>nime2010_427.pdf</name>
    <abstract>Maintaining a sense of personal connection between increasingly synthetic performers and increasingly diffuse audiences is vital to storytelling and entertainment. Sonic intimacy is important, because voice is one of the highestbandwidth channels for expressing our real and imagined selves.New tools for highly focused spatialization could help improve acoustical clarity, encourage audience engagement, reduce noise pollution and inspire creative expression. We have a particular interest in embodied, embedded systems for vocal performance enhancement and transformation. This short paper describes work in progress on a toolkit for high-quality wearable sound suits. Design goals include tailored directionality and resonance, full bandwidth, and sensible ergonomics. Engineering details to accompany a demonstration of recent prototypes are presented, highlighting a novel magnetostrictive flextensional transducer. Based on initial observations we suggest that vocal acoustic output from the torso, and spatial perception of situated low frequency sources, are two areas deserving greater attention and further study.</abstract>
  </document>
  <document>
    <name>nime2010_431.pdf</name>
    <abstract>The Ghost has been developed to create a merger between the standard MIDI keyboard controller, MIDI/digital guitars and alternative desktop controllers. Using a custom software editor, The Ghost's controls can be mapped to suit the users performative needs. The interface takes its interaction and gestural cues from the guitar but it is not a MIDI guitar. The Ghost's hardware, firmware and software will be open sourced with the hopes of creating a community of users that are invested in creating music with controller.</abstract>
    <keywords>Controller, MIDI, Live Performance, Programmable, Open- Source </keywords>
  </document>
  <document>
    <name>nime2010_436.pdf</name>
    <abstract>This paper presents a discussion regarding organology classification and taxonomies for digital musical instruments (DMI), arising from the TIEM (Taxonomy of Interfaces for Electronic Music performance) survey (http://tiem.emf.org/), conducted as part of an Australian Research Council Linkage project titled "Performance Practice in New Interfaces for Realtime Electronic Music Performance". This research is being carried out at the VIPRe Lab at, the University of Western Sydney in partnership with the Electronic Music Foundation (EMF), Infusion Systems1 and The Input Devices and Music Interaction Laboratory (IDMIL) at McGill University. The project seeks to develop a schema of new interfaces for realtime electronic music performance. </abstract>
    <keywords>Instrument, Interface, Organology, Taxonomy.  </keywords>
  </document>
  <document>
    <name>nime2010_440.pdf</name>
    <abstract>humanaquarium is a self-contained, transportable performance environment that is used to stage technology-mediated interactive performances in public spaces. Drawing upon the creative practices of busking and street performance, humanaquarium incorporates live musicians, real-time audiovisual content generation, and frustrated total internal reflection (FTIR) technology to facilitate participatory interaction by members of the public. </abstract>
  </document>
  <document>
    <name>nime2010_444.pdf</name>
    <keywords>Mobile device, music composer, pattern composing, MIDI  </keywords>
  </document>
  <document>
    <name>nime2010_451.pdf</name>
    <abstract>Drawing on a model of spectator understanding of error inperformance in the literature, we document a qualitativeexperiment that explores the relationships between domainknowledge, mental models, intention and error recognitionby spectators of performances with electronic instruments.Participants saw two performances with contrasting instruments, with controls on their mental model and understanding of intention. Based on data from a subsequent structured interview, we identify themes in participants' judgements and understanding of performance and explanationsof their spectator experience. These reveal both elementsof similarity and difference between the two performances,instruments and between domain knowledge groups. Fromthese, we suggest and discuss implications for the design ofnovel performative interactions with technology.</abstract>
  </document>
  <document>
    <name>nime2010_455.pdf</name>
    <abstract>Gaining access to a prototype motion capture suit designedby the Animazoo company, the Interactive Systems groupat the University of Sussex have been investigating application areas. This paper describes our initial experimentsin mapping the suit control data to sonic attributes for musical purposes. Given the lab conditions under which weworked, an agile design cycle methodology was employed,with live coding of audio software incorporating fast feedback, and more reflective preparations between sessions, exploiting both individual and pair programming. As the suitprovides up to 66 channels of information, we confront achallenging mapping problem, and techniques are describedfor automatic calibration, and the use of echo state networksfor dimensionality reduction.</abstract>
    <keywords>Motion Capture, Musical Controller, Mapping, Agile De- sign </keywords>
  </document>
  <document>
    <name>nime2010_459.pdf</name>
    <abstract>This paper describes a study of membrane potentiometers and long force sensing resistors as tools to enable greater interaction between performers and audiences. This is accomplished through the building of a new interface called the Helio. In preparation for the Helio's construction, a variety of brands of membrane potentiometers and long force sensing resistors were analyzed for their suitability for use in a performance interface. Analog and digital circuit design considerations are discussed. We discuss in detail the design process and performance scenarios explored with the Helio. </abstract>
    <keywords>Force Sensing Resistors, Membrane Potentiometers, Force Sensing Resistors, Haptic Feedback, Helio </keywords>
  </document>
  <document>
    <name>nime2010_463.pdf</name>
    <abstract>We present a novel user interface device based around ferromagnetic sensing. The physical form of the interface can easily be reconfigured by simply adding and removing a variety of ferromagnetic objects to the device's sensing surface. This allows the user to change the physical form of the interface resulting in a variety of different interaction modes. When used in a musical context, the performer can leverage the physical reconfiguration of the device to affect the method of playing and ultimately the sound produced. We describe the implementation of the sensing system, along with a range of mapping techniques used to transform the sensor data into musical output, including both the direct synthesis of sound and also the generation of MIDI data for use with Ableton Live. We conclude with a discussion of future directions for the device. </abstract>
    <keywords>Ferromagnetic sensing, ferrofluid, reconfigurable user interface, wave terrain synthesis, MIDI controller.  </keywords>
  </document>
  <document>
    <name>nime2010_467.pdf</name>
  </document>
  <document>
    <name>nime2010_469.pdf</name>
    <abstract>We propose an online generative algorithm to enhance musical expression via intelligent improvisation accompaniment.Our framework called the ImprovGenerator, takes a livestream of percussion patterns and generates an improvisedaccompaniment track in real-time to stimulate new expressions in the improvisation. We use a mixture model togenerate an accompaniment pattern, that takes into account both the hierarchical temporal structure of the liveinput patterns and the current musical context of the performance. The hierarchical structure is represented as astochastic context-free grammar, which is used to generateaccompaniment patterns based on the history of temporalpatterns. We use a transition probability model to augmentthe grammar generated pattern to take into account thecurrent context of the performance. In our experiments weshow how basic beat patterns performed by a percussioniston a cajon can be used to automatically generate on-the-flyimprovisation accompaniment for live performance.</abstract>
    <keywords>Machine Improvisation, Grammatical Induction, Stochastic Context-Free Grammars, Algorithmic Composition </keywords>
  </document>
  <document>
    <name>nime2010_473.pdf</name>
    <abstract>This paper presents the development of rapid and reusablegestural interface prototypes for navigation by similarity inan audio database and for sound manipulation, using theAudioCycle application. For this purpose, we propose andfollow guidelines for rapid prototyping that we apply usingthe PureData visual programming environment. We havemainly developed three prototypes of manual control: onecombining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits andshortcomings we experienced while prototyping using thisapproach.</abstract>
    <keywords>Human-computer interaction, gestural interfaces, rapid pro- totyping, browsing by similarity, audio database </keywords>
  </document>
  <document>
    <name>nime2010_477.pdf</name>
    <abstract>In this paper we present a novel system for tactile actuation in stylus-based musical interactions. The proposed controller aims to support rhythmical musical performance. The system builds on resistive force feedback, which is achieved through a brakeaugmented ball pen stylus on a sticky touch-sensitive surface. Along the device itself, we present musical interaction principles that are enabled through the aforementioned tactile response. Further variations of the device and perspectives of the friction-based feedback are outlined. </abstract>
  </document>
  <document>
    <name>nime2010_479.pdf</name>
    <keywords>Multi-touch Interfaces, Computer-Assisted Composition </keywords>
  </document>
  <document>
    <name>nime2010_481.pdf</name>
    <abstract>In this paper, we describe a comparison between parameters drawn from 3-dimensional measurement of a dance performance, and continuous emotional response data recorded from an audience present during this performance. A continuous time series representing the mean movement as the dance unfolds is extracted from the 3-dimensional data. The audiences' continuous emotional response data are also represented as a time series, and the series are compared. We concluded that movement in the dance performance directly influences the emotional arousal response of the audience. </abstract>
    <keywords>Dance, Emotion, Motion Capture, Continuous Response.  </keywords>
  </document>
  <document>
    <name>nime2010_485.pdf</name>
    <abstract>This paper investigates whether a dynamic vibrotactile feedback improves the playability of a gesture controlled virtual instrument. The instrument described in this study is based on a virtual control surface that player strikes with a hand held sensor-actuator device. We designed two tactile cues to augment the stroke across the control surface: a static and dynamic cue. The static cue was a simple burst of vibration triggered when crossing the control surface. The dynamic cue was continuous vibration increasing in amplitude when approaching the surface. We arranged an experiment to study the influence of the tactile cues in performance. In a tempo follow task, the dynamic cue yielded significantly the best temporal and periodic accuracy and control of movement velocity and amplitude. The static cue did not significantly improve the rhythmic accuracy but assisted the control of movement velocity compared to the condition without tactile feedback at all. The findings of the study indicate that careful design of dynamic vibrotactile feedback can improve the controllability of gesture based virtual instrument. </abstract>
    <keywords>Virtual instrument, Gesture, Tactile feedback, Motor control  </keywords>
  </document>
  <document>
    <name>nime2010_489.pdf</name>
    <abstract>In his demonstration, the author discusses the sequential progress of his technical and aesthetic decisions as composer and videographer for four large-scale works for dance through annotated video examples of live performances and PowerPoint slides. In addition, he discusses his current real-time dance work with wireless sensor interfaces using sewable LilyPad Arduino modules and Xbee radio hardware. Keywords </abstract>
    <keywords>dance, video processing, video tracking, LilyPad Arduino.  </keywords>
  </document>
  <document>
    <name>nime2010_493.pdf</name>
    <abstract>This paper describes a series of mathematical functions implemented by the author in the commercial algorithmic software language ArtWonk, written by John Dunn, which are offered with that language as resources for composers. It gives a history of the development of the functions, with an emphasis on how I developed them for use in my compositions. </abstract>
    <keywords>Algorithmic composition, mathematical composition, probability distributions, fractals, additive sequences  </keywords>
  </document>
  <document>
    <name>nime2010_497.pdf</name>
    <abstract>The console gaming industry is experiencing a revolution in terms of user control, and a large part to Nintendo's introduction of the Wii remote. The online open source development community has embraced the Wii remote, integrating the inexpensive technology into numerous applications. Some of the more interesting applications demonstrate how the remote hardware can be leveraged for nonstandard uses. In this paper we describe a new way of interacting with the Wii remote and sensor bar to produce music. The Wiiolin is a virtual instrument which can mimic a violin or cello. Sensor bar motion relative to the Wii remote and button presses are analyzed in real-time to generate notes. Our design is novel in that it involves the remote's infrared camera and sensor bar as an integral part of music production, allowing users to change notes by simply altering the angle of their wrist, and henceforth, bow. The Wiiolin introduces a more realistic way of instrument interaction than other attempts that rely on button presses and accelerometer data alone. </abstract>
    <keywords>Wii remote, virtual instrument, violin, cello, motion recognition, human computer interaction, gesture recognition.  </keywords>
  </document>
  <document>
    <name>nime2010_501.pdf</name>
  </document>
  <document>
    <name>nime2011_004.pdf</name>
    <abstract>The Overtone Fiddle is a new violin-family instrument that incorporates electronic sensors, integrated DSP, and physical actuation of the acoustic body. An embedded tactile sound transducer creates extra vibrations in the body of the Overtone Fiddle, allowing performer control and sensation via both traditional violin techniques, as well as extended playing techniques that incorporate shared man/machine control of the resulting sound. A magnetic pickup system is mounted to the end of the fiddle's fingerboard in order to detect the signals from the vibrating strings, deliberately not capturing vibrations from the full body of the instrument. This focused sensing approach allows less restrained use of DSP-generated feedback signals, as there is very little direct leakage from the actuator embedded in the body of the instrument back to the pickup. </abstract>
    <keywords>Actuated Musical Instruments, Hybrid Instruments, Active Acoustics, Electronic Violin  </keywords>
  </document>
  <document>
    <name>nime2011_008.pdf</name>
    <keywords>multi-touch, haptics, frustrated total internal reflection, mu- sic performance, music composition, latency, DIY </keywords>
  </document>
  <document>
    <name>nime2011_014.pdf</name>
    <abstract>The Electromagnetically Sustained Rhodes Piano is an augmentation of the original instrument with additional control over the amplitude envelope of individual notes. Thisincludes slow attacks and infinite sustain while preservingthe familiar spectral qualities of this classic electromechanical piano. These additional parameters are controlled withaftertouch on the existing keyboard, extending standardpiano technique. Two sustain methods were investigated,driving the actuator first with a pure sine wave, and secondwith the output signal of the sensor. A special isolationmethod effectively decouples the sensors from the actuatorsand tames unruly feedback in the high-gain signal path.</abstract>
    <keywords>Rhodes, keyboard, electromagnetic, sustain, augmented in- strument, feedback, aftertouch </keywords>
  </document>
  <document>
    <name>nime2011_018.pdf</name>
    <abstract>This paper describes the motivation and construction ofGamelan Elektrika, a new electronic gamelan modeled aftera Balinese Gong Kebyar. The first of its kind, Elektrika consists of seven instruments acting as MIDI controllers accompanied by traditional percussion and played by 11 or moreperformers following Balinese performance practice. Threemain percussive instrument designs were executed using acombination of force sensitive resistors, piezos, and capacitive sensing. While the instrument interfaces are designedto play interchangeably with the original, the sound andtravel possiblilities they enable are tremendous. MIDI enables a massive new sound palette with new scales beyondthe quirky traditional tuning and non-traditional sounds.It also allows simplified transcription for an aurally taughttradition. Significantly, it reduces the transportation challenges of a previously large and heavy ensemble, creatingopportunities for wider audiences to experience Gong Kebyar's enchanting sound. True to the spirit of oneness inBalinese music, as one of the first large all-MIDI ensembles,Elek Trika challenges performers to trust silent instrumentsand develop an understanding of highly intricate and interlocking music not through the sound of the individual, butthrough the sound of the whole.</abstract>
    <keywords>bali, gamelan, musical instrument design, MIDI ensemble </keywords>
  </document>
  <document>
    <name>nime2011_024.pdf</name>
    <keywords>Stereotypical transducers, audible sound, Doppler effect, hand- free interface, musical instrument, interactive performance  </keywords>
  </document>
  <document>
    <name>nime2011_028.pdf</name>
    <abstract>This paper describes recent developments in the creation of sound-making instruments and devices powered by photovoltaic (PV) technologies. With the rise of more efficient PV products in diverse packages, the possibilities for creating solar-powered musical instruments, sound installations, and loudspeakers are becoming increasingly realizable. This paper surveys past and recent developments in this area, including several projects by the author, and demonstrates how the use of PV technologies can influence the creative process in unique ways. In addition, this paper discusses how solar sound arts can enhance the aesthetic direction taken by recent work in soundscape studies and acoustic ecology. Finally, this paper will point towards future directions and possibilities as PV technologies continue to evolve and improve in terms of performance, and become more affordable. </abstract>
    <keywords>Solar Sound Arts, Circuit Bending, Hardware Hacking, Human-Computer Interface Design, Acoustic Ecology, Sound Art, Electroacoustics, Laptop Orchestra, PV Technology  </keywords>
  </document>
  <document>
    <name>nime2011_032.pdf</name>
    <abstract>This paper provides a discussion of how the electronic, solely ITbased composition and performance of electronic music can besupported in realtime with a collaborative application on a tabletopinterface, mediating between single-user style music compositiontools and co-located collaborative music improvisation. After having elaborated on the theoretical backgrounds of prerequisites ofco-located collaborative tabletop applications as well as the common paradigms in music composition/notation, we will review related work on novel IT approaches to music composition and improvisation. Subsequently, we will present our prototypical implementation and the results.</abstract>
    <keywords>Tabletop Interface, Collaborative Music Composition, Creativity Support </keywords>
  </document>
  <document>
    <name>nime2011_036.pdf</name>
    <abstract>Popular music (characterized by improvised instrumental parts, beat and measure-level organization, and steady tempo) poses challenges for human-computer music performance (HCMP). Pieces of music are typically rearrangeable on-the-fly and involve a high degree of variation from ensemble to ensemble, and even between rehearsal and performance. Computer systems aiming to participate in such ensembles must therefore cope with a dynamic high-level structure in addition to the more traditional problems of beat-tracking, score-following, and machine improvisation. There are many approaches to integrating the components required to implement dynamic human-computer music performance systems. This paper presents a reference architecture designed to allow the typical sub-components (e.g. beat-tracking, tempo prediction, improvisation) to be integrated in a consistent way, allowing them to be combined and/or compared systematically. In addition, the paper presents a dynamic score representation particularly suited to the demands of popular music performance by computer. </abstract>
  </document>
  <document>
    <name>nime2011_040.pdf</name>
    <abstract>V'OCT(Ritual) is a work for solo vocalist/performer and Bodycoder System, composed in residency at Dartington College of Arts (UK) Easter 2010. This paper looks at the technical and compositional methodologies used in the realization of the work, in particular, the choices made with regard to the mapping of sensor elements to various spatialization functions. Kinaesonics will be discussed in relation to the coding of real-time one-to-one mapping of sound to gesture and its expression in terms of hardware and software design. Four forms of expressivity arising out of interactive work with the Bodycoder system will be identified. How sonic (electro-acoustic), programmed, gestural (kinaesonic) and in terms of the V'Oct(Ritual) vocal expressivities are constructed as pragmatic and tangible elements within the compositional practice will be discussed and the subsequent importance of collaboration with a performer will be exposed. </abstract>
    <keywords>Bodycoder, Kinaesonics, Expressivity, Gestural Control,  Interactive Performance Mechanisms, Collaboration.   </keywords>
  </document>
  <document>
    <name>nime2011_044.pdf</name>
    <abstract>First Person Shooters are among the most played computer videogames. They combine navigation, interaction and collaboration in3D virtual environments using simple input devices, i.e. mouseand keyboard. In this paper, we study the possibilities broughtby these games for musical interaction. We present the Couacs, acollaborative multiprocess instrument which relies on interactiontechniques used in FPS together with new techniques adding theexpressiveness required for musical interaction. In particular, theFaders For All game mode allows musicians to perform patternbased electronic compositions.</abstract>
    <keywords>the couacs, fps, first person shooters, collaborative, 3D interaction, multiprocess instrument </keywords>
  </document>
  <document>
    <name>nime2011_048.pdf</name>
  </document>
  <document>
    <name>nime2011_052.pdf</name>
    <keywords>Sound installation, robotic music, interactive systems  </keywords>
  </document>
  <document>
    <name>nime2011_056.pdf</name>
    <abstract>Many performers of novel musical instruments find it difficult to engage audiences beyond those in the field. Previousresearch points to a failure to balance complexity with usability, and a loss of transparency due to the detachmentof the controller and sound generator. The issue is oftenexacerbated by an audience's lack of prior exposure to theinstrument and its workings.However, we argue that there is a conflict underlyingmany novel musical instruments in that they are intendedto be both a tool for creative expression and a creative workof art in themselves, resulting in incompatible requirements.By considering the instrument, the composition and theperformance together as a whole with careful considerationof the rate of learning demanded of the audience, we propose that a lack of transparency can become an asset ratherthan a hindrance. Our approach calls for not only controllerand sound generator to be designed in sympathy with eachother, but composition, performance and physical form too.Identifying three design principles, we illustrate this approach with the Serendiptichord, a wearable instrument fordancers created by the authors.</abstract>
    <keywords>Performance, composed instrument, transparency, constraint. </keywords>
  </document>
  <document>
    <name>nime2011_060.pdf</name>
    <abstract>In this paper, we discuss the use of the clothesline as ametaphor for designing a musical interface called Airer Choir. This interactive installation is based on the function ofan ordinary object that is not a traditional instrument, andhanging articles of clothing is literally the gesture to use theinterface. Based on this metaphor, a musical interface withhigh transparency was designed. Using the metaphor, weexplored the possibilities for recognizing of input gesturesand creating sonic events by mapping data to sound. Thus,four different types of Airer Choir were developed. By classifying the interfaces, we concluded that various musicalexpressions are possible by using the same metaphor.</abstract>
    <keywords>musical interface, metaphor, clothesline installation </keywords>
  </document>
  <document>
    <name>nime2011_064.pdf</name>
    <abstract>In this paper, we discuss the results obtained by means of the EGGS (Elementary Gestalts for Gesture Sonification) system in terms of artistic realizations. EGGS was introduced in a previous edition of this conference. The works presented include interactive installations in the form of public art and interactive onstage performances. In all of the works, the EGGS principles of simplicity based on the correspondence between elementary sonic and movement units, and of organicity between sound and gesture are applied. Indeed, we study both sound as a means for gesture representation and gesture as embodiment of sound. These principles constitute our guidelines for the investigation of the bidirectional relationship between sound and body expression with various strategies involving both educated and non-educated executors. </abstract>
    <keywords>Gesture sonification, Interactive performance, Public art.  </keywords>
  </document>
  <document>
    <name>nime2011_068.pdf</name>
    <abstract>The present article describes a reverberation instrumentwhich is based on cognitive categorization of reverberating spaces. Different techniques for artificial reverberationwill be covered. A multidimensional scaling experimentwas conducted on impulse responses in order to determinehow humans acoustically perceive spatiality. This researchseems to indicate that the perceptual dimensions are related to early energy decay and timbral qualities. Theseresults are applied to a reverberation instrument based ondelay lines. It can be contended that such an instrumentcan be controlled more intuitively than other delay line reverberation tools which often provide a confusing range ofparameters which have a physical rather than perceptualmeaning.</abstract>
    <keywords>Reverberation, perception, multidimensional scaling, map- ping </keywords>
  </document>
  <document>
    <name>nime2011_072.pdf</name>
    <keywords>Vibrotactile feedback, human-computer interfaces, digital composition, real-time performance, augmented instruments. </keywords>
  </document>
  <document>
    <name>nime2011_076.pdf</name>
    <abstract>The use of Interactive Evolutionary Computation(IEC) issuitable to the development of art-creation aid system forbeginners. This is because of important features of IEC,like the ability of optimizing with ambiguous evaluationmeasures, and not requiring special knowledge about artcreation. With the popularity of Consumer Generated Media, many beginners in term of art-creation are interestedin creating their own original art works. Thus developing ofuseful IEC system for musical creation is an urgent task.However, user-assist functions for IEC proposed in pastworks decrease the possibility of getting good unexpectedresults, which is an important feature of art-creation withIEC. In this paper, The author proposes a new IEC evaluation process named "Shopping Basket" procedure IEC.In the procedure, an user-assist function called SimilarityBased Reasoning allows for natural evaluation by the user.The function reduces user's burden without reducing thepossibility of unexpected results. The author performs anexperiment where subjects use the new interface to validateit. As a result of the experiment, the author concludes thethe new interface is better to motivate users to composewith IEC system than the old interface.</abstract>
    <keywords>Interactive Evolutionary Computation, User-Interface, Com- position Aid </keywords>
  </document>
  <document>
    <name>nime2011_080.pdf</name>
    <abstract>BioRhythm is an interactive bio-feedback installation controlled by the cardiovascular system. Data from a photoplethysmograph (PPG) sensor controls sonification and visualization parameters in real-time. Biological signals areobtained using the techniques of Resonance Theory in Hemodynamics and mapped to audiovisual cues via the Five Element Philosophy. The result is a new media interface utilizing sound synthesis and spatialization with advanced graphics rendering. BioRhythm serves as an artistic explorationof the harmonic spectra of pulse waves.</abstract>
  </document>
  <document>
    <name>nime2011_084.pdf</name>
    <keywords>Electromechanical sonic art, kinetic sound art, prepared speakers, Infinite Spring.  </keywords>
  </document>
  <document>
    <name>nime2011_088.pdf</name>
    <abstract>We introduce a novel algorithm for automatically generating rhythms in real time in a certain meter. The generated rhythms are "generic" in the sense that they are characteristic of each time signature without belonging to a specific musical style. The algorithm is based on a stochastic model in which various aspects and qualities of the generated rhythm can be controlled intuitively and in real time. Such qualities are the density of the generated events per bar, the amount of variation in generation, the amount of syncopation, the metrical strength, and of course the meter itself. The kin.rhythmicator software application was developed to implement this algorithm. During a performance with the kin.rhythmicator the user can control all aspects of the performance through descriptive and intuitive graphic controls. </abstract>
    <keywords>automatic music generation, generative, stochastic, metric  indispensability, syncopation, Max/MSP, Max4Live  </keywords>
  </document>
  <document>
    <name>nime2011_092.pdf</name>
    <abstract>The importance of embedded devices as new devices to thefield of Voltage-Controlled Synthesizers is realized. Emphasis is directed towards understanding the importance of suchdevices in Voltage-Controlled Synthesizers. Introducing theVoltage-Controlled Computer as a new paradigm. Specifications for hardware interfacing and programming techniquesare described based on real prototypes. Implementationsand successful results are reported.</abstract>
    <keywords>Voltage-controlled synthesizer, embedded systems, voltage- controlled computer, computer driven control voltage gen- eration </keywords>
  </document>
  <document>
    <name>nime2011_096.pdf</name>
    <abstract>We developed an automatic piano performance system calledPolyhymnia that is able to generate expressive polyphonicpiano performances with music scores so that it can be usedas a computer-based tool for an expressive performance.The system automatically renders expressive piano musicby means of automatic musical symbol interpretation andstatistical models of structure-expression relations regarding polyphonic features of piano performance. Experimental results indicate that the generated performances of various piano pieces with diverse trained models had polyphonicexpression and sounded expressively. In addition, the models trained with different performance styles reflected thestyles observed in the training performances, and they werewell distinguishable by human listeners. Polyhymnia wonthe first prize in the autonomous section of the PerformanceRendering Contest for Computer Systems (Rencon) 2010.</abstract>
    <keywords>performance rendering, polyphonic expression, statistical modeling, conditional random fields </keywords>
  </document>
  <document>
    <name>nime2011_100.pdf</name>
    <abstract>Audio mixing is the adjustment of relative volumes, panning and other parameters corresponding to different soundsources, in order to create a technically and aestheticallyadequate sound sum. To do this, audio engineers employ"panpots" and faders, the standard controls in audio mixers. The design of such devices has remained practically unchanged for decades since their introduction. At the time,no usability studies seem to have been conducted on suchdevices, so one could question if they are really optimizedfor the task they are meant for.This paper proposes a new set of controls that might beused to simplify and/or improve the performance of audiomixing tasks, taking into account the spatial characteristicsof modern mixing technologies such as surround and 3Daudio and making use of multitouch interface technologies.A preliminary usability test has shown promising results.</abstract>
  </document>
  <document>
    <name>nime2011_104.pdf</name>
    <abstract>This paper explores how a general cognitive architecture canpragmatically facilitate the development and exploration ofinteractive music interfaces on a mobile platform. To thisend we integrated the Soar cognitive architecture into themobile music meta-environment urMus. We develop anddemonstrate four artificial agents which use diverse learningmechanisms within two mobile music interfaces. We alsoinclude details of the computational performance of theseagents, evincing that the architecture can support real-timeinteractivity on modern commodity hardware.</abstract>
  </document>
  <document>
    <name>nime2011_108.pdf</name>
    <abstract>Supervised machine learning enables complex many-to-manymappings and control schemes needed in interactive performance systems. One of the persistent problems in theseapplications is generating, identifying and choosing inputoutput pairings for training. This poses problems of scope(limiting the realm of potential control inputs), effort (requiring significant pre-performance training time), and cognitive load (forcing the performer to learn and remember thecontrol areas). We discuss the creation and implementationof an automatic "supervisor," using unsupervised machinelearning algorithms to train a supervised neural networkon the fly. This hierarchical arrangement enables networktraining in real time based on the musical or gestural control inputs employed in a performance, aiming at freeing theperformer to operate in a creative, intuitive realm, makingthe machine control transparent and automatic. Three implementations of this self supervised model driven by iPod,iPad, and acoustic violin are described.</abstract>
    <keywords>NIME, machine learning, interactive computer music, ma- chine listening, improvisation, adaptive resonance theory </keywords>
  </document>
  <document>
    <name>nime2011_112.pdf</name>
    <abstract>A mixed media tool was created that promotes ensemblevirtuosity through tight coordination and interdepence inmusical performance. Two different types of performers interact with a virtual space using Wii remote and tangibleinterfaces using the reacTIVision toolkit [11]. One group ofperformers uses a tangible tabletop interface to place andmove sound objects in a virtual environment. The soundobjects are represented by visual avatars and have audiosamples associated with them. A second set of performersmake use of Wii remotes to create triggering waves thatcan collide with those sound objects. Sound is only produced upon collision of the waves with the sound objects.What results is a performance in which users must negotiate through a physical and virtual space and are positionedto work together to create musical pieces.</abstract>
    <keywords>reacTIVision, processing, ensemble, mixed media, virtual- ization, tangible, sample </keywords>
  </document>
  <document>
    <name>nime2011_116.pdf</name>
    <abstract>This paper presents MoodifierLive, a mobile phone application for interactive control of rule-based automatic musicperformance. Five different interaction modes are available,of which one allows for collaborative performances with upto four participants, and two let the user control the expressive performance using expressive hand gestures. Evaluations indicate that the application is interesting, fun touse, and that the gesture modes, especially the one based ondata from free expressive gestures, allow for performanceswhose emotional content matches that of the gesture thatproduced them.</abstract>
    <keywords>Expressive performance, gesture, collaborative performance, mobile phone </keywords>
  </document>
  <document>
    <name>nime2011_120.pdf</name>
  </document>
  <document>
    <name>nime2011_124.pdf</name>
    <abstract>This paper presents a study of blowing pressure profilesacquired from recorder playing. Blowing pressure signalsare captured from real performance by means of a a lowintrusiveness acquisition system constructed around commercial pressure sensors based on piezoelectric transducers.An alto recorder was mechanically modified by a luthierto allow the measurement and connection of sensors whilerespecting playability and intrusiveness. A multi-modaldatabase including aligned blowing pressure and sound signals is constructed from real practice, covering the performance space by considering different fundamental frequencies, dynamics, articulations and note durations. Once signals were pre-processed and segmented, a set of temporalenvelope features were defined as a basis for studying andconstructing a simplified model of blowing pressure profilesin different performance contexts.</abstract>
  </document>
  <document>
    <name>nime2011_128.pdf</name>
    <abstract>This is an overview of the three installations Hoppsa Universum, CLOSE and Flying Carpet. They were all designed as choreographed sound and music installations controlled by the visitors movements. The perspective is from an artistic goal/vision intention in combination with the technical challenges and possibilities. All three installations were realized with video cameras in the ceiling registering the users' position or movement. The video analysis was then controlling different types of interactive software audio players. Different aspects like narrativity, user control, and technical limitations are discussed. </abstract>
    <keywords>Gestures, dance, choreography, music installation, interactive music.  </keywords>
  </document>
  <document>
    <name>nime2011_136.pdf</name>
    <abstract>We developed a kinetic particles synthesizer for mobile devices having a multi-touch screen such as a tablet PC and a smart phone. This synthesizer generates music based on the kinetics of particles under a two-dimensional physics engine. The particles move in the screen to synthesize sounds according to their own physical properties, which are shape, size, mass, linear and angular velocity, friction, restitution, etc. If a particle collides with others, a percussive sound is generated. A player can play music by the simple operation of touching or dragging on the screen of the device. Using a three-axis acceleration sensor, a player can perform music by shuffling or tilting the device. Each particle sounds just a simple tone. However, a large amount of various particles play attractive music by aggregating their sounds. This concept has been inspired by natural sounds made from an assembly of simple components, for example, rustling leaves or falling rain. For a novice who has no experience of playing a musical instrument, it is easy to learn how to play instantly and enjoy performing music with intuitive operation. Our system is used for musical instruments for interactive music entertainment. </abstract>
    <keywords>Particle, Tablet PC, iPhone, iPod touch, iPad, Smart phone, Kinetics, Touch screen, Physics engine.  </keywords>
  </document>
  <document>
    <name>nime2011_140.pdf</name>
    <abstract>Daft Datum is an autonomous new media artefact that takes input from movement of the feet (i.e. tapping/stomping/stamping) on a wooden surface, underneath which is a sensor sheet. The sensors in the sheet are mapped to various sound samples and synthesized sounds. Attributes of the synthesized sound, such as pitch and octave, can be controlled using the Nintendo Wii Remote. It also facilitates switching between modes of sound and recording/playing back a segment of audio. The result is music generated by dancing on the device that is further modulated by a hand-held controller. </abstract>
    <keywords>Daft Datum, Wii, Dance Pad, Feet, Controller, Bluetooth, Musical Interface, Dance, Sensor Sheet  </keywords>
  </document>
  <document>
    <name>nime2011_144.pdf</name>
    <abstract>In this paper we present an experimental study concerninggestural embodiment of environmental sounds in a listeningcontext. The presented work is part of a project aiming atmodeling movement-sound relationships, with the end goalof proposing novel approaches for designing musical instruments and sounding objects. The experiment is based onsound stimuli corresponding to "causal" and "non-causal"sounds. It is divided into a performance phase and an interview. The experiment is designed to investigate possiblecorrelation between the perception of the "causality" of environmental sounds and different gesture strategies for thesound embodiment. In analogy with the perception of thesounds' causality, we propose to distinguish gestures that"mimic" a sound's cause and gestures that "trace" a sound'smorphology following temporal sound characteristics. Results from the interviews show that, first, our causal soundsdatabase lead to consistent descriptions of the action at theorigin of the sound and participants mimic this action. Second, non-causal sounds lead to inconsistent metaphoric descriptions of the sound and participants make gestures following sound "contours". Quantitatively, the results showthat gesture variability is higher for causal sounds that noncausal sounds.</abstract>
    <keywords>Embodiment, Environmental Sound Perception, Listening, Gesture Sound Interaction </keywords>
  </document>
  <document>
    <name>nime2011_149.pdf</name>
    <abstract>The use of physiological signals in Human Computer Interaction (HCI) is becoming popular and widespread, mostlydue to sensors miniaturization and advances in real-timeprocessing. However, most of the studies that use physiologybased interaction focus on single-user paradigms, and itsusage in collaborative scenarios is still in its beginning. Inthis paper we explore how interactive sonification of brainand heart signals, and its representation through physicalobjects (physiopucks) in a tabletop interface may enhancemotivational and controlling aspects of music collaboration.A multimodal system is presented, based on an electrophysiology sensor system and the Reactable, a musical tabletop interface. Performance and motivation variables wereassessed in an experiment involving a test "Physio" group(N=22) and a control "Placebo" group (N=10). Pairs ofparticipants used two methods for sound creation: implicitinteraction through physiological signals, and explicit interaction by means of gestural manipulation. The resultsshowed that pairs in the Physio Group declared less difficulty, higher confidence and more symmetric control thanthe Placebo Group, where no real-time sonification was provided as subjects were using pre-recorded physiological signal being unaware of it. These results support the feasibilityof introducing physiology-based interaction in multimodalinterfaces for collaborative music generation.</abstract>
  </document>
  <document>
    <name>nime2011_155.pdf</name>
    <abstract>This paper examines the creation of augmented musicalinstruments by a number of musicians. Equipped with asystem called the Augmentalist, 10 musicians created newaugmented instruments based on their traditional acousticor electric instruments. This paper discusses the ways inwhich the musicians augmented their instruments, examines the similarities and differences between the resultinginstruments and presents a number of interesting findingsresulting from this process.</abstract>
    <keywords>Augmented Instruments, Instrument Design, Digital Musi- cal Instruments, Performance </keywords>
  </document>
  <document>
    <name>nime2011_161.pdf</name>
    <abstract>We present "Tahakum", an open source, extensible collection of software tools designed to enhance workflow on multichannel audio systems within complex multi-functional research and development environments. Tahakum aims to provide critical functionality required across a broad spectrum of audio systems usage scenarios, while at the same time remaining sufficiently open as to easily support modifications and extensions via 3rd party hardware and software. Features provided in the framework include software for custom mixing/routing and audio system preset automation, software for network message routing/redirection and protocol conversion, and software for dynamic audio asset management and control. </abstract>
    <keywords>Audio Control Systems, Audio for VR, Max/MSP, Spatial Audio   </keywords>
  </document>
  <document>
    <name>nime2011_167.pdf</name>
    <abstract>Computer music systems that coordinate or interact with human musicians exist in many forms. Often, coordination is at the level of gestures and phrases without synchronization at the beat level (or perhaps the notion of "beat" does not even exist). In music with beats, fine-grain synchronization can be achieved by having humans adapt to the computer (e.g. following a click track), or by computer accompaniment in which the computer follows a predetermined score. We consider an alternative scenario in which improvisation prevents traditional score following, but where synchronization is achieved at the level of beats, measures, and cues. To explore this new type of human-computer interaction, we have created new software abstractions for synchronization and coordination of music and interfaces in different modalities. We describe these new software structures, present examples, and introduce the idea of music notation as an interactive musical interface rather than a static document. </abstract>
  </document>
  <document>
    <name>nime2011_173.pdf</name>
    <abstract>This paper describes a new Beagle Board-based platform forteaching and practicing interaction design for musical applications. The migration from desktop and laptop computerbased sound synthesis to a compact and integrated control, computation and sound generation platform has enormous potential to widen the range of computer music instruments and installations that can be designed, and improvesthe portability, autonomy, extensibility and longevity of designed systems. We describe the technical features of theSatellite CCRMA platform and contrast it with personalcomputer-based systems used in the past as well as emergingsmart phone-based platforms. The advantages and tradeoffs of the new platform are considered, and some projectwork is described.</abstract>
  </document>
  <document>
    <name>nime2011_179.pdf</name>
    <keywords>Digital scratching, mobile music, digital DJ, smartphone, turntable, turntablism, record player, accelerometer, gyro- scope, vinyl emulation software </keywords>
  </document>
  <document>
    <name>nime2011_185.pdf</name>
    <abstract>MadPad is a networked audiovisual sample station for mobile devices. Twelve short video clips are loaded onto thescreen in a grid and playback is triggered by tapping anywhere on the clip. This is similar to tapping the pads of anaudio sample station, but extends that interaction to addvisual sampling. Clips can be shot on-the-fly with a cameraenabled mobile device and loaded into the player instantly,giving the performer an ability to quickly transform his orher surroundings into a sample-based, audiovisual instrument. Samples can also be sourced from an online community in which users can post or download content. The recent ubiquity of multitouch mobile devices and advances inpervasive computing have made this system possible, providing for a vast amount of content only limited by theimagination of the performer and the community. This paper presents the core features of MadPad and the designexplorations that inspired them.</abstract>
    <keywords>mobile music, networked music, social music, audiovisual, sampling, user-generated content, crowdsourcing, sample station, iPad, iPhone </keywords>
  </document>
  <document>
    <name>nime2011_191.pdf</name>
    <abstract>Visual information integration in mobile music performanceis an area that has not been thoroughly explored and currentapplications are often individually designed. From camerainput to flexible output rendering, we discuss visual performance support in the context of urMus, a meta-environmentfor mobile interaction and performance development. Theuse of cameras, a set of image primitives, interactive visualcontent, projectors, and camera flashes can lead to visuallyintriguing performance possibilities.</abstract>
    <keywords>Mobile performance, visual interaction, camera phone, mo- bile collaboration </keywords>
  </document>
  <document>
    <name>nime2011_197.pdf</name>
    <abstract>This paper describes the origin, design, and implementation of Smule's Magic Fiddle, an expressive musical instrument for the iPad. Magic Fiddle takes advantage of the physical aspects of the device to integrate game-like and pedagogical elements. We describe the origin of Magic Fiddle, chronicle its design process, discuss its integrated music education system, and evaluate the overall experience. </abstract>
    <keywords>Magic Fiddle, iPad, physical interaction design, experiential design, music education.  </keywords>
  </document>
  <document>
    <name>nime2011_203.pdf</name>
  </document>
  <document>
    <name>nime2011_207.pdf</name>
    <abstract>Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups ofperformers to use ordinary laptop computers as instrumentsand sound sources in the performance of specially createdmusic software. Perhaps the biggest challenge for LOs isthe distribution, management and control of software acrossheterogeneous collections of networked computers. Software must be stored and distributed from a central repository, but launched on individual laptops immediately beforeperformance. The GRENDL project leverages proven gridcomputing frameworks and approaches the Laptop Orchestra as a distributed computing platform for interactive computer music. This allows us to readily distribute softwareto each laptop in the orchestra depending on the laptop'sinternal configuration, its role in the composition, and theplayer assigned to that computer. Using the SAGA framework, GRENDL is able to distribute software and managesystem and application environments for each composition.Our latest version includes tangible control of the GRENDLenvironment for a more natural and familiar user experience.</abstract>
    <keywords>laptop orchestra, tangible interaction, grid computing </keywords>
  </document>
  <document>
    <name>nime2011_211.pdf</name>
    <abstract>A contemporary PC user, typically expects a sound cardto be a piece of hardware, that: can be manipulated by'audio' software (most typically exemplified by 'media players'); and allows interfacing of the PC to audio reproduction and/or recording equipment. As such, a 'sound card'can be considered to be a system, that encompasses designdecisions on both hardware and software levels - that alsodemand a certain understanding of the architecture of thetarget PC operating system.This project outlines how an Arduino Duemillanoveboard (containing a USB interface chip, manufactured byFuture Technology Devices International Ltd [FTDI]company) can be demonstrated to behave as a full-duplex,mono, 8-bit 44.1 kHz soundcard, through an implementation of: a PC audio driver for ALSA (Advanced LinuxSound Architecture); a matching program for theArduino'sATmega microcontroller - and nothing more than headphones (and a couple of capacitors). The main contributionof this paper is to bring a holistic aspect to the discussionon the topic of implementation of soundcards - also by referring to open-source driver, microcontroller code and testmethods; and outline a complete implementation of an open- yet functional - soundcard system.</abstract>
  </document>
  <document>
    <name>nime2011_217.pdf</name>
    <abstract>In this paper, we introduce a pipe interface that recognizestouch on tone holes by the resonances in the pipe instead ofa touch sensor. This work was based on the acoustic principles of woodwind instruments without complex sensors andelectronic circuits to develop a simple and durable interface.The measured signals were analyzed to show that differentfingerings generate various sounds. The audible resonancesignal in the pipe interface can be used as a sonic event formusical expression by itself and also as an input parameterfor mapping different sounds.</abstract>
    <keywords>resonance, mapping, pipe </keywords>
  </document>
  <document>
    <name>nime2011_220.pdf</name>
    <abstract>In this paper a collaborative music game for two pen tablets is studied in order to see how two people with no professional music background negotiated musical improvisation. In an initial study of what it is that constitutes play fluency in improvisation, a music game has been designed and evaluated through video analysis: A qualitative view of mutual action describes the social context of music improvisation: how two people with speech, laughter, gestures, postures and pauses negotiate individual and joint action. The objective behind the design of the game application was to support players in some aspects of their mutual play. Results show that even though players activated additional sound feedback as a result of their mutual play, players also engaged in forms of mutual play that the game engine did not account for. These ways of mutual play are descibed further along with some suggestions for how to direct future designs of collaborative music improvisation games towards ways of mutual play.  </abstract>
    <keywords>Collaborative interfaces, improvisation, interactive music games, social interaction, play, novice.  </keywords>
  </document>
  <document>
    <name>nime2011_224.pdf</name>
    <abstract>The Bass Sleeve uses an Arduino board with a combination of buttons, switches, flex sensors, force sensing resistors, and an accelerometer to map the ancillary movements of a performer to sampling, real-time audio and video processing including pitch shifting, delay, low pass filtering, and onscreen video movement. The device was created to augment the existing functions of the electric bass and explore the use of ancillary gestures to control the laptop in a live performance. In this research it was found that incorporating ancillary gestures into a live performance could be useful when controlling the parameters of audio processing, sound synthesis and video manipulation. These ancillary motions can be a practical solution to gestural multitasking allowing independent control of computer music parameters while performing with the electric bass. The process of performing with the Bass Sleeve resulted in a greater amount of laptop control, an increase in the amount of expressiveness using the electric bass in combination with the laptop, and an improvement in the interactivity on both the electric bass and laptop during a live performance. The design uses various gesture-to-sound mapping strategies to accomplish a compositional task during an electro acoustic multimedia musical performance piece. </abstract>
    <keywords>Interactive Music, Interactive Performance Systems, Gesture Controllers, Augmented Instruments, Electric Bass, Video Tracking  </keywords>
  </document>
  <document>
    <name>nime2011_228.pdf</name>
    <abstract>This paper describes the KarmetiK NotomotoN, a new musical robotic system for performance and education. A long time goal of the authors has been to provide users with plug-andplay, highly expressive musical robot system with a high degree of portability. This paper describes the technical details of the NotomotoN, and discusses its use in performance and educational scenarios. Detailed tests performed to optimize technical aspects of the NotomotoN are described to highlight usability and performance specifications for electronic musicians and educators. </abstract>
  </document>
  <document>
    <name>nime2011_232.pdf</name>
    <abstract>The Manipuller is a novel Gestural Controller based on strings manipulation and multi-dimensional force sensing technology. This paper describes its motivation, design and operational principles along with some of its musical applications. Finally the results of a preliminary usability test are presented and discussed. </abstract>
  </document>
  <document>
    <name>nime2011_236.pdf</name>
    <abstract>The Surface Editor is a software tool for creating control interfaces and mapping input actions to OSC or MIDI actions very easily and intuitively. Originally conceived to be used with a tactile interface, the Surface Editor has been extended to support the creation of graspable interfaces as well. This paper presents a new framework for the generic mapping of user actions with graspable objects on a surface. We also present a system for detecting touch on thin objects, allowing for extended interactive possibilities. The Surface Editor is not limited to a particular tracking system though, and the generic mapping approach for objects can have a broader use with various input interfaces supporting touch and/or objects. </abstract>
    <keywords>NIME, mapping, interaction, user-defined interfaces, tangibles, graspable interfaces.  </keywords>
  </document>
  <document>
    <name>nime2011_240.pdf</name>
    <abstract>This paper presents the SmartFiducial, a wireless tangible object that facilitates additional modes of expressivity for vision-based tabletop surfaces. Using infrared proximity sensing and resistive based force-sensors, the SmartFiducial affords users unique, and highly gestural inputs. Furthermore, the SmartFiducial incorporates additional customizable pushbutton switches. Using XBee radio frequency (RF) wireless transmission, the SmartFiducial establishes bipolar communication with a host computer. This paper describes the design and implementation of the SmartFiducial, as well as an exploratory use in a musical context. </abstract>
    <keywords>Fiducial, Tangible Interface, Multi-touch, Sensors, Gesture, Hap- tics, Bricktable, Proximity Sensing  </keywords>
  </document>
  <document>
    <name>nime2011_244.pdf</name>
    <keywords>generalized keyboard, isomorphic layout, multi-touch sur- face, tablet, musical interface design, iPad, microtonality </keywords>
  </document>
  <document>
    <name>nime2011_248.pdf</name>
  </document>
  <document>
    <name>nime2011_252.pdf</name>
    <abstract>This research presents a 3D gestural interface for collaborative concatenative sound synthesis and audio mosaicing.Our goal is to improve the communication between the audience and performers by means of an enhanced correlationbetween gestures and musical outcome. Nuvolet consists ofa 3D motion controller coupled to a concatenative synthesis engine. The interface detects and tracks the performers hands in four dimensions (x,y,z,t) and allows them toconcurrently explore two or three-dimensional sound cloudrepresentations of the units from the sound corpus, as wellas to perform collaborative target-based audio mosaicing.Nuvolet is included in the Esmuc Laptop Orchestra catalogfor forthcoming performances.</abstract>
    <keywords>concatenative synthesis, audio mosaicing, open-air inter- face, gestural controller, musical instrument, 3D </keywords>
  </document>
  <document>
    <name>nime2011_256.pdf</name>
    <abstract>We report on a performance study of a French-Canadian fiddler. The fiddling tradition forms an interesting contrast toclassical violin performance in several ways. Distinguishingfeatures include special elements in the bowing techniqueand the presence of an accompanying foot clogging pattern.These two characteristics are described, visualized and analyzed using video and motion capture recordings as sourcematerial.</abstract>
    <keywords>fiddler, violin, French-Canadian, bowing, feet, clogging, mo- tion capture, video, motiongram, kinematics, sonification </keywords>
  </document>
  <document>
    <name>nime2011_260.pdf</name>
    <abstract>In this paper an audio-visual installation is discussed, which combines interactive, immersive and generative elements. After introducing some of the challenges in the field of Generative Art and placing the work within its research context, conceptual reflections are made about the spatial, behavioural, perceptual and social issues that are raised within the entire installation. A discussion about the artistic content follows, focussing on the scenography and on working with flocking algorithms in general, before addressing three specific pieces realised for the exhibition. Next the technical implementation for both hardand software are detailed before the idea of a hybrid ecosystem gets discussed and further developments outlined.</abstract>
    <keywords>Generative Art, Interactive Environment, Immersive Installation, Swarm Simulation, Hybrid Ecosystem  </keywords>
  </document>
  <document>
    <name>nime2011_264.pdf</name>
    <abstract>In this paper, we describe an implementation of a real-time sound synthesizer using Finite Difference-based simulation of a two-dimensional membrane. Finite Difference (FD) methods can be the basis for physics-based music instrument models that generate realistic audio output. However, such methods are compute-intensive; large simulations cannot run in real time on current CPUs. Many current systems now include powerful Graphics Processing Units (GPUs), which are a good fit for FD methods. We demonstrate that it is possible to use this method to create a usable real-time audio synthesizer. </abstract>
    <keywords>Finite Difference, GPU, CUDA, Synthesis  </keywords>
  </document>
  <document>
    <name>nime2011_268.pdf</name>
    <abstract>Interacting with musical avatars have been increasingly popular over the years, with the introduction of games likeGuitar Hero and Rock Band. These games provide MIDIequipped controllers that look like their real-world counterparts (e.g. MIDI guitar, MIDI drumkit) that the users playto control their designated avatar in the game. The performance of the user is measured against a score that needs tobe followed. However, the avatar does not move in responseto how the user plays, it follows some predefined movementpattern. If the user plays badly, the game ends with theavatar ending the performance (i.e. throwing the guitar onthe floor). The gaming experience would increase if theavatar would move in accordance with user input. This paper presents an architecture that couples musical input withbody movement. Using imitation learning, a simulated human robot learns to play the drums like human drummersdo, both visually and auditory. Learning data is recordedusing MIDI and motion tracking. The system uses an artificial intelligence approach to implement imitation learning,employing artificial neural networks.</abstract>
  </document>
  <document>
    <name>nime2011_272.pdf</name>
    <abstract>TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.</abstract>
    <keywords>Twitter, audience participation, sonification, data visual- ization, text processing, interaction, multi-user instrument. </keywords>
  </document>
  <document>
    <name>nime2011_276.pdf</name>
    <abstract>JunctionBox is a new software toolkit for creating multitouch interfaces for controlling sound and music. Morespecifically, the toolkit has special features which make iteasy to create TUIO-based touch interfaces for controllingsound engines via Open Sound Control. Programmers using the toolkit have a great deal of freedom to create highlycustomized interfaces that work on a variety of hardware.</abstract>
    <keywords>Multi-touch, Open Sound Control, Toolkit, TUIO </keywords>
  </document>
  <document>
    <name>nime2011_280.pdf</name>
    <abstract>This paper presents an approach to practice-based researchin new musical instrument design. At a high level, the process involves drawing on relevant theories and aesthetic approaches to design new instruments, attempting to identify relevant applied design criteria, and then examiningthe experiences of performers who use the instruments withparticular reference to these criteria. Outcomes of this process include new instruments, theories relating to musicianinstrument interaction and a set of design criteria informedby practice and research.</abstract>
    <keywords>practice-based research, evaluation, Human-Computer In- teraction, research methods, user studies </keywords>
  </document>
  <document>
    <name>nime2011_284.pdf</name>
    <keywords>Spectral Model Synthesis, Gesture Recognition, Synthesis Control, Wacom Tablet, Machine Learning </keywords>
  </document>
  <document>
    <name>nime2011_288.pdf</name>
    <abstract>We present BeatJockey, a prototype interface which makesuse of Audio Mosaicing (AM), beat-tracking and machinelearning techniques, for supporting Diskjockeys (DJs) byproposing them new ways of interaction with the songs onthe DJ's playlist. This prototype introduces a new paradigmto DJing in which the user has the capability to mix songsinteracting with beat-units that accompany the DJ's mix.For this type of interaction, the system suggests song slicestaken from songs selected from a playlist, which could gowell with the beats of whatever master song is being played.In addition the system allows the synchronization of multiple songs, thus permitting flexible, coherent and rapid progressions in the DJ's mix. BeatJockey uses the Reactable,a musical tangible user interface (TUI), and it has beendesigned to be used by all DJs regardless of their level ofexpertise, as the system helps the novice while bringing newcreative opportunities to the expert.</abstract>
    <keywords>DJ, music information retrieval, audio mosaicing, percus- sion, turntable, beat-mash, interactive music interfaces, real- time, tabletop interaction, reactable. </keywords>
  </document>
  <document>
    <name>nime2011_292.pdf</name>
    <abstract>In this paper the relationship between body, motion and sound is addressed. The comparison with traditional instruments and dance is shown with regards to basic types of motion. The difference between gesture and movement is outlined and some of the models used in dance for structuring motion sequences are described. In order to identify expressive aspects of motion sequences a test scenario is devised. After the description of the methods and tools used in a series of measurements, two types of data-display are shown and the applied in the interpretation. One salient feature is recognized and put into perspective with regards to movement and gestalt perception. Finally the merits of the technical means that were applied are compared and a model-based approach to motion-sound mapping is proposed. </abstract>
    <keywords>Interactive Dance, Motion and Gesture, Sonification, Motion Perception, Mapping   </keywords>
  </document>
  <document>
    <name>nime2011_296.pdf</name>
    <abstract>MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.</abstract>
    <keywords>EEG, BCMI, collaboration, sonification, visualization </keywords>
  </document>
  <document>
    <name>nime2011_304.pdf</name>
    <abstract>The goal of our research is to find ways of supporting and encouraging musical behavior by non-musicians in shared public performance environments. Previous studies indicated simultaneous music listening and performance is difficult for non-musicians, and that visual support for the task might be helpful. This paper presents results from a preliminary user study conducted to evaluate the effect of visual feedback on a musical tracking task. Participants generated a musical signal by manipulating a hand-held device with two dimensions of control over two parameters, pitch and density of note events, and were given the task of following a target pattern as closely as possible. The target pattern was a machine-generated musical signal comprising of variation over the same two parameters. Visual feedback provided participants with information about the control parameters of the musical signal generated by the machine. We measured the task performance under different visual feedback strategies. Results show that single parameter visualizations tend to improve the tracking performance with respect to the visualized parameter, but not the non-visualized parameter. Visualizing two independent parameters simultaneously decreases performance in both dimensions. </abstract>
    <keywords>Mobile phone, Interactive music performance, Listening, Group music play, Visual support   </keywords>
  </document>
  <document>
    <name>nime2011_308.pdf</name>
    <abstract>An effective programming style for gesture signal processing is described using a new library that brings efficient run-time polymorphism, functional and instance-based object-oriented programming to Max/MSP. By introducing better support for generic programming and composability Max/MSP becomes a more productive environment for managing the growing scale and complexity of gesture sensing systems for musical instruments and interactive installations. </abstract>
  </document>
  <document>
    <name>nime2011_316.pdf</name>
    <abstract>The article describes a flexible mapping technique realized as a many-to-many dynamic mapping matrix. Digital sound generation is typically controlled by a large number of parameters and efficient and flexible mapping is necessary to provide expressive control over the instrument. The proposed modulation matrix technique may be seen as a generic and selfmodifying mapping mechanism integrated in a dynamic interpolation scheme. It is implemented efficiently by taking advantage of its inherent sparse matrix structure. The modulation matrix is used within the Hadron Particle Synthesizer, a complex granular module with 200 synthesis parameters and a simplified performance control structure with 4 expression parameters. </abstract>
    <keywords>Mapping, granular synthesis, modulation, live performance   </keywords>
  </document>
  <document>
    <name>nime2011_322.pdf</name>
    <abstract>The purpose of this brief paper is to revisit the question oflongevity in present experimental practice and coin the termautonomous new media artefacts (AutoNMA), which arecomplete and independent of external computer systems,so they can be operable for a longer period of time andcan be demonstrated at a moment's notice. We argue thatplatforms for prototyping should promote the creation ofAutoNMA to make extant the devices which will be a partof the future history of new media.</abstract>
    <keywords>autonomous, standalone, Satellite CCRMA, Arduino </keywords>
  </document>
  <document>
    <name>nime2011_324.pdf</name>
    <abstract>Recently, Microsoft introduced a game interface called Kinect for the Xbox 360 video game platform. This interface enables users to control and interact with the game console without the need to touch a controller. It largely increases the users' degree of freedom to express their emotion. In this paper, we first describe the system we developed to use this interface for sound generation and controlling musical expression. The skeleton data are extracted from users' motions and the data are translated to pre-defined MIDI data. We then use the MIDI data to control several applications. To allow the translation between the data, we implemented a simple Kinect-to-MIDI data convertor, which is introduced in this paper. We describe two applications to make music with Kinect: we first generate sound with Max/MSP, and then control the adlib with our own adlib generating system by the body movements of the users. </abstract>
    <keywords>Kinect, gaming interface, sound generation, adlib generation   </keywords>
  </document>
  <document>
    <name>nime2011_326.pdf</name>
    <abstract>This paper proposes a new research direction for the large family of instrumental musical interfaces where sound is generated using digital granular synthesis, and where interaction and control involve the (fine) operation of stiff, flat contact surfaces. First, within a historical context, a general absence of, and clear need for, tangible output that is dynamically instantiated by the grain-generating process itself is identified. Second, to fill this gap, a concrete general approach is proposed based on the careful construction of non-vibratory and vibratory force pulses, in a one-to-one relationship with sonic grains.An informal pilot psychophysics experiment initiating the approach was conducted, which took into account the two main cases for applying forces to the human skin: perpendicular, and lateral. Initial results indicate that the force pulse approach can enable perceivably multidimensional, tangible display of the ongoing grain-generating process. Moreover, it was found that this can be made to meaningfully happen (in real time) in the same timescale of basic sonic grain generation. This is not a trivial property, and provides an important and positive fundament for further developing this type of enhanced display. It also leads to the exciting prospect of making arbitrary sonic grains actual physical manipulanda. </abstract>
  </document>
  <document>
    <name>nime2011_329.pdf</name>
    <abstract>This paper presents a prototypical tool for sound selection driven by users' gestures. Sound selection by gesturesis a particular case of "query by content" in multimediadatabases. Gesture-to-Sound matching is based on computing the similarity between both gesture and sound parameters' temporal evolution. The tool presents three algorithms for matching gesture query to sound target. Thesystem leads to several applications in sound design, virtualinstrument design and interactive installation.</abstract>
    <keywords>Query by Gesture, Time Series Analysis, Sonic Interaction </keywords>
  </document>
  <document>
    <name>nime2011_331.pdf</name>
    <abstract>We propose and discuss an open source real-time interface that focuses in the vast potential for interactive soundart creation emerging from biological neural networks, asparadigmatic complex systems for musical exploration. Inparticular, we focus on networks that are responsible for thegeneration of rhythmic patterns.The interface relies uponthe idea of relating metaphorically neural behaviors to electronic and acoustic instruments notes, by means of flexiblemapping strategies. The user can intuitively design network configurations by dynamically creating neurons andconfiguring their inter-connectivity. The core of the systemis based in events emerging from his network design, whichfunctions in a similar way to what happens in real smallneural networks. Having multiple signal and data inputsand outputs, as well as standard communications protocolssuch as MIDI, OSC and TCP/IP, it becomes and uniquetool for composers and performers, suitable for different performance scenarios, like live electronics, sound installationsand telematic concerts.</abstract>
    <keywords>rhythm generation, biological neural networks, complex pat- terns, musical interface, network performance </keywords>
  </document>
  <document>
    <name>nime2011_337.pdf</name>
    <abstract>This paper presents a novel algorithm that has been specifically designed for the recognition of multivariate temporal musical gestures. The algorithm is based on DynamicTime Warping and has been extended to classify any N dimensional signal, automatically compute a classificationthreshold to reject any data that is not a valid gesture andbe quickly trained with a low number of training examples.The algorithm is evaluated using a database of 10 temporalgestures performed by 10 participants achieving an averagecross-validation result of 99%.</abstract>
    <keywords>Dynamic Time Warping, Gesture Recognition, Musician- Computer Interaction, Multivariate Temporal Gestures </keywords>
  </document>
  <document>
    <name>nime2011_343.pdf</name>
    <abstract>This paper presents the SARC EyesWeb Catalog, (SEC),a machine learning toolbox that has been specifically developed for musician-computer interaction. The SEC featuresa large number of machine learning algorithms that can beused in real-time to recognise static postures, perform regression and classify multivariate temporal gestures. Thealgorithms within the toolbox have been designed to workwith any N -dimensional signal and can be quickly trainedwith a small number of training examples. We also providethe motivation for the algorithms used for the recognitionof musical gestures to achieve a low intra-personal generalisation error, as opposed to the inter-personal generalisation error that is more common in other areas of humancomputer interaction.</abstract>
    <keywords>Machine learning, gesture recognition, musician-computer interaction, SEC </keywords>
  </document>
  <document>
    <name>nime2011_349.pdf</name>
    <abstract>In composer Tod Machover's new opera Death and the Powers, the main character uploads his consciousness into anelaborate computer system to preserve his essence and agencyafter his corporeal death. Consequently, for much of theopera, the stage and the environment itself come alive asthe main character. This creative need brings with it a hostof technical challenges and opportunities. In order to satisfythe needs of this storyline, Machover's Opera of the Futuregroup at the MIT Media Lab has developed a suite of newperformance technologies, including robot characters, interactive performance capture systems, mapping systems forauthoring interactive multimedia performances, new musical instruments, unique spatialized sound controls, anda unified control system for all these technological components. While developed for a particular theatrical production, many of the concepts and design procedures remain relevant to broader contexts including performance,robotics, and interaction design.</abstract>
    <keywords>opera, Death and the Powers, Tod Machover, gestural in- terfaces, Disembodied Performance, ambisonics </keywords>
  </document>
  <document>
    <name>nime2011_355.pdf</name>
    <abstract>In this paper we introduce a multimodal platform for Hybrid Reality live performances: by means of non-invasiveVirtual Reality technology, we developed a system to presentartists and interactive virtual objects in audio/visual choreographies on the same real stage. These choreographiescould include spectators too, providing them with the possibility to directly modify the scene and its audio/visual features. We also introduce the first interactive performancestaged with this technology, in which an electronic musician played live five tracks manipulating the 3D projectedvisuals. As questionnaires have been distributed after theshow, in the last part of this work we discuss the analysisof collected data, underlining positive and negative aspectsof the proposed experience.This paper belongs together with a performance proposalcalled Dissonance, in which two performers exploit the platform to create a progressive soundtrack along with the exploration of an interactive virtual environment.</abstract>
    <keywords>Interactive Performance, Hybrid Choreographies, Virtual Reality, Music Control </keywords>
  </document>
  <document>
    <name>nime2011_361.pdf</name>
    <abstract>We conducted three studies with contemporary music composers at IRCAM. We found that even highly computer-literate composers use an iterative process that begins with expressing musical ideas on paper, followed by active parallel exploration on paper and in software, prior to final execution of their ideas as an original score. We conducted a participatory design study that focused on the creative exploration phase, to design tools that help composers better integrate their paper-based and electronic activities. We then developed InkSplorer as a technology probe that connects users' hand-written gestures on paper to Max/MSP and OpenMusic. Composers appropriated InkSplorer according to their preferred composition styles, emphasizing its ability to help them quickly explore musical ideas on paper as they interact with the computer. We conclude with recommendations for designing interactive paper tools that support the creative process, letting users explore musical ideas both on paper and electronically. </abstract>
    <keywords>Composer, Creativity, Design Exploration, InkSplorer, Interac- tive Paper, OpenMusic, Technology Probes.  </keywords>
  </document>
  <document>
    <name>nime2011_367.pdf</name>
    <abstract>The DJ culture uses a gesture lexicon strongly rooted in thetraditional setup of turntables and a mixer. As novel toolsare introduced in the DJ community, this lexicon is adaptedto the features they provide. In particular, multitouch technologies can offer a new syntax while still supporting the oldlexicon, which is desired by DJs.We present a classification of DJ tools, from an interaction point of view, that divides the previous work into Traditional, Virtual and Hybrid setups. Moreover, we presenta multitouch tabletop application, developed with a groupof DJ consultants to ensure an adequate implementation ofthe traditional gesture lexicon.To conclude, we conduct an expert evaluation, with tenDJ users in which we compare the three DJ setups with ourprototype. The study revealed that our proposal suits expectations of Club/Radio-DJs, but fails against the mentalmodel of Scratch-DJs, due to the lack of haptic feedback torepresent the record's physical rotation. Furthermore, testsshow that our multitouch DJ setup, reduces task durationwhen compared with Virtual setups.</abstract>
    <keywords>DJing, Multitouch Interaction, Expert User evaluation, HCI </keywords>
  </document>
  <document>
    <name>nime2011_373.pdf</name>
    <abstract>As NIME's focus has expanded beyond the design reportswhich were pervasive in the early days to include studies andexperiments involving music control devices, we report on aparticular area of activity that has been overlooked: designsof music devices in experimental contexts. We demonstratethis is distinct from designing for artistic performances, witha unique set of novel challenges. A survey of methodologicalapproaches to experiments in NIME reveals a tendency torely on existing instruments or evaluations of new devicesdesigned for broader creative application. We present twoexamples from our own studies that reveal the merits ofdesigning purpose-built devices for experimental contexts.</abstract>
    <keywords>Experiment, Methodology, Instrument Design, DMIs </keywords>
  </document>
  <document>
    <name>nime2011_377.pdf</name>
    <abstract>This paper describes the design of Crackle, a interactivesound and touch experience inspired by the CrackleBox.We begin by describing a ruleset for Crackle's interactionderived from the salient interactive qualities of the CrackleBox. An implementation strategy is then described forrealizing the ruleset as an application for the iPhone. Thepaper goes on to consider the potential of using Crackleas an encapsulated interaction paradigm for exploring arbitrary sound spaces, and concludes with lessons learned ondesigning for multitouch surfaces as expressive input sensors.</abstract>
    <keywords>touchscreen, interface topology, mobile music, interaction paradigm, dynamic mapping, CrackleBox, iPhone </keywords>
  </document>
  <document>
    <name>nime2011_381.pdf</name>
    <abstract>This paper introduces Improcess, a novel cross-disciplinarycollaborative project focussed on the design and development of tools to structure the communication between performer and musical process. We describe a 3-tiered architecture centering around the notion of a Common MusicRuntime, a shared platform on top of which inter-operatingclient interfaces may be combined to form new musical instruments. This approach allows hardware devices such asthe monome to act as an extended hardware interface withthe same power to initiate and control musical processesas a bespoke programming language. Finally, we reflect onthe structure of the collaborative project itself, which offers an opportunity to discuss general research strategy forconducting highly sophisticated technical research within aperforming arts environment such as the development of apersonal regime of preparation for performance.</abstract>
    <keywords>Improvisation, live coding, controllers, monome, collabora- tion, concurrency, abstractions </keywords>
  </document>
  <document>
    <name>nime2011_387.pdf</name>
  </document>
  <document>
    <name>nime2011_393.pdf</name>
    <abstract>The design space of fabric multitouch surface interaction is explored with emphasis on novel materials and construction techniques aimed towards reliable, repairable pressure sensing surfaces for musical applications. </abstract>
    <keywords>Multitouch, surface interaction, piezoresistive, fabric sensor, e- textiles, tangible computing, drum controller  </keywords>
  </document>
  <document>
    <name>nime2011_399.pdf</name>
    <abstract>This paper deals with the effects of integrated vibrotactile feedback on the "feel" of a digital musical instrument(DMI). Building on previous work developing a DMI withintegrated vibrotactile feedback actuators, we discuss howto produce instrument-like vibrations, compare these simulated vibrations with those produced by an acoustic instrument and examine how the integration of this feedbackeffects performer ratings of the instrument. We found thatintegrated vibrotactile feedback resulted in an increase inperformer engagement with the instrument, but resulted ina reduction in the perceived control of the instrument. Wediscuss these results and their implications for the design ofnew digital musical instruments.</abstract>
    <keywords>Vibrotactile Feedback, Digital Musical Instruments, Feel, Loudspeakers </keywords>
  </document>
  <document>
    <name>nime2011_405.pdf</name>
    <abstract>This paper presents a series of open-source firmwares for the latest iteration of the popular Arduino microcontroller platform. A portmanteau of Human Interface Device and Arduino, the HIDUINO project tackles a major problem in designing NIMEs: easily and reliably communicating with a host computer using standard MIDI over USB. HIDUINO was developed in conjunction with a class at the California Institute of the Arts intended to teach introductory-level human-computer and human-robot interaction within the context of musical controllers. We describe our frustration with existing microcontroller platforms and our experiences using the new firmware to facilitate the development and prototyping of new music controllers. </abstract>
    <keywords>Arduino, USB, HID, MIDI, HCI, controllers, microcontrollers  </keywords>
  </document>
  <document>
    <name>nime2011_409.pdf</name>
    <abstract>We present a strategy for the improvement of wireless sensor data transmission latency, implemented in two current projects involving gesture/control sound interaction. Our platform was designed to be capable of accepting accessories using a digital bus. The receiver features a IEEE 802.15.4 microcontroller associated to a TCP/IP stack integrated circuit that transmits the received wireless data to a host computer using the Open Sound Control protocol. This paper details how we improved the latency and sample rate of the said technology while keeping the device small and scalable. </abstract>
    <keywords>Embedded sensors, gesture recognition, wireless, sound and music computing, interaction, 802.15.4, Zigbee.     </keywords>
  </document>
  <document>
    <name>nime2011_413.pdf</name>
    <abstract>The Snyderphonics Manta controller is a USB touch controller for music and video. It features 48 capacitive touch sensors, arranged in a hexagonal grid, with bi-color LEDs that are programmable from the computer. The sensors send continuous data proportional to surface area touched, and a velocitydetection algorithm has been implemented to estimate attack velocity based on this touch data. In addition to these hexagonal sensors, the Manta has two high-dimension touch sliders (giving 12-bit values), and four assignable function buttons. In this paper, I outline the features of the controller, the available methods for communicating between the device and a computer, and some current uses for the controller. </abstract>
    <keywords>Snyderphonics, Manta, controller, USB, capacitive, touch, sensor, decoupled LED, hexagon, grid, touch slider, HID, portable, wood, live music, live video  </keywords>
  </document>
  <document>
    <name>nime2011_421.pdf</name>
    <abstract>This paper deals with the usage of bio-data from performers to create interactive multimedia performances or installations. It presents this type of research in some art works produced in the last fifty years (such as Lucier's Music for a Solo Performance, from 1965), including two interactive performances of my authorship, which use two different types of bio-interfaces: on the one hand, an EMG (Electromyography) and on the other hand, an EEG (electroencephalography). The paper explores the interaction between the human body and real-time media (audio and visual) by the usage of bio-interfaces. This research is based on biofeedback investigations pursued by the psychologist Neal E. Miller in the 1960s, mainly based on finding new methods to reduce stress. However, this article explains and shows examples in which biofeedback research is used for artistic purposes only. </abstract>
    <keywords>Live electronics, Butoh, performance, biofeedback, interactive sound and video.  </keywords>
  </document>
  <document>
    <name>nime2011_425.pdf</name>
    <abstract>TresnaNet explores the potential of Telematics as a generator ofmusical expressions. I pretend to sound the silent flow ofinformation from the network.This is realized through the fabrication of a prototypefollowing the intention of giving substance to the intangibleparameters of our communication. The result may haveeducational, commercial and artistic applications because it is aphysical and perceptible representation of the transfer ofinformation over the network. This paper describes the design,implementation and conclusions about TresnaNet.</abstract>
    <keywords>Interface, musical generation, telematics, network, musical instrument, network sniffer. </keywords>
  </document>
  <document>
    <name>nime2011_429.pdf</name>
    <keywords>Music interfaces, music therapy, modifiable interfaces, design tools, Human-Technology Interaction (HTI), User-Centred Design (UCD), design for all (DfA), prototyping, performance.   </keywords>
  </document>
  <document>
    <name>nime2011_433.pdf</name>
    <abstract>Motion-based interactive systems have long been utilizedin contemporary dance performances. These performancesbring new insight to sound-action experiences in multidisciplinary art forms. This paper discusses the related technology within the framework of the dance piece, Raja. The performance set up of Raja gives a possibility to use two complementary tracking systems and two alternative choices formotion sensors in real-time audio-visual synthesis.</abstract>
    <keywords>raja, performance, dance, motion sensor, accelerometer, gyro, positioning, sonification, pure data, visualization, Qt </keywords>
  </document>
  <document>
    <name>nime2011_437.pdf</name>
    <keywords>Controller, Sensor, MIDI, USB, Computer Music, USB, OSC, CV, MIDI, DMX, A/D Converter, Interface.  </keywords>
  </document>
  <document>
    <name>nime2011_441.pdf</name>
    <abstract>The aim of this study was to investigate how well subjectsbeat out a rhythm using eye movements and to establishthe most accurate method of doing this. Eighteen subjectsparticipated in an experiment were five different methodswere evaluated. A fixation based method was found to bethe most accurate. All subjects were able to synchronizetheir eye movements with a given beat but the accuracywas much lower than usually found in finger tapping studies. Many parts of the body are used to make music but sofar, with a few exceptions, the eyes have been silent. The research presented here provides guidelines for implementingeye controlled musical interfaces. Such interfaces would enable performers and artists to use eye movement for musicalexpression and would open up new, exiting possibilities.</abstract>
    <keywords>Rhythm, Eye tracking, Sensorimotor synchronization, Eye tapping </keywords>
  </document>
  <document>
    <name>nime2011_445.pdf</name>
    <abstract>I present MelodyMorph, a reconfigurable musical instrument designed with a focus on melodic improvisation. It is designed for a touch-screen interface, and allows the user to create "bells" which can be tapped to play a note, and dragged around on a pannable and zoomable canvas. Colors, textures and shapes of the bells represent pitch and timbre properties. "Recorder bells" can store and play back performances. Users can construct instruments that are modifiable as they play, and build up complex melodies hierarchically from simple parts. </abstract>
    <keywords>Melody, improvisation, representation, multi-touch, iPad  </keywords>
  </document>
  <document>
    <name>nime2011_453.pdf</name>
    <abstract>In this paper we discuss how the band 000000Swan uses machine learning to parse complex sensor data and create intricate artistic systems for live performance. Using the Wekinator software for interactive machine learning, we have created discrete and continuous models for controlling audio and visual environments using human gestures sensed by a commercially-available sensor bow and the Microsoft Kinect. In particular, we have employed machine learning to quickly and easily prototype complex relationships between performer gesture and performative outcome. </abstract>
    <keywords>Wekinator, K-Bow, Machine Learning, Interactive, Multimedia, Kinect, Motion-Tracking, Bow Articulation, Animation  </keywords>
  </document>
  <document>
    <name>nime2011_457.pdf</name>
    <abstract>In the past decade we have seen a growing presence of tabletop systems applied to music, lately with even some products becoming commercially available and being used byprofessional musicians in concerts. The development of thistype of applications requires several demanding technicalexpertises such as input processing, graphical design, realtime sound generation or interaction design, and because ofthis complexity they are usually developed by a multidisciplinary group.In this paper we present the Musical Tabletop CodingFramework (MTCF) a framework for designing and codingmusical tabletop applications by using the graphical programming language for digital sound processing Pure Data(Pd). With this framework we try to simplify the creationprocess of such type of interfaces, by removing the need ofany programming skills other than those of Pd.</abstract>
    <keywords>Pure Data, tabletop, tangible, framework </keywords>
  </document>
  <document>
    <name>nime2011_465.pdf</name>
    <abstract>This paper documents the first developmental phase of aninterface that enables the performance of live music usinggestures and body movements. The work included focuseson the first step of this project: the composition and performance of live music using hand gestures captured using asingle data glove. The paper provides a background to thefield, the aim of the project and a technical description ofthe work completed so far. This includes the developmentof a robust posture vocabulary, an artificial neural networkbased posture identification process and a state-based system to map identified postures onto a set of performanceprocesses. The paper is closed with qualitative usage observations and a projection of future plans.</abstract>
    <keywords>Music Controller, Gestural Music, Data Glove, Neural Net- work, Live Music Composition, Looping, Imogen Heap </keywords>
  </document>
  <document>
    <name>nime2011_469.pdf</name>
    <abstract>The use of non-invasive electroencephalography (EEG) in the experimental arts is not a novel concept. Since 1965, EEG has been used in a large number of, sometimes highly sophisticated, systems for musical and artistic expression. However, since the advent of the synthesizer, most such systems have utilized digital and/or synthesized media in sonifying the EEG signals. There have been relatively few attempts to create interfaces for musical expression that allow one to mechanically manipulate acoustic instruments by modulating one's mental state. Secondly, few such systems afford a distributed performance medium, with data transfer and audience participation occurring over the Internet. The use of acoustic instruments and Internet-enabled communication expands the realm of possibilities for musical expression in Brain-Computer Music Interfaces (BCMI), while also introducing additional challenges. In this paper we report and examine a first demonstration (Music for Online Performer) of a novel system for Internet-enabled manipulation of robotic acoustic instruments, with feedback, using a non-invasive EEG-based BCI and low-cost, commercially available robotics hardware. </abstract>
    <keywords>EEG, Brain-Computer Music Interface, Internet, Arduino.  </keywords>
  </document>
  <document>
    <name>nime2011_473.pdf</name>
    <abstract>A shoe-based interface is presented, which enables users toplay percussive virtual instruments by tapping their feet.The wearable interface consists of a pair of sandals equippedwith four force sensors and four actuators affording audiotactile feedback. The sensors provide data via wireless transmission to a host computer, where they are processed andmapped to a physics-based sound synthesis engine. Sincethe system provides OSC and MIDI compatibility, alternative electronic instruments can be used as well. The audiosignals are then sent back wirelessly to audio-tactile excitersembedded in the sandals' sole, and optionally to headphonesand external loudspeakers. The round-trip wireless communication only introduces very small latency, thus guaranteeing coherence and unity in the multimodal percept andallowing tight timing while playing.</abstract>
    <keywords>interface, audio, tactile, foot tapping, embodiment, footwear, wireless, wearable, mobile </keywords>
  </document>
  <document>
    <name>nime2011_477.pdf</name>
    <abstract>We present a generic, structured model for design and evaluation of musical interfaces. This model is developmentoriented, and it is based on the fundamental function of themusical interfaces, i.e., to coordinate the human action andperception for musical expression, subject to human capabilities and skills. To illustrate the particulars of this modeland present it in operation, we consider the previous designand evaluation phase of iPalmas, our testbed for exploringrhythmic interaction. Our findings inform the current design phase of iPalmas visual and auditory displays, wherewe build on what has resonated with the test users, and explore further possibilities based on the evaluation results.</abstract>
  </document>
  <document>
    <name>nime2011_481.pdf</name>
    <abstract>This paper introduces and evaluates a novel methodologyfor the estimation of bow pressing force in violin performance, aiming at a reduced intrusiveness while maintaininghigh accuracy. The technique is based on using a simplifiedphysical model of the hair ribbon deflection, and feeding thismodel solely with position and orientation measurements ofthe bow and violin spatial coordinates. The physical modelis both calibrated and evaluated using real force data acquired by means of a load cell.</abstract>
    <keywords>bow pressing force, bow force, pressing force, force, violin playing, bow simplified physical model, 6DOF, hair ribbon ends, string ends </keywords>
  </document>
  <document>
    <name>nime2011_487.pdf</name>
    <abstract>Remixing audio samples is a common technique for the creation of electronic music, and there are a wide variety oftools available to edit, process, and recombine pre-recordedaudio into new compositions. However, all of these toolsconceive of the timeline of the pre-recorded audio and theplayback timeline as identical. In this paper, we introducea dual time axis representation in which these two timelines are described explicitly. We also discuss the randomaccess remix application for the iPad, an audio sample editor based on this representation. We describe an initialuser study with 15 high school students that indicates thatthe random access remix application has the potential todevelop into a useful and interesting tool for composers andperformers of electronic music.</abstract>
    <keywords>interactive systems, sample editor, remix, iPad, multi-touch </keywords>
  </document>
  <document>
    <name>nime2011_491.pdf</name>
    <abstract>This paper outlines the formation of the Expanded Performance (EP) trio, a chamber ensemble comprised of electriccello with sensor bow, augmented digital percussion, anddigital turntable with mixer. Decisions relating to physical set-ups and control capabilities, sonic identities, andmappings of each instrument, as well as their roles withinthe ensemble, are explored. The contributions of these factors to the design of a coherent, expressive ensemble andits emerging performance practice are considered. The trioproposes solutions to creation, rehearsal and performanceissues in ensemble live electronics.</abstract>
    <keywords>Live electronics, digital performance, mapping, chamber music, ensemble, instrument identity </keywords>
  </document>
  <document>
    <name>nime2011_495.pdf</name>
    <abstract>We present observations from two separate studies of spectators' perceptions of musical performances, one involvingtwo acoustic instruments, the other two electronic instruments. Both studies followed the same qualitative method,using structured interviews to ascertain and compare spectators' experiences. In this paper, we focus on outcomespertaining to perceptions of the performers' skill, relatingto concepts of embodiment and communities of practice.</abstract>
    <keywords>skill, embodiment, perception, effort, control, spectator </keywords>
  </document>
  <document>
    <name>nime2011_499.pdf</name>
    <keywords>Computer music, programming language, the psychology of programming, usability  </keywords>
  </document>
  <document>
    <name>nime2011_503.pdf</name>
    <abstract>This paper introduces the Seaboard, a new tangible musicalinstrument which aims to provide musicians with significantcapability to manipulate sound in real-time in a musicallyintuitive way. It introduces the core design features whichmake the Seaboard unique, and describes the motivationand rationale behind the design. The fundamental approachto dealing with problems associated with discrete and continuous inputs is summarized.</abstract>
    <keywords>Piano keyboard-related interface, continuous and discrete control, haptic feedback, Human-Computer Interaction (HCI) </keywords>
  </document>
  <document>
    <name>nime2011_507.pdf</name>
    <keywords>Interactive music, public displays, user experience, out-of- home media, algorithmic composition, soft constraints  </keywords>
  </document>
  <document>
    <name>nime2011_511.pdf</name>
    <abstract>The traditional role of the musical instrument is to be the working tool of the professional musician. On the instrument the musician performs music for the audience to listen to. In this paper we present an interactive installation, where we expand the role of the instrument to motivate musicking and cocreation between diverse users. We have made an open installation, where users can perform a variety of actions in several situations. By using the abilities of the computer, we have made an installation, which can be interpreted to have many roles. It can both be an instrument, a co-musician, a communication partner, a toy, a meeting place and an ambient musical landscape. The users can dynamically shift between roles, based on their abilities, knowledge and motivation. </abstract>
  </document>
  <document>
    <name>nime2011_515.pdf</name>
    <abstract>We developed very small and light sensors, each equippedwith 3-axes accelerometers, magnetometers and gyroscopes.Those MARG (Magnetic, Angular Rate, and Gravity) sensors allow for a drift-free attitude computation which in turnleads to the possibility of recovering the skeleton of bodyparts that are of interest for the performance, improvingthe results of gesture recognition and allowing to get relative position between the extremities of the limbs and thetorso of the performer. This opens new possibilities in termsof mapping. We kept our previous approach developed atARTeM [2]: wireless from the body to the host computer,but wired through a 4-wire digital bus on the body. Byrelieving the need for a transmitter on each sensing node,we could built very light and flat sensor nodes that can bemade invisible under the clothes. Smaller sensors, coupledwith flexible wires on the body, give more freedom of movement to dancers despite the need for cables on the body.And as the weight of each sensor node, box included, isonly 5 grams (Figure 1), they can also be put on the upper and lower arm and hand of a violin or viola player, toretrieve the skeleton from the torso to the hand, withoutadding any weight that would disturb the performer. Weused those sensors in several performances with a dancingviola player and in one where she was simultaneously controlling gas flames interactively. We are currently applyingthem to other types of musical performances.</abstract>
    <keywords>wireless MARG sensors </keywords>
  </document>
  <document>
    <name>nime2011_519.pdf</name>
    <abstract>This paper covers and also describes an ongoing research project focusing on new artistic possibilities by exchanging music technological methods and techniques between two distinct musical genres. Through my background as a guitarist and composer in an experimental metal band I have experienced a vast development in music technology during the last 20 years. This development has made a great impact in changing the procedures for composing and producing music within my genre without necessarily changing the strategies of how the technology is used. The transition from analogue to digital sound technology not only opened up new ways of manipulating and manoeuvring sound, it also opened up challenges in how to integrate and control the digital sound technology as a seamless part of my musical genre. By using techniques and methods known from electro-acoustic/computer music, and adapting them for use within my tradition, this research aims to find new strategies for composing and producing music within my genre. </abstract>
    <keywords>Artistic research, strategies for composition and production, convolution, environmental sounds, real time control  </keywords>
  </document>
  <document>
    <name>nime2011_523.pdf</name>
    <keywords>LPC, software instrument, analysis, modeling, csound  </keywords>
  </document>
  <document>
    <name>nime2011_527.pdf</name>
    <abstract>Gliss is an application for iOS that lets the user sequence five separate instruments and play them back in various ways. Sequences can be created by drawing onto the screen while the sequencer is running. The playhead of the sequencer can be set to randomly deviate from the drawings or can be controlled via the accelerometer of the device. This makes Gliss a hybrid of a sequencer, an instrument and a generative music system. </abstract>
    <keywords>Gliss, iOS, iPhone, iPad, interface, UPIC, music, sequencer, accelerometer, drawing  </keywords>
  </document>
  <document>
    <name>nime2011_529.pdf</name>
    <abstract>This paper describes a new musical instrument inspired by the pedal-steel guitar, along with its motivations and other considerations. Creating a multi-dimensional, expressive instrument was the primary driving force. For these criteria the pedal steel guitar proved an apt model as it allows control over several instrument parameters simultaneously and continuously. The parameters we wanted control over were volume, timbre, release time and pitch.The Quadrofeelia is played with two hands on a horizontal surface. Single notes and melodies are easily played as well as chordal accompaniment with a variety of timbres and release times enabling a range of legato and staccato notes in an intuitive manner with a new yet familiar interface.</abstract>
    <keywords>NIME, pedal-steel, electronic, slide, demonstration, membrane, continuous, ribbon, instrument, polyphony, lead </keywords>
  </document>
  <document>
    <name>nime2011_533.pdf</name>
    <abstract>In this paper, we suggest a conceptual model of a Web application framework for the composition and documentation of soundscape and introduce corresponding prototype projects, SeoulSoundMap and SoundScape Composer. We also survey the current Web-based sound projects in terms of soundscape documentation. </abstract>
    <keywords>soundscape, web application framework, sound archive, sound map, soundscape composition, soundscape documentation.   </keywords>
  </document>
  <document>
    <name>nime2011_535.pdf</name>
    <abstract>We are presenting a set of applications that have been realized with the MO modular wireless motion capture deviceand a set of software components integrated into Max/MSP.These applications, created in the context of artistic projects,music pedagogy, and research, allow for the gestural reembodiment of recorded sound and music. They demonstrate a large variety of different "playing techniques" inmusical performance using wireless motion sensor modulesin conjunction with gesture analysis and real-time audioprocessing components.</abstract>
    <keywords>Music, Gesture, Interface, Wireless Sensors, Gesture Recog- nition, Audio Processing, Design, Interaction </keywords>
  </document>
  <document>
    <name>nime2011_537.pdf</name>
    <abstract>(land)moves is an interactive installation: the user's gestures control the multimedia processing with a total synergybetween audio and video synthesis and treatment.</abstract>
    <keywords>mapping gesture-audio-video, gesture recognition, landscape, soundscape </keywords>
  </document>
  <document>
    <name>nime2011_539.pdf</name>
    <abstract>Haptic interfaces using active force-feedback have mostly been used for emulating existing instruments and making conventional music. With the right speed, force, precision and software they can also be used to make new sounds and perhaps new music. The requirements are local microprocessors (for low-latency and high update rates), strategic sensors (for force as well as position), and non-linear dynamics (that make for rich overtones and chaotic music).</abstract>
    <keywords>NIME, Haptics, Music Controllers, Microprocessors.  </keywords>
  </document>
  <document>
    <name>nime2012_009.pdf</name>
    <abstract>This paper describes three hardware devices for integrating modular synthesizers with computers, each with a different approach to the relationship between hardware and software. The devices discussed are the USB-Octomod, an 8-channel OSC-compatible computer-controlled control-voltage generator, the tabulaRasa, a hardware table-lookup oscillator synthesis module with corresponding waveform design software, and the pucktronix.snake.corral, a dual 8x8 computer-controlled analog signal routing matrix. The devices make use of open-source hardware and software, and are designed around affordable micro-controllers and integrated circuits. </abstract>
  </document>
  <document>
    <name>nime2012_020.pdf</name>
    <abstract>The Sonik Spring is a portable and wireless digital instrument, created for real-time synthesis and control of sound. It brings together different types of sensory input, linking gestural motion and kinesthetic feedback to the production of sound. The interface consists of a 15-inch spring with unique flexibility, which allows multiple degrees of variation in its shape and length. The design of the instrument is described and its features discussed. Three performance modes are detailed highlighting the instrument's expressive potential and wide range of functionality. </abstract>
    <keywords>Interface for sound and music, Gestural control of sound, Kinesthetic and visual feedback  </keywords>
  </document>
  <document>
    <name>nime2012_025.pdf</name>
    <keywords>Antheil, Stravinsky, player piano, pianola, mechanical instruments, synchronization  </keywords>
  </document>
  <document>
    <name>nime2012_030.pdf</name>
    <abstract>This paper presents ongoing work on methods dedicated torelations between composers and performers in the contextof experimental music. The computer music community hasover the last decade paid a strong interest on various kindsof gestural interfaces to control sound synthesis processes.The mapping between gesture and sound parameters hasspecially been investigated in order to design the most relevant schemes of sonic interaction. In fact, this relevanceresults in an aesthetic choice that encroaches on the process of composition. This work proposes to examine therelations between composers and performers in the contextof the new interfaces for musical expression. It aims to define a theoretical and methodological framework clarifyingthese relations. In this project, this paper is the first experimental study about the use of physical models as gesturalmaps for the production of textural sounds.</abstract>
    <keywords>Simulation, Interaction, Sonic textures </keywords>
  </document>
  <document>
    <name>nime2012_036.pdf</name>
    <abstract>Musician Maker is a system to allow novice players the opportunity to create expressive improvisational music. While the system plays an accompaniment background chord progression, each participant plays some kind of controller to make music through the system. The program takes the signals from the controllers and adjusts the pitches somewhat so that the players are limited to notes which fit the chord progression. The various controllers are designed to be very easy and intuitive so anyone can pick one up and quickly be able to play it. Since the computer is making sure that wrong notes are avoided, even inexperienced players can immediately make music and enjoy focusing on some of the more expressive elements and thus become musicians. </abstract>
    <keywords>Musical Instrument, Electronic, Computer Music, Novice, Controller  </keywords>
  </document>
  <document>
    <name>nime2012_037.pdf</name>
    <abstract>Force-feedback devices can provide haptic feedback duringinteraction with physical models for sound synthesis. However, low-end devices may not always provide high-fidelitydisplay of the acoustic characteristics of the model. This article describes an enhanced handle for the Phantom Omnicontaining a vibration actuator intended to display the highfrequency portion of the synthesized forces. Measurementsare provided to show that this approach achieves a morefaithful representation of the acoustic signal, overcominglimitations in the device control and dynamics.</abstract>
    <keywords>Haptics, force feedback, bowing, audio, interaction </keywords>
  </document>
  <document>
    <name>nime2012_047.pdf</name>
    <abstract>With the advent of high resolution digital video projection and high quality spatial sound systems in modern planetariums, the planetarium can become the basis for a unique set of virtual musical instrument capabilities that go well beyond packaged multimedia shows. The dome, circular speaker and circular seating arrangements provide means for skilled composers and performers to create a virtual reality in which attendees are immersed in the composite instrument. This initial foray into designing an audio-visual computerbased instrument for improvisational performance in a planetarium builds on prior, successful work in mapping the rules and state of two-dimensional computer board games to improvised computer music. The unique visual and audio geometries of the planetarium present challenges and opportunities. The game tessellates the dome in mobile, colored hexagons that emulate both atoms and musical scale intervals in an expanding universe. Spatial activity in the game maps to spatial locale and instrument voices in the speakers, in essence creating a virtual orchestra with a string section, percussion section, etc. on the dome. Future work includes distribution of game play via mobile devices to permit attendees to participate in a performance. This environment is open-ended, with great educational and aesthetic potential. </abstract>
    <keywords>aleatory music, algorithmic improvisation, computer game, planetarium  </keywords>
  </document>
  <document>
    <name>nime2012_048.pdf</name>
    <abstract>This paper is an in depth exploration of the fashion object and device, the Play-A-Grill. It details inspirations, socio-cultural implications, technical function and operation, and potential applications for the Play-A-Grill system. </abstract>
    <keywords>Digital Music Players, Hip Hop, Rap, Music Fashion, Grills, Mouth Jewelry, Mouth Controllers, and Bone Conduction Hearing.  </keywords>
  </document>
  <document>
    <name>nime2012_057.pdf</name>
    <abstract>Sound generators and synthesis engines expose a large set of parameters, allowing run-time timbre morphing and exploration of sonic space. However, control over these high-dimensional interfaces is constrained by the physical limitations of performers. In this paper we propose the exploitation of vocal gesture as an extension or alternative to traditional physical controllers. The approach uses dynamic aspects of vocal sound to control variations in the timbre of the synthesized sound. The mapping from vocal to synthesis parameters is automatically adapted to information extracted from vocal examples as well as to the relationship between parameters and timbre within the synthesizer. The mapping strategy aims to maximize the breadth of the explorable perceptual sonic space over a set of the synthesizer's real-valued parameters, indirectly driven by the voice-controlled interface. </abstract>
  </document>
  <document>
    <name>nime2012_060.pdf</name>
    <abstract>In this work, a comprehensive study is performed on the relationship between audio, visual and emotion by applying the principles of cognitive emotion theory into digital creation. The study is driven by an audiovisual emotion library project that is named AVIEM, which provides an interactive interface for experimentation and evaluation of the perception and creation processes of audiovisuals. AVIEM primarily consists of separate audio and visual libraries and grows with user contribution as users explore different combinations between them. The library provides a wide range of experimentation possibilities by allowing users to create audiovisual relations and logging their emotional responses through its interface. Besides being a resourceful tool of experimentation, AVIEM aims to become a source of inspiration, where digitally created abstract virtual environments and soundscapes can elicit target emotions at a preconscious level, by building genuine audiovisual relations that would engage the viewer on a strong emotional stage. Lastly, various schemes are proposed to visualize information extracted through AVIEM, to improve the navigation and designate the trends and dependencies among audiovisual relations. </abstract>
    <keywords>Designing emotive audiovisuals, cognitive emotion theory,  audiovisual perception and interaction, synaesthesia  </keywords>
  </document>
  <document>
    <name>nime2012_061.pdf</name>
    <abstract>Tok! is a collaborative acoustic instrument application for iOS devices aimed at real time percussive music making in a colocated setup. It utilizes the mobility of hand-held devices and transforms them into drumsticks to tap on flat surfaces and produce acoustic music. Tok! is also networked and consists of a shared interactive music score to which the players tap their phones, creating a percussion ensemble. Through their social interaction and real-time modifications to the music score, and through their creative selection of tapping surfaces, the players can collaborate and dynamically create interesting rhythmic music with a variety of timbres. </abstract>
    <keywords>Mobile Phones, Collaboration, Social Interaction, Acoustic  Musical Instrument  </keywords>
  </document>
  <document>
    <name>nime2012_062.pdf</name>
    <abstract>This paper describes recent extensions to LOLC, a text-based environment for collaborative improvisation for laptop ensembles, which integrate acoustic instrumental musicians into the environment. Laptop musicians author short commands to create, transform, and share pre-composed musical fragments, and the resulting notation is digitally displayed, in real time, to instrumental musicians to sight-read in performance. The paper describes the background and motivations of the project, outlines the design of the original LOLC environment and describes its new real-time notation components in detail, and explains the use of these new components in a musical composition, SGLC, by one of the authors. </abstract>
  </document>
  <document>
    <name>nime2012_063.pdf</name>
    <abstract>Platforms for mobile computing and gesture recognitionprovide enticing interfaces for creative expression on virtualmusical instruments. However, sound synthesis on thesesystems is often limited to sample-based synthesizers, whichlimits their expressive capabilities. Source-filter models areadept for such interfaces since they provide flexible, algorithmic sound synthesis, especially in the case of the guitar.In this paper, we present a data-driven approach for modeling guitar excitation signals using principal componentsderived from a corpus of excitation signals. Using thesecomponents as features, we apply nonlinear principal components analysis to derive a feature space that describesthe expressive attributes characteristic to our corpus. Finally, we propose using the reduced dimensionality space asa control interface for an expressive guitar synthesizer.</abstract>
    <keywords>Source-filter models, musical instrument synthesis, PCA, touch musical interfaces </keywords>
  </document>
  <document>
    <name>nime2012_064.pdf</name>
    <abstract>This article proposes a wireless handheld multimedia digital instrument, which allows one to compose and perform digital music for films in real-time. Not only does it allow the performer and the audience to follow the film images in question, but also the relationship between the gestures performed and the sound generated. Furthermore, it allows one to have an effective control over the sound, and consequently achieve great musical expression. In addition, a method for calibrating the multimedia digital instrument, devised to overcome the lack of a reliable reference point of the accelerometer and a process to obtain a video score are presented. This instrument has been used in a number of concerts (Portugal and Brazil) so as to test its robustness.  </abstract>
  </document>
  <document>
    <name>nime2012_066.pdf</name>
    <abstract>Tweet Harp is a musical instrument using Twitter and a laser harp. This instrument features the use of the human voice speaking tweets in Twitter as sounds for music. It is played by touching the six harp strings of laser beams. Tweet Harp gets the latest tweets from Twitter in real-time, and it creates music like a song with unexpected words. It also creates animation displaying the texts at the same time. The audience can visually enjoy this performance by sounds synchronized with animation. If the audience has a Twitter account, they can participate in the performance by tweeting. </abstract>
    <keywords>Twitter, laser harp, text, speech, voice, AppleScript, Quartz Composer, Max/MSP, TTS, Arduino  </keywords>
  </document>
  <document>
    <name>nime2012_068.pdf</name>
    <abstract>Machine learning models are useful and attractive tools forthe interactive computer musician, enabling a breadth of interfaces and instruments. With current consumer hardwareit becomes possible to run advanced machine learning algorithms in demanding performance situations, yet expertiseremains a prominent entry barrier for most would-be users.Currently available implementations predominantly employsupervised machine learning techniques, while the adaptive,self-organizing capabilities of unsupervised models are notgenerally available. We present a free, new toolbox of unsupervised machine learning algorithms implemented in Max5 to support real-time interactive music and video, aimedat the non-expert computer artist.</abstract>
    <keywords>NIME, unsupervised machine learning, adaptive resonance theory, self-organizing maps, Max 5 </keywords>
  </document>
  <document>
    <name>nime2012_070.pdf</name>
    <abstract>We introduce a prototype of a new tangible step sequencerthat transforms everyday objects into percussive musicalinstruments. DrumTop adapts our everyday task-orientedhand gestures with everyday objects as the basis of musicalinteraction, resulting in an easily graspable musical interfacefor musical novices. The sound, tactile, and visual feedbackcomes directly from everyday objects as the players programdrum patterns and rearrange the objects on the tabletopinterface. DrumTop encourages the players to explore themusical potentiality of their surroundings and be musicallycreative through rhythmic interactions with everyday objects. The interface consists of transducers that trigger ahit, causing the objects themselves to produce sound whenthey are in close contact with the transducers. We discusshow we designed and implemented our current DrumTopprototype and describe how players interact with the interface. We then highlight the players' experience with Drumtop and our plans for future work in the fields of musiceducation and performance.</abstract>
    <keywords>Tangible User Interfaces, Playful Experience, Percussion, Step Sequencer, Transducers, Everyday Objects </keywords>
  </document>
  <document>
    <name>nime2012_073.pdf</name>
    <abstract>This paper demonstrates the practical benefits and performance opportunities of using the dual-analog gamepad as a controller for real-time live electronics. Numerous diverse instruments and interfaces, as well as detailed control mappings, are described. Approaches to instrument and preset switching are also presented. While all of the instrument implementations presented are made available through the Martingale Pd library, resources for other synthesis languages are also described. </abstract>
    <keywords>Controllers, live electronics, dual-analog, gamepad, joystick, computer music, instrument, interface  </keywords>
  </document>
  <document>
    <name>nime2012_074.pdf</name>
    <abstract>Potential users of audio production software, such as parametric audio equalizers, may be discouraged by the complexity of the interface. A new approach creates a personalized on-screen slider that lets the user manipulate the audio in terms of a descriptive term (e.g. "warm"), without the user needing to learn or use the interface of an equalizer. This system learns mappings by presenting a sequence of sounds to the user and correlating the gain in each frequency band with the user's preference rating. The system speeds learning through transfer learning. Results on a study of 35 participants show how an effective, personalized audio manipulation tool can be automatically built after only three ratings from the user.  </abstract>
    <keywords>Human computer interaction, music, multimedia production, transfer learning  </keywords>
  </document>
  <document>
    <name>nime2012_077.pdf</name>
    <abstract>We describe a system that allows non-programmers to specify the grammar for a novel graphic score notation of theirown design, defining performance notations suitable for drawing in live situations on a surface such as a whiteboard. Thescore can be interpreted via the camera of a smartphone,interactively scanned over the whiteboard to control theparameters of synthesisers implemented in Overtone. Thevisual grammar of the score, and its correspondence to thesound parameters, can be defined by the user with a simple visual condition-action language. This language can beedited on the touchscreen of an Android phone, allowingthe grammar to be modified live in performance situations.Interactive scanning of the score is visible to the audience asa performance interface, with a colour classifier and visualfeature recogniser causing the grammar-specified events tobe sent using OSC messages via Wi-Fi from the hand-heldsmartphone to an audio workstation.</abstract>
    <keywords>Graphic Notation, Disposable Notation, Live Coding, Com- puter Vision, Mobile Music </keywords>
  </document>
  <document>
    <name>nime2012_082.pdf</name>
    <abstract>In this paper we present a multimodal system for analyzing drum performance. In the first example we perform automatic drum hand recognition utilizing a technique for automatic labeling of training data using direct sensors, and only indirect sensors (e.g. a microphone) for testing. Left/Right drum hand recognition is achieved with an average accuracy of 84.95% for two performers. Secondly we provide a study investigating multimodality dependent performance metrics analysis. </abstract>
    <keywords>Multimodality, Drum stroke identification, surrogate sensors, surrogate data training, machine learning, music information retrieval, performance metrics  </keywords>
  </document>
  <document>
    <name>nime2012_094.pdf</name>
    <keywords>Multi-Touch, User Study, Relational-point interface </keywords>
  </document>
  <document>
    <name>nime2012_096.pdf</name>
    <abstract>TedStick is a new wireless musical instrument that processesacoustic sounds resonating within its wooden body and manipulates them via gestural movements. The sounds aretransduced by a piezoelectric sensor inside the wooden body,so any tactile contact with TedStick is transmitted as audioand further processed by a computer. The main methodfor performing with TedStick focuses on extracting diversesounds from within the resonant properties of TedStick itself. This is done by holding TedStick in one hand anda standard drumstick in the opposite hand while tapping,rubbing, or scraping the two against each other. Gesturalmovements of TedStick are then mapped to parameters forseveral sound effects including pitch shift, delay, reverb andlow/high pass filters. Using this technique the hand holdingthe drumstick can control the acoustic sounds/interactionbetween the sticks while the hand holding TedStick can focus purely on controlling the sound manipulation and effectsparameters.</abstract>
    <keywords>tangible user interface, piezoelectric sensors, gestural per- formance, digital sound manipulation </keywords>
  </document>
  <document>
    <name>nime2012_098.pdf</name>
    <abstract>WIS platform is a wireless interactive sensor platform designed to support dynamic and interactive applications. Theplatform consists of a capture system which includes multiple on-body Zigbee compatible motion sensors, a processingunit and an audio-visual display control unit. It has a complete open architecture and provides interfaces to interactwith other user-designed applications. Therefore, WIS platform is highly extensible. Through gesture recognitions byon-body sensor nodes and data processing, WIS platformcan offer real-time audio and visual experiences to the users.Based on this platform, we set up a multimedia installationthat presents a new interaction model between the participants and the audio-visual environment. Furthermore, weare also trying to apply WIS platform to other installationsand performances.</abstract>
    <keywords>Interactive, Audio-visual experience </keywords>
  </document>
  <document>
    <name>nime2012_099.pdf</name>
    <abstract>In this paper, we introduce Kritaanjli, a robotic harmonium. Details concerning the design, construction, and useof Kritaanjli are discussed. After an examination of relatedwork, quantitative research concerning the hardware chosenin the construction of the instrument is shown, as is a thorough exposition of the design process and use of CAD/CAMtechniques in the design lifecycle of the instrument. Additionally, avenues for future work and compositional practices are focused upon, with particular emphasis placed onhuman/robot interaction, pedagogical techniques affordedby the robotic instrument, and compositional avenues madeaccessible through the use of Kritaanjli.</abstract>
  </document>
  <document>
    <name>nime2012_100.pdf</name>
    <abstract>A problem with many contemporary musical robotic percussion systems lies in the fact that solenoids fail to respond linearly to linear increases in input velocity. This nonlinearityforces performers to individually tailor their compositionsto specific robotic drummers. To address this problem, weintroduce a method of pre-performance calibration usingmetaheuristic search techniques. A variety of such techniques are introduced and evaluated and the results of theoptimized solenoid-based percussion systems are presentedand compared with output from non-calibrated systems.</abstract>
  </document>
  <document>
    <name>nime2012_101.pdf</name>
    <abstract>The EMvibe is an augmented vibraphone that allows forcontinuous control over the amplitude and spectrum of individual notes. The system uses electromagnetic actuatorsto induce vibrations in the vibraphone's aluminum tonebars. The tone bars and the electromagnetic actuators arecoupled via neodymium magnets affixed to each bar. Theacoustic properties of the vibraphone allowed us to developa very simple, low-cost and powerful amplification solutionthat requires no heat sinking. The physical design is meantto be portable and robust, and the system can be easily installed on any vibraphone without interfering with normalperformance techniques. The system supports multiple interfacing solutions, affording the performer and composerthe ability to interact with the EMvibe in different waysdepending on the musical context.</abstract>
    <keywords>Vibraphone, augmented instrument, electromagnetic actu- ation </keywords>
  </document>
  <document>
    <name>nime2012_102.pdf</name>
    <keywords>Augmented instruments, controllers, motion tracking, map- ping </keywords>
  </document>
  <document>
    <name>nime2012_105.pdf</name>
    <abstract>The upper limit of frequency sensitivity for vibrotactile stimulation of the fingers and hand is commonly accepted as 1 kHz. However, during the course of our research to develop a full-hand vibrotactile musical communication device for the hearing-impaired, we repeatedly found evidence suggesting sensitivity to higher frequencies. Most of the studies on which vibrotactile sensitivity are based have been conducted using sine tones delivered by point-contact actuators. The current study was designed to investigate vibrotactile sensitivity using complex signals and full, open-hand contact with a flat vibrating surface representing more natural environmental conditions. Sensitivity to frequencies considerably higher than previously reported was demonstrated for all the signal types tested. Furthermore, complex signals seem to be more easily detected than sine tones, especially at low frequencies. Our findings are applicable to a general understanding of sensory physiology, and to the development of new vibrotactile display devices for music and other applications. </abstract>
    <keywords>Haptic Sensitivity, Hearing-impaired, Vibrotactile Threshold  </keywords>
  </document>
  <document>
    <name>nime2012_108.pdf</name>
    <abstract>In this paper strategies for augmenting the social dimensionof collaborative music making, in particular in the formof bodily and situated interaction are presented. Mobileinstruments are extended by means of relational descriptors democratically controlled by the group and mapped tosound parameters. A qualitative evaluation approach is described and a user test with participants playing in groupsof three conducted. The results of the analysis show corecategories such as familiarity with instrument and situation , shift of focus in activity , family of interactionsand different categories of the experience emerging fromthe interviews. Our evaluation shows the suitability of ourapproach but also the need for iterating on our design on thebasis of the perspectives brought forth by the users. Thislatter observation confirms the importance of conducting athorough interview session followed by data analysis on theline of grounded theory.</abstract>
    <keywords>Collaborative music making, evaluation methods, mobile music, human-human interaction. </keywords>
  </document>
  <document>
    <name>nime2012_109.pdf</name>
    <keywords>music, cochlear implants, perception, rehabilitation, auditory training, interactive learning, client-centred software   </keywords>
  </document>
  <document>
    <name>nime2012_114.pdf</name>
    <abstract>The Electric Slide Organistrum (Figure 1) is an acousticstringed instrument played through a video capture system.The vibration of the instrument string is generated electromagnetically and the pitch variation is achieved by movements carried out by the player in front of a video camera.This instrument results from integrating an ancient technique for the production of sounds as it is the vibration ofa string on a soundbox and actual human-computer interaction technology such as motion detection.Figure 1: Electric Slide Organistrum.</abstract>
    <keywords>Gestural Interface, eBow, Pickup, Bowed string, Electro- magnetic actuation </keywords>
  </document>
  <document>
    <name>nime2012_117.pdf</name>
    <abstract>There is growing interest in the field of augmented musicalinstruments, which extend traditional acoustic instrumentsusing new sensors and actuators. Several designs use electromagnetic actuation to induce vibrations in the acousticmechanism, manipulating the traditional sound of the instrument without external speakers. This paper presentstechniques and guidelines for the use of electromagnetic actuation in augmented instruments, including actuator design and selection, interfacing with the instrument, and circuits for driving the actuators. The material in this paper forms the basis of the magnetic resonator piano, anelectromagnetically-augmented acoustic grand piano now inits second design iteration. In addition to discussing applications to the piano, this paper aims to provide a toolboxto accelerate the design of new hybrid acoustic-electronicinstruments.</abstract>
    <keywords>augmented instruments, electromagnetic actuation, circuit design, hardware </keywords>
  </document>
  <document>
    <name>nime2012_119.pdf</name>
    <abstract>This paper describes a recent addition to LOLC, a text-based environment for collaborative improvisation for laptop ensembles, incorporating a machine musician that plays along with human performers. The machine musician LOLbot analyses the patterns created by human performers and the composite music they create as they are layered in performance. Based on user specified settings, LOLbot chooses appropriate patterns to play with the ensemble, either to add contrast to the existing performance or to be coherent with the rhythmic structure of the performance. The paper describes the background and motivations of the project, outlines the design of the original LOLC environment and describes the architecture and implementation of LOLbot. </abstract>
    <keywords>Machine Musicianship, Live Coding, Laptop Orchestra  </keywords>
  </document>
  <document>
    <name>nime2012_120.pdf</name>
    <abstract>Corpus-based concatenative synthesis is a fairly recentsound synthesis method, based on descriptor analysis of anynumber of existing or live-recorded sounds, and synthesisby selection of sound segments from the database matchinggiven sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a newinterface for musical expression. The interesting outcomeof such an examination is that the actual instrument is thespace of sound characteristics, through which the performernavigates with gestures captured by various input devices.We will take a look at different types of interaction modesand controllers (positional, inertial, audio analysis) and thegestures they afford, and provide a critical assessment oftheir musical and expressive capabilities, based on severalyears of musical experience, performing with the CataRTsystem for real-time CBCS.</abstract>
  </document>
  <document>
    <name>nime2012_123.pdf</name>
    <abstract>This paper presents the results of user interaction with two explorative music environments (sound system A and B) that were inspired from the Banda Linda music tradition in two different ways. The sound systems adapted to how a team of two players improvised and made a melody together in an interleaved fashion: Systems A and B used a fuzzy logic algorithm and pattern recognition to respond with modifications of a background rhythms. In an experiment with a pen tablet interface as the music instrument, users aged 10-13 were to tap tones and continue each other's melody. The sound systems rewarded users sonically, if they managed to add tones to their mutual melody in a rapid turn taking manner with rhythmical patterns. Videos of experiment sessions show that user teams contributed to a melody in ways that resemble conversation. Interaction data show that each sound system made player teams play in different ways, but players in general had a hard time adjusting to a non-Western music tradition. The paper concludes with a comparison and evaluation of the two sound systems. Finally it proposes a new approach to the design of collaborative and shared music environments that is based on "listening applications". </abstract>
    <keywords>Music improvisation, novices, social learning, interaction studies, interaction design.  </keywords>
  </document>
  <document>
    <name>nime2012_125.pdf</name>
    <abstract>SoundStrand is a tangible music composition tool. It demonstrates a paradigm developed to enable music composition through the use of tangible interfaces. This paradigm attempts to overcome the contrast between the relatively small of amount degrees of freedom usually demonstrated by tangible interfaces and the vast number of possibilities that musical composition presents. SoundStrand is comprised of a set of physical objects called cells, each representing a musical phrase. Cells can be sequentially connected to each other to create a musical theme. Cells can also be physically manipulated to access a wide range of melodic, rhythmic and harmonic variations. The SoundStrand software assures that as the cells are manipulated, the melodic flow, harmonic transitions and rhythmic patterns of the theme remain musically plausible while preserving the user's intentions. </abstract>
    <keywords>Tangible, algorithmic, composition, computer assisted  </keywords>
  </document>
  <document>
    <name>nime2012_128.pdf</name>
    <abstract>massMobile is a client-server system for mass audience participation in live performances using smartphones. It was designed to flexibly adapt to a variety of participatory performance needs and to a variety of performance venues. It allows for real time bi-directional communication between performers and audiences utilizing existing wireless 3G, 4G, or WiFi networks. In this paper, we discuss the goals, design, and implementation of the framework, and we describe several projects realized with massMobile. </abstract>
    <keywords>audience participation, network music, smartphone, performance, mobile  </keywords>
  </document>
  <document>
    <name>nime2012_131.pdf</name>
    <abstract>This paper introduces the concept of Kugelschwung, a digital musical instrument centrally based around the use ofpendulums and lasers to create unique and highly interactive electronic ambient soundscapes. Here, we explorethe underlying design and physical construction of the instrument, as well as its implementation and feasibility asan instrument in the real world. To conclude, we outlinepotential expansions to the instrument, describing how itsrange of applications can be extended to accommodate avariety of musical styles.</abstract>
    <keywords>laser, pendulums, instrument design, electronic, sampler, soundscape, expressive performance </keywords>
  </document>
  <document>
    <name>nime2012_132.pdf</name>
  </document>
  <document>
    <name>nime2012_133.pdf</name>
    <abstract>Performing music with a computer and loudspeakers represents always a challenge. The lack of a traditional instrument requires the performer to study idiomatic strategiesby which musicianship becomes apparent. On the otherhand, the audience needs to decode those strategies, so toachieve an understanding and appreciation of the music being played. The issue is particularly relevant to the performance of music that results from the mediation betweenbiological signals of the human body and physical performance.The present article tackles this concern by demonstratinga new model of musical performance; what I define biophysical music. This is music generated and played in real timeby amplifying and processing the acoustic sound of a performer's muscle contractions. The model relies on an original and open source technology made of custom biosensorsand a related software framework. The succesfull application of these tools is discussed in the practical context of asolo piece for sensors, laptop and loudspeakers. Eventually,the compositional strategies that characterize the piece arediscussed along with a systematic description of the relevantmapping techniques and their sonic outcome.</abstract>
    <keywords>Muscle sounds, biophysical music, augmented body, real- time performance, human-computer interaction, embodi- ment. </keywords>
  </document>
  <document>
    <name>nime2012_136.pdf</name>
    <abstract> In this paper, we argue that the design of New Interfaces for Musical Expression has much to gain from the study of interaction in ensemble laptop performance contexts using ethnographic techniques. Inspired by recent third-stream research in the field of human computer interaction, we describe a recent ethnomethodologically-informed study of the Birmingham Laptop Ensemble (BiLE), and detail our approach to thick description of the group's working practices. Initial formal analysis of this material sheds light on the fluidity of composer, performer and designer roles within the ensemble and shows how confluences of these roles constitute member's differing viewpoints. We go on to draw out a number of strands of interaction that highlight the essentially complex, socially constructed and value driven nature of the group's practice and conclude by reviewing the implications of these factors on the design of software tools for laptop ensembles. </abstract>
    <keywords>Laptop Performance, Ethnography, Ethnomethodology, Human Computer Interaction.  </keywords>
  </document>
  <document>
    <name>nime2012_142.pdf</name>
    <keywords>Network music, mobile music, distributed music, interactivity, sound art installation, collaborative instrument, site-specific, electromagnetic signals, WiFi, trilateration, traceroute, echolocation, SuperCollider, Pure Data, RjDj, mapping  </keywords>
  </document>
  <document>
    <name>nime2012_150.pdf</name>
    <abstract>The configurability and networking abilities of digital musical instruments increases the possibilities for collaborationin musical performances. Computer music ensembles suchas laptop orchestras are becoming increasingly common andprovide laboratories for the exploration of these possibilities. However, much of the literature regarding the creation of DMIs has been focused on individual expressivity,and their potential for collaborative performance has beenunder-utilized. This paper makes the case for the benefitsof an approach to digital musical instrument design thatbegins with their collaborative potential, examines severalframeworks and sets of principles for the creation of digitalmusical instruments, and proposes a dimension space representation of collaborative approaches which can be used toevaluate and guide future DMI creation. Several examplesof DMIs and compositions are then evaluated and discussedin the context of this dimension space.</abstract>
    <keywords>dimension space, collaborative, digital musical instrument, dmi, digital music ensemble, dme </keywords>
  </document>
  <document>
    <name>nime2012_152.pdf</name>
    <abstract>Borderlands is a new interface for composing and performing with granular synthesis. The software enables flexible, realtime improvisation and is designed to allow users to engage with sonic material on a fundamental level, breaking free of traditional paradigms for interaction with this technique. The user is envisioned as an organizer of sound, simultaneously assuming the roles of curator, performer, and listener. This paper places the software within the context of painterly interfaces and describes the user interaction design and synthesis methodology. </abstract>
  </document>
  <document>
    <name>nime2012_153.pdf</name>
    <abstract>The Physical Computing Ensemble was created in order todetermine the viability of an approach to musical performance which focuses on the relationships and interactionsof the performers. Three performance systems utilizing gestural controllers were designed and implemented, each witha different strategy for performer interaction.These strategies took advantage of the opportunities forcollaborative performance inherent in digital musical instruments due to their networking abilities and reconfigurablesoftware. These characteristics allow for the easy implementation of varying approaches to collaborative performance.Ensembles who utilize digital musical instruments providea fertile environment for the design, testing, and utilizationof collaborative performance systems.The three strategies discussed in this paper are the parameterization of musical elements, turn-based collaborative control of sound, and the interaction of musical systems created by multiple performers. Design principles,implementation, and a performance using these strategiesare discussed, and the conclusion is drawn that performerinteraction and collaboration as a primary focus for systemdesign, composition, and performance is viable.</abstract>
    <keywords>Collaborative performance, interaction, digital musical in- struments, gestural controller, digital music ensemble, Wii </keywords>
  </document>
  <document>
    <name>nime2012_155.pdf</name>
    <abstract>This paper presents a comparison of three-dimensional (3D)position tracking systems in terms of some of their performance parameters such as static accuracy and precision,update rate, and shape of the space they sense. The underlying concepts and characteristics of position tracking technologies are reviewed, and four position tracking systems(Vicon, Polhemus, Kinect, and Gametrak), based on different technologies, are empirically compared according totheir performance parameters and technical specifications.Our results show that, overall, the Vicon was the positiontracker with the best performance.</abstract>
    <keywords>Position tracker, comparison, touch-less, gestural control </keywords>
  </document>
  <document>
    <name>nime2012_159.pdf</name>
    <abstract>A new method for interpolating between presets is described.The interpolation algorithm called Intersecting N-SpheresInterpolation is simple to compute and its generalization tohigher dimensions is straightforward. The current implementation in the SuperCollider environment is presentedas a tool that eases the design of many-to-many mappingsfor musical interfaces. Examples of its uses, including suchmappings in conjunction with a musical interface called thesponge, are given and discussed.</abstract>
    <keywords>Mapping, Preset, Interpolation, Sponge, SuperCollider </keywords>
  </document>
  <document>
    <name>nime2012_161.pdf</name>
    <keywords>music balls, instruments, controllers, inexpensive </keywords>
  </document>
  <document>
    <name>nime2012_162.pdf</name>
  </document>
  <document>
    <name>nime2012_164.pdf</name>
    <abstract>In this paper, we describe our pioneering work in developingspeech synthesis beyond the Text-To-Speech paradigm. Weintroduce tangible speech synthesis as an alternate way ofenvisioning how artificial speech content can be produced.Tangible speech synthesis refers to the ability, for a givensystem, to provide some physicality and interactivity to important speech production parameters. We present MAGE,our new software platform for high-quality reactive speechsynthesis, based on statistical parametric modeling and moreparticularly hidden Markov models. We also introduce anew HandSketch-based musical instrument. This instrument brings pen and posture based interaction on the topof MAGE, and demonstrates a first proof of concept.</abstract>
  </document>
  <document>
    <name>nime2012_167.pdf</name>
    <abstract>This paper describes the development of the Emotion Light, an interactive biofeedback artwork where the user listens to a piece of electronic music whilst holding a semi-transparent sculpture that tracks his/her bodily responses and translates these into changing light patterns that emerge from the sculpture. The context of this work is briefly described and the questions it poses are derived from interviews held with audience members. </abstract>
    <keywords>Interactive biofeedback artwork, music and emotion, novel interfaces, practice based research, bodily response, heart rate,  biosignals, affective computing, aesthetic interaction, mediating  body, biology inspired system    </keywords>
  </document>
  <document>
    <name>nime2012_168.pdf</name>
    <abstract>As a part of the research project Voice Meetings, a solo liveelectronic vocal performance was presented for 63 students. Through a mixed method approach applying both written and oral response, feedback from one blindfolded and one seeing audience group was collected and analyzed. There were marked differences between the groups regarding focus, in that the participants in blindfolded group tended to focus on fewer aspects, have a heightened focus and be less distracted than the seeing group. The seeing group, on its part, focused more on the technological instruments applied in the performance, the performer herself and her actions. This study also shows that there were only minor differences between the groups regarding the experience of skill and control, and argues that this observation can be explained by earlier research on skill in NIMEs. </abstract>
    <keywords>Performance, audience reception, acousmatic listening, live- electronics, voice, qualitative research  </keywords>
  </document>
  <document>
    <name>nime2012_169.pdf</name>
    <abstract>Through a series of collaborative research projects usingOrient, a wireless, inertial sensor-based motion capture system,I have studied the requirements of musicians, dancers,performers and choreographers and identified various designstrategies for the realization of Whole Body Interactive (WBI)performance systems. The acquired experience and knowledgeled to the design and development of EnActor, prototypeWhole Body Interaction Design software. The software hasbeen realized as a collection of modules that were provedvaluable for the design of interactive performance systems thatare directly controlled by the body.This paper presents EnActor's layout as a blueprint for thedesign and development of more sophisticated descendants.Complete video archive of my research projects in WBIperformance systems at: http://www.inter-axions.com</abstract>
    <keywords>Whole Body Interaction, Motion Capture, Interactive Performance Systems, Interaction Design, Software Prototype, </keywords>
  </document>
  <document>
    <name>nime2012_170.pdf</name>
    <abstract>In this paper we introduce an interactive mobile music performance system using the digital compass of mobile phones. Compass-based interface can detect the aiming orientation of performers on stage, allowing us to obtain information on interactions between performers and use it for both musical mappings and visualizations on screen for the audience. We document and discuss the result of a compassbased mobile music performance, Where Are You Standing, and present an algorithm for a new app to track down the performers' positions in real-time. </abstract>
  </document>
  <document>
    <name>nime2012_171.pdf</name>
    <keywords>Many person musical instruments, cooperative music, asym- metric interfaces, transmodal feedback </keywords>
  </document>
  <document>
    <name>nime2012_174.pdf</name>
    <keywords>Empirical methods, quantitative, usability testing and evaluation,  digital musical instruments, evaluation methodology, Illusio  </keywords>
  </document>
  <document>
    <name>nime2012_175.pdf</name>
  </document>
  <document>
    <name>nime2012_177.pdf</name>
    <keywords>Skin-based instruments, skin conductivity, collaborative interfaces, embodiment, intimacy, multi-player performance  </keywords>
  </document>
  <document>
    <name>nime2012_179.pdf</name>
    <abstract>Empatheater is a video playing system that is controlled by multimodal interaction. As the video is played, the user must interact and emulate predefined "events" for the video to continue on. The user is given the illusion of playing an active role in the unraveling video content and can empathize with the performer. In this paper, we report about user experiences with Empatheater when applied to musical video contents. </abstract>
    <keywords>Music video, Empathy, Interactive video, Musical event,  Multimodal interaction.  </keywords>
  </document>
  <document>
    <name>nime2012_180.pdf</name>
    <abstract>The augmented ballet project aims at gathering research from several fields and directing them towards a same application case: adding virtual elements (visual and acoustic) to a dance live performance, and allowing the dancer to interact with them. In this paper, we describe a novel interaction that we used in the frame of this project: using the dancer's movements to recognize the emotions he expresses, and use these emotions to generate musical audio flows evolving in real-time. The originality of this interaction is threefold. First, it covers the whole interaction cycle from the input (the dancer's movements) to the output (the generated music). Second, this interaction isn't direct but goes through a high level of abstraction: dancer's emotional expression is recognized and is the source of music generation. Third, this interaction has been designed and validated through constant collaboration with a choreographer, culminating in an augmented ballet performance in front of a live audience. </abstract>
    <keywords>Interactive sonification, motion, gesture and music, interaction, live performance, musical human-computer interaction  </keywords>
  </document>
  <document>
    <name>nime2012_181.pdf</name>
    <abstract>In this paper we present our project to make sound synthesisand music controller construction accessible to children ina technology design workshop. We present the work wehave carried out to develop a graphical user interface, andgive account of the workshop we conducted in collaborationwith a local primary school. Our results indicate that theproduction of audio events by means of digital synthesisand algorithmic composition provides a rich and interestingfield to be discovered for pedagogical workshops taking aConstructionist approach.</abstract>
    <keywords>Child Computer Interaction, Constructionism, Sound and Music Computing, Human-Computer Interface Design, Mu- sic Composition and Generation, Interactive Audio Sys- tems, Technology Design Activities. </keywords>
  </document>
  <document>
    <name>nime2012_185.pdf</name>
    <abstract>Meaning crossword of sound, Crossole is a musical metainstrument where the music is visualized as a set of virtualblocks that resemble a crossword puzzle. In Crossole, thechord progressions are visually presented as a set of virtualblocks. With the aid of the Kinect sensing technology, a performer controls music by manipulating the crossword blocksusing hand movements. The performer can build chords inthe high level, traverse over the blocks, step into the lowlevel to control the chord arpeggiations note by note, loopa chord progression or map gestures to various processingalgorithms to enhance the timbral scenery.</abstract>
    <keywords>Kinect, meta-instrument, chord progression, body gesture </keywords>
  </document>
  <document>
    <name>nime2012_187.pdf</name>
    <abstract>This paper presents the JD-1, a digital controller for analog modular synthesizers. The JD-1 features a capacitive touchsensing keyboard that responds to continuous variations in finger contact, high-accuracy polyphonic control-voltage outputs, a built-in sequencer, and digital interfaces for connection to MIDI and OSC devices. Design goals include interoperability with a wide range of synthesizers, very highresolution pitch control, and intuitive control of the sequencer from the keyboard. </abstract>
  </document>
  <document>
    <name>nime2012_189.pdf</name>
    <abstract>Development of new musical interfaces often requires experimentation with the mapping of available controller inputs to output parameters. Useful mappings for a particular application may be complex in nature, with one or more inputs being linked to one or more outputs. Existing development environments are commonly used to program such mappings, while code libraries provide powerful data-stream manipulation. However, room exists for a standalone application with a simpler graphical user interface for dynamically patching between inputs and outputs. This paper presents an early prototype version of a software tool that allows the user to route control signals in real time, using various messaging formats. It is cross-platform and runs as a standalone application in desktop and Android OS versions. The latter allows the users of mobile devices to experiment with mapping signals to and from physical computing components using the inbuilt multi-touch screen. Potential uses therefore include real-time mapping during performance in a more expressive manner than facilitated by existing tools. </abstract>
    <keywords>Mapping, Software Tools, Android.  </keywords>
  </document>
  <document>
    <name>nime2012_193.pdf</name>
    <abstract>An augmented bass clarinet is developed in order to extendthe performance and composition potential of the instrument. Four groups of sensors are added: key positions, inertial movement, mouth pressure and trigger switches. Theinstrument communicates wirelessly with a receiver setupwhich produces an OSC data stream, usable by any application on a host computer.The SABRe projects intention is to be neither tied to itsinventors nor to one single player but to offer a referencedesign for a larger community of bass clarinet players andcomposers. For this purpose, several instruments are madeavailable and a number of composer residencies, workshops,presentations and concerts are organized. These serve forevaluation and improvement purposes in order to build arobust and user friendly extended musical instrument, thatopens new playing modalities.</abstract>
    <keywords>augmented instrument, bass clarinet, sensors, air pressure, gesture, OSC </keywords>
  </document>
  <document>
    <name>nime2012_194.pdf</name>
    <keywords>Musical Interaction Design, NIME education, Microcontroller, Arduino language, StickOS BASIC, Open Sound Control, Microchip PIC32, Wireless, Zigflea, Wifi, 802.11g, Bluetooth, CUI32, CUI32Stem   </keywords>
  </document>
  <document>
    <name>nime2012_195.pdf</name>
    <abstract>Capacitive touch sensing is increasingly used in musical controllers, particularly those based on multi-touch screen interfaces. However, in contrast to the venerable piano-stylekeyboard, touch screen controllers lack the tactile feedbackmany performers find crucial. This paper presents an augmentation system for acoustic and electronic keyboards inwhich multi-touch capacitive sensors are added to the surface of each key. Each key records the position of fingerson the surface, and by combining this data with MIDI noteonsets and aftertouch from the host keyboard, the systemfunctions as a multidimensional polyphonic controller for awide variety of synthesis software. The paper will discussgeneral capacitive touch sensor design, keyboard-specificimplementation strategies, and the development of a flexiblemapping engine using OSC and MIDI.</abstract>
    <keywords>augmented instruments, keyboard, capacitive sensing, multi- touch </keywords>
  </document>
  <document>
    <name>nime2012_197.pdf</name>
    <abstract>This paper addresses the issue of engaging the audience with new musical instruments in live performance context. We introduce design concerns that we consider influential to enhance the communication flow between the audience and the performer. We also propose and put in practice a design approach that considers the use of performance space as a way to engage with the audience. A collaborative project, Sound Gloves, presented here exemplifies such a concept by dissolving the space between performers and audience. Our approach resulted in a continuous interaction between audience and performers, in which the social dynamics was changed in a positive way in a live performance context of NIMEs. Such an approach, we argue, may be considered as one way to further engage and interact with the audience. </abstract>
    <keywords>NIME, wearable electronics, performance, design approach  </keywords>
  </document>
  <document>
    <name>nime2012_199.pdf</name>
    <abstract>This paper describes an interactive gestural microphone forvocal performance named Voicon. Voicon is a non-invasiveand gesture-sensitive microphone which allows vocal performers to use natural gestures to create vocal augmentations and modifications by using embedded sensors in a microphone. Through vocal augmentation and modulation,the performers can easily generate desired amount of thevibrato and achieve wider vocal range. These vocal enhancements will deliberately enrich the vocal performanceboth in its expressiveness and the dynamics. Using Voicon,singers can generate additional vibrato, control the pitchand activate customizable vocal effect by simple and intuitive gestures in live and recording context.</abstract>
    <keywords>Gesture, Microphone, Vocal Performance, Performance In- terface </keywords>
  </document>
  <document>
    <name>nime2012_200.pdf</name>
  </document>
  <document>
    <name>nime2012_201.pdf</name>
    <abstract>This paper describes a novel music control sensate surface, which enables integration between any musical instruments with a v ersatile, customizable, and essentially cost-effective user interface. This sensate surface is based on c onductive inkjet printing technology which allows capacitive sensor electrodes and connections between electronics components to be printed onto a large roll of flexible substrate that is unrestricted in length. The high dynamic range capacitive sensing electrodes can not only infer touch, but near-range, non-contact gestural nuance in a music performance. With this sensate surface, users can "cut" out their desired shapes, "paste" the number of inputs, and customize their controller interface, which can then send signals wirelessly to effects or software synthesizers. We seek to find a solution for integrating the form factor of traditional music controllers seamlessly on top of one's music instrument and meanwhile adding expressiveness to the music performance by sensing and incorporating movements and gestures to manipulate the musical output. We present an example of implementation on an electric ukulele and provide several design examples to demonstrate the versatile capabilities of this system.  </abstract>
    <keywords>Sensate surface, music controller skin, customizable controller surface, flexible electronics  </keywords>
  </document>
  <document>
    <name>nime2012_202.pdf</name>
    <abstract>We have developed a prototype wireless microphone thatprovides vocalists with control over their vocal effects directly from the body of the microphone. A wireless microphone has been augmented with six momentary switches,one fader, and three axes of motion and position sensors,all of which provide MIDI output from the wireless receiver.The MIDI data is used to control external vocal effects unitssuch as live loopers, reverbs, distortion pedals, etc. The goalwas to to provide dramatically increased expressive controlto vocal performances, and address some of the shortcomings of pedal-controlled effects. The addition of gesturalcontrols from the motion sensors opens up new performancepossibilities such as panning the voice simply by pointingthe microphone in one direction or another. The result is ahybrid microphone-musical instrument which has recievedextremely positive results from vocalists in numerous informal workshops.</abstract>
    <keywords>NIME, Sennheiser, Concept Tahoe, MIDI, control, micro- phone </keywords>
  </document>
  <document>
    <name>nime2012_203.pdf</name>
    <abstract>We augment the piano keyboard with a 3D gesture spaceusing Microsoft Kinect for sensing and top-down projection for visual feedback. This interface provides multi-axialgesture controls to enable continuous adjustments to multiple acoustic parameters such as those on the typical digitalsynthesizers. We believe that using gesture control is morevisceral and aesthetically pleasing, especially during concertperformance where the visibility of the performer's action isimportant. Our system can also be used for other types ofgesture interaction as well as for pedagogical applications.</abstract>
    <keywords>NIME, piano, depth camera, musical instrument, gesture, tabletop projection </keywords>
  </document>
  <document>
    <name>nime2012_205.pdf</name>
    <abstract>We present a new wireless transceiver board for the CUI32sensor interface, aimed at creating a solution that is flexible,reliable, and with little power consumption. Communication with the board is based on the ZigFlea protocol andit has been evaluated on a CUI32 using the StickOS operating system. Experiments show that the total sensor datacollection time is linearly increasing with the number of sensor samples used. A data rate of 0.8 kbit/s is achieved forwirelessly transmitting three axes of a 3D accelerometer.Although this data rate is low compared to other systems,our solution benefits from ease-of-use and stability, and isuseful for applications that are not time-critical.</abstract>
    <keywords>wireless sensing, CUI32, StickOS, ZigBee, ZigFlea </keywords>
  </document>
  <document>
    <name>nime2012_208.pdf</name>
    <abstract>"Perfect Take" is a public installation out of networked acoustic instruments that let composers from all over the world exhibit their MIDI-works by means of the Internet. The primary aim of this system is to offer composers a way to have works exhibited and recorded in venues and with technologies not accessible to him/her under normal circumstances. The Secondary aim of this research is to highlight experience design as a complement to interaction design, and a shift of focus from functionality of a specific gestural controller, towards the environments, events and processes that they are part of. </abstract>
    <keywords>NIME, Networked Music, MIDI, Disklavier, music collaboration, creativity  </keywords>
  </document>
  <document>
    <name>nime2012_209.pdf</name>
    <abstract> FutureGrab is a new wearable musical instrument for live performance that is highly intuitive while still generating an interesting sound by subtractive synthesis. Its sound effects resemble the human vowel pronunciation, which were mapped to hand gestures that are similar to the mouth shape of human to pronounce corresponding vowel. FutureGrab also provides all necessary features for a lead musical instrument such as pitch control, trigger, glissando and key adjustment. In addition, pitch indicator was added to give visual feedback to the performer, which can reduce the mistakes during live performances. This paper describes the motivation, system design, mapping strategy and implementation of FutureGrab, and evaluates the overall experience. </abstract>
    <keywords>Wearable musical instrument, Pure Data, gestural synthesis, formant synthesis, data-glove, visual feedback, subtractive synthesis   </keywords>
  </document>
  <document>
    <name>nime2012_211.pdf</name>
    <abstract>In an attempt to utilize the expert pianist's technique and spare bandwidth, a new keyboard-based instrument augmented by sensors suggested by the examination of existing acoustic instruments is introduced. The complete instrument includes a keyboard, various pedals and knee levers, several bowing controllers, and breath and embouchure sensors connected to an Arduino microcontroller that sends sensor data to a laptop running Max/MSP, where custom software maps the data to synthesis algorithms. The audio is output to a digital amplifier powering a transducer mounted on a resonator box to which several of the sensors are attached. Careful sensor selection and mapping help to facilitate performance mode. </abstract>
    <keywords>Gesture, controllers, Digital Musical Instrument, keyboard </keywords>
  </document>
  <document>
    <name>nime2012_212.pdf</name>
    <abstract>Dirty Tangible Interfaces (DIRTI) are a new concept in interface design that forgoes the dogma of repeatability in favor of a richer and more complex experience, constantly evolving, never reversible, and infinitely modifiable. We built a prototype based on granular or liquid interaction material placed in a glass dish, that is analyzed by video tracking for its 3D relief. This relief, and the dynamic changes applied to it by the user, are interpreted as activation profiles to drive corpus-based concatenative sound synthesis, allowing one or more players to mold sonic landscapes and to plow through them in an inherently collaborative, expressive, and dynamic experience.  </abstract>
    <keywords>Tangible interface, Corpus-based concatenative synthesis, Non- standard interaction  </keywords>
  </document>
  <document>
    <name>nime2012_213.pdf</name>
    <abstract>In this paper we present a series of algorithms developedto detect the following guitar playing techniques : bend,hammer-on, pull-off, slide, palm muting and harmonic. Detection of playing techniques can be used to control external content (i.e audio loops and effects, videos, light events,etc.), as well as to write real-time score or to assist guitar novices in their learning process. The guitar used is aGodin Multiac with an under-saddle RMC hexaphonic piezopickup (one pickup per string, i.e six mono signals).</abstract>
    <keywords>Guitar audio analysis, playing techniques, hexaphonic pickup, controller, augmented guitar </keywords>
  </document>
  <document>
    <name>nime2012_214.pdf</name>
    <abstract>The Deckle Group1 is an ensemble that designs, builds andperforms on electroacoustic drawing boards. These drawing surfaces are augmented with Satellite CCRMA BeagleBoards and Arduinos2.[1] Piezo microphones are used inconjunction with other sensors to produce sounds that arecoupled tightly to mark-making gestures. Position trackingis achieved with infra-red object tracking, conductive fabricand a magnetometer.</abstract>
    <keywords>Deckle, BeagleBoard, Drawing, Sonification, Performance, Audiovisual, Gestural Interface </keywords>
  </document>
  <document>
    <name>nime2012_215.pdf</name>
    <abstract>In this paper we describe the EyeHarp, a new gaze-controlledmusical instrument, and the new features we recently addedto its design. In particular, we report on the EyeHarp newcontrols, the arpeggiator, the new remote eye-tracking device, and the EyeHarp capacity to act as a MIDI controllerfor any VST plugin virtual instrument. We conducted anevaluation of the EyeHarp Temporal accuracy by monitoring 10 users while performing a melody task, and comparingtheir gaze control accuracy with their accuracy using a computer keyboard. We report on the results of the evaluation.</abstract>
    <keywords>Eye-tracking systems, music interfaces, gaze interaction </keywords>
  </document>
  <document>
    <name>nime2012_216.pdf</name>
    <abstract>Virtual Pottery is an interactive audiovisual piece that uses hand gesture to create 3D pottery objects and sound shape. Using the OptiTrack motion capture (Rigid Body) system at TransLab in UCSB, performers can take a glove with attached trackers, move the hand in x, y, and z axis and create their own sound pieces. Performers can also manipulate their pottery pieces in real time and change arrangement on the musical score interface in order to create a continuous musical composition. In this paper we address the relationship between body, sound and 3D shapes. We also describe the origin of Virtual Pottery, its design process, discuss its aesthetic value and musical sound synthesis system, and evaluate the overall experience. </abstract>
    <keywords>Virtual Pottery, virtual musical instrument, sound synthesis, motion and gesture, pottery, motion perception, interactive sound installation.  </keywords>
  </document>
  <document>
    <name>nime2012_217.pdf</name>
    <abstract>This paper presents concepts, models, and empirical findings relating to liveness and flow in the user experience of systems mediated by notation. Results from an extensive two-year field study of over 1,000 sequencer and tracker users, combining interaction logging, user surveys, and a video study, are used to illustrate the properties of notations and interfaces that facilitate greater immersion in musical activities and domains, borrowing concepts from programming to illustrate the role of visual and musical feedback, from the notation and domain respectively. The Cognitive Dimensions of Notations framework and Csikszentmihalyi's flow theory are combined to demonstrate how non-realtime, notation-mediated interaction can support focused, immersive, energetic, and intrinsicallyrewarding musical experiences, and to what extent they are supported in the interfaces of music production software. Users are shown to maintain liveness through a rapid, iterative editaudition cycle that integrates audio and visual feedback. </abstract>
    <keywords>notation, composition, liveness, flow, feedback, sequencers,  DAWs, soundtracking, performance, user studies, programming   </keywords>
  </document>
  <document>
    <name>nime2012_222.pdf</name>
    <abstract>The Gyil is a pentatonic African wooden xylophone with14-15 keys. The work described in this paper has beenmotivated by three applications: computer analysis of Gyilperformance, live improvised electro-acoustic music incorporating the Gyil, and hybrid sampling and physical modeling. In all three of these cases, detailed information aboutwhat is played on the Gyil needs to be digitally capturedin real-time. We describe a direct sensing apparatus thatcan be used to achieve this. It is based on contact microphones and is informed by the specific characteristics of theGyil. An alternative approach based on indirect acquisitionis to apply polyphonic transcription on the signal acquiredby a microphone without requiring the instrument to bemodified. The direct sensing apparatus we have developedcan be used to acquire ground truth for evaluating differentapproaches to polyphonic transcription and help create a"surrogate" sensor. Some initial results comparing differentstrategies to polyphonic transcription are presented.</abstract>
    <keywords>hyperinstruments, indirect acquisition, surrogate sensors, computational ethnomusicology, physical modeling, perfor- mance analysis </keywords>
  </document>
  <document>
    <name>nime2012_223.pdf</name>
    <abstract>The Instant Instrument Anywhere (IIA) is a small devicewhich can be attached to any metal object to create anelectronic instrument. The device uses capacitive sensingto detect proximity of the player's body to the metal object, and sound is generated through a surface transducerwhich can be attached to any flat surface. Because the capacitive sensor can be any shape or size, absolute capacitivethresholding is not possible since the baseline capacitancewill change. Instead, we use a differential-based movingsum threshold which can rapidly adjust to changes in theenvironment or be re-calibrated to a new metal object. Weshow that this dynamic threshold is effective in rejectingenvironmental noise and rapidly adapting to new objects.We also present details for constructing Instant InstrumentsAnywhere, including using smartphone as the synthesis engine and power supply.</abstract>
    <keywords>Capacitive Sensing, Arduino </keywords>
  </document>
  <document>
    <name>nime2012_226.pdf</name>
  </document>
  <document>
    <name>nime2012_228.pdf</name>
    <abstract>A modular and reconfigurable hardware platform for analogoptoelectronic signal acquisition is presented. Its intendedapplication is for fiber optic sensing in electronic musicalinterfaces, however the flexible design enables its use witha wide range of analog and digital sensors. Multiple gainand multiplexing stages as well as programmable analog anddigital hardware blocks allow for the acquisition, processing,and communication of single-ended and differential signals.Along with a hub board, multiple acquisition boards canbe connected to modularly extend the system's capabilitiesto suit the needs of the application. Fiber optic sensorsand their application in DMIs are briefly discussed, as wellas the use of the hardware platform with specific musicalinterfaces.</abstract>
  </document>
  <document>
    <name>nime2012_229.pdf</name>
    <abstract>Music for Sleeping &amp; Waking Minds (2011-2012) is a new, overnight work in which four performers fall asleep while wearing custom designed EEG sensors which monitor their brainwave activity. The data gathered from the EEG sensors is applied in real time to different audio and image signal processing functions, resulting in continuously evolving multichannel sound environment and visual projection. This material serves as an audiovisual description of the individual and collective neurophysiological state of the ensemble. Audiences are invited to experience the work in different states of attention: while alert and asleep, resting and awakening. </abstract>
    <keywords>EEG, sleep, dream, biosignals, bio art, consciousness, BCI  </keywords>
  </document>
  <document>
    <name>nime2012_230.pdf</name>
    <abstract>This paper describes the design and realization of TC-11,a software instrument based on programmable multi-pointcontrollers. TC-11 is a modular synthesizer for the iPadthat uses multi-touch and device motion sensors for control.It has a robust patch programming interface that centersaround multi-point controllers, providing powerful flexibility. This paper details the origin, design principles, programming implementation, and performance result of TC11.</abstract>
    <keywords>TC-11, iPad, multi-touch, multi-point, controller mapping, synthesis programming </keywords>
  </document>
  <document>
    <name>nime2012_232.pdf</name>
    <abstract>We developed original solenoid actuator units with severalbuilt-in sensors, and produced a box-shaped musical interface "PocoPoco" using 16 units of them as a universal input/output device. We applied up-and-down movement ofthe solenoid-units and user's intuitive input to musical interface. Using transformation of the physical interface, wecan apply movement of the units to new interaction design.At the same time we intend to suggest a new interface whosemovement itself can attract the user.</abstract>
    <keywords>musical interface, interaction design, tactile, moving, kinetic </keywords>
  </document>
  <document>
    <name>nime2012_235.pdf</name>
    <abstract>In this paper we discuss aspects of our work in developing performance systems that are geared towards humanmachine co-performance with a particular emphasis on improvisation. We present one particular system, FILTER,which was created in the context of a larger project related to artificial intelligence and performance, and has beentested in the context of our electro-acoustic performancetrio. We discuss how this timbrally rich and highly nonidiomatic musical context has challenged the design of thesystem, with particular emphasis on the mapping of machine listening parameters to higher-level behaviors of thesystem in such a way that spontaneity and creativity areencouraged while maintaining a sense of novel dialogue.</abstract>
    <keywords>Electroacoustic Improvisation, Machine Learning, Mapping, Sonic Gestures, Spatialization </keywords>
  </document>
  <document>
    <name>nime2012_237.pdf</name>
    <abstract>The purpose of the Musician Assistance and Score Distribution (MASD) system is to assist novice musicians withplaying in an orchestra, concert band, choir or other musicalensemble. MASD helps novice musicians in three ways. Itremoves the confusion that results from page turns, aides amusician's return to the proper location in the music scoreafter the looking at the conductor and notifies musiciansof conductor instructions. MASD is currently verified byevaluating the time between sending beats or conductor information and this information being rendered for the musician. Future work includes user testing of this system.There are three major components to the MASD system.These components are Score Distribution, Score Rendering and Information Distribution. Score Distribution passesscore information to clients and is facilitated by the Internet Communication Engine (ICE). Score Rendering uses theGUIDO Library to display the musical score. InformationDistribution uses ICE and the IceStorm service to pass beatand instruction information to musicians.</abstract>
    <keywords>score distribution, score-following, score rendering, musi- cian assistance </keywords>
  </document>
  <document>
    <name>nime2012_240.pdf</name>
    <abstract>Mobile devices represent a growing research field within NIME, and a growing area for commercial music software. They present unique design challenges and opportunities, which are yet to be fully explored and exploited. In this paper, we propose using a survey method combined with qualitative analysis to investigate the way in which people use mobiles musically. We subsequently present as an area of future research our own PDplayer, which provides a completely self contained end application in the mobile device, potentially making the mobile a more viable and expressive tool for musicians. </abstract>
    <keywords>NIME, Mobile Music, Pure Data  </keywords>
  </document>
  <document>
    <name>nime2012_241.pdf</name>
    <abstract>This paper presents a system for mobile percussive collaboration. We show that reinforcement learning can incrementally learn percussive beat patterns played by humans andsupports real-time collaborative performance in the absenceof one or more performers. This work leverages an existingintegration between urMus and Soar and addresses multiplechallenges involved in the deployment of machine-learningalgorithms for mobile music expression, including tradeoffsbetween learning speed &amp; quality; interface design for human collaborators; and real-time performance and improvisation.</abstract>
  </document>
  <document>
    <name>nime2012_243.pdf</name>
    <abstract>Force-feedback and physical modeling technologies now allow to achieve the same kind of relation with virtual instruments as with acoustic instruments, but the design of such elaborate models needs guidelines based on the study of the human sensory-motor system and behaviour. This article presents a qualitative study of a simulated instrumental interaction in the case of the virtual bowed string, using both waveguide and mass-interaction models. Subjects were invited to explore the possibilities of the simulations and to express themselves verbally at the same time, allowing us to identify key qualities of the proposed systems that determine the construction of an intimate and rich relationship with the users. </abstract>
    <keywords>Instrumental interaction, presence, force-feedback, physical modeling, simulation, haptics, bowed string.  </keywords>
  </document>
  <document>
    <name>nime2012_247.pdf</name>
  </document>
  <document>
    <name>nime2012_248.pdf</name>
    <abstract>This paper presents Digito, a gesturally controlled virtualmusical instrument. Digito is controlled through a number of intricate hand gestures, providing both discrete andcontinuous control of Digito's sound engine; with the finegrain hand gestures captured by a 3D depth sensor andrecognized using computer vision and machine learning algorithms. We describe the design and initial iterative development of Digito, the hand and finger tracking algorithmsand gesture recognition algorithms that drive the system,and report the insights gained during the initial development cycles and user testing of this gesturally controlledvirtual musical instrument.</abstract>
    <keywords>Gesture Recognition, Virtual Musical Instrument </keywords>
  </document>
  <document>
    <name>nime2012_253.pdf</name>
    <abstract>A study is presented examining the participatory designof digital musical interactions. The study takes into consideration the entire ecology of digital musical interactionsincluding the designer, performer and spectator. A newinstrument is developed through iterative participatory design involving a group of performers. Across the study theevolution of creative practice and skill development in anemerging community of practice is examined and a spectator study addresses the cognition of performance and theperception of skill with the instrument. Observations arepresented regarding the cognition of a novel interaction andevolving notions of skill. The design process of digital musical interactions is reflected on focusing on involvement ofthe spectator in design contexts.</abstract>
    <keywords>participatory design, DMIs, skill, cognition, spectator </keywords>
  </document>
  <document>
    <name>nime2012_254.pdf</name>
    <abstract>In order to further understand our emotional reaction to music, a museum-based installation was designed to collect physiological and self-report data from people listening to music. This demo will describe the technical implementation of this installation as a tool for collecting large samples of data in public spaces. The Emotion in Motion terminal is built upon a standard desktop computer running Max/MSP and using sensors that measure physiological indicators of emotion that are connected to an Arduino. The terminal has been installed in museums and galleries in Europe and the USA, helping create the largest database of physiology and self-report data while listening to music. </abstract>
    <keywords>Biosignals, EDA, SC, GSR, HR, POX, Self-Report, Database,  Physiological Signals, Max/MSP, FTM, SAM, GEMS  </keywords>
  </document>
  <document>
    <name>nime2012_256.pdf</name>
    <abstract>From a technical point of view, instrumental music making involves audible, visible and hidden playing parameters.Hidden parameters like force, pressure and fast movements,happening within milliseconds are particularly difficult tocapture. Here, we present data focusing on movement coordination parameters of the left hand fingers with the bowhand in violinists and between two violinists in group playing. Data was recorded with different position sensors, amicro camcorder fixed on a violin and an acceleration sensor placed on the bow. Sensor measurements were obtainedat a high sampling rate, gathering the data with a small microcontroller unit, connected with a laptop computer. Tocapture bow's position, rotation and angle directly on thebow to string contact point, the micro camcorder was fixednear the bridge. Main focuses of interest were the changesof the left hand finger, the temporal synchronization between left hand fingers with the right hand, the close upview to the bow to string contact point and the contact ofthe left hand finger and/or string to the fingerboard. Sevenviolinists, from beginners to master class students playedscales in different rhythms, speeds and bowings and musicexcerpts of free choice while being recorded. One measurement with 2 violinists was made to see the time differencesbetween two musicians while playing together. For simpleintegration of a conventional violin into electronic music environments, left hand sensor data were exemplary convertedto MIDI and OSC.</abstract>
    <keywords>Strings, violin, coordination, left, finger, right, hand </keywords>
  </document>
  <document>
    <name>nime2012_257.pdf</name>
    <abstract>Tangible tabletop musical interfaces allowing for a collaborative real-time interaction in live music performances areone of the promising fields in NIMEs. At present, this kindof interfaces present at least some of the following characteristics that limit their musical use: latency in the interaction, and partial or complete lack of responsiveness togestures such as tapping, scrubbing or pressing force. Ourcurrent research is exploring ways of improving the qualityof interaction with this kind of interfaces, and in particularwith the tangible tabletop instrument Reactable . In thispaper we present a system based on a circular array of mechanically intercoupled force sensing resistors used to obtaina low-latency, affordable, and easily embeddable hardwaresystem able to detect surface impacts and pressures on thetabletop perimeter. We also consider the option of completing this detected gestural information with the soundinformation coming from a contact microphone attached tothe mechanical coupling layer, to control physical modellingsynthesis of percussion instruments.</abstract>
    <keywords>tangible tabletop interfaces, force sensing resistor, mechan- ical coupling, fast low-noise analog to digital conversion, low-latency sensing, micro controller, multimodal systems, complementary sensing. </keywords>
  </document>
  <document>
    <name>nime2012_258.pdf</name>
    <abstract>This paper introduces Simpletones, an interactive sound system that enables a sense of musical collaboration for non-musicians. Participants can easily create simple sound compositions in real time by collaboratively operating physical artifacts as sound controllers. The physical configuration of the artifacts requires coordinated actions between participants to control sound (thus requiring, and emphasizing collaboration).Simpletones encourages playful human-to-human interaction by introducing a simple interface and a set of basic rules [1]. This enables novices to focus on the collaborative aspects of making music as a group (such as synchronization and taking collective decisions through non-verbal communication) to ultimately engage a state of group flow[2]. This project is relevant to a contemporary discourse on musical expression because it allows novices to experience the social aspects of group music making, something that is usually reserved only for trained performers [3].</abstract>
    <keywords>Collaboration, Artifacts, Computer Vision, Color Tracking, State of Flow. </keywords>
  </document>
  <document>
    <name>nime2012_259.pdf</name>
    <abstract>Composing music for ensembles of computer-based instruments, such as laptop orchestra or mobile phone orchestra,is a multi-faceted and challenging endeavor whose parameters and criteria for success are ill-defined. In the designcommunity, tasks with these qualities are known as wickedproblems. This paper frames composing for computer-basedensemble as a design task, shows how Buchanan's four domains of design are present in the task, and discusses itswicked properties. The themes of visibility, risk, and embodiment, as formulated by Klemmer, are shown to be implicitly present in this design task. Composers are encouraged to address them explicitly and to take advantage ofthe practices of prototyping and iteration.</abstract>
    <keywords>Design, laptop orchestra, mobile phone orchestra, instru- ment design, interaction design, composition </keywords>
  </document>
  <document>
    <name>nime2012_260.pdf</name>
    <abstract>This paper presents the LoopJam installation which allowsparticipants to interact with a sound map using a 3D computer vision tracking system. The sound map results fromsimilarity-based clustering of sounds. The playback of thesesounds is controlled by the positions or gestures of participants tracked with a Kinect depth-sensing camera. Thebeat-inclined bodily movements of participants in the installation are mapped to the tempo of played sounds, whilethe playback speed is synchronized by default among allsounds. We presented and tested an early version of the installation to three exhibitions in Belgium, Italy and France.The reactions among participants ranged between curiosityand amusement.</abstract>
    <keywords>Interactive music systems and retrieval, user interaction and interfaces, audio similarity, depth sensors </keywords>
  </document>
  <document>
    <name>nime2012_262.pdf</name>
    <abstract>This paper describes the conceptualization and development of an open source tool for controlling the sound of a saxophone via the gestures of its performer. The motivation behind this work is the need for easy access tools to explore, compose and perform electroacoustic music in Colombian music schools and conservatories. This work led to the adaptation of common hardware to be used as a sensor attached to an acoustic instrument and the development of software applications to record, visualize and map performers gesture data into signal processing parameters. The scope of this work suggested that focus was to be made on a specific instrument so the saxophone was chosen. Gestures were selected in an iterative process with the performer, although a more ambitious strategy to figure out main gestures of an instruments performance was first defined. Detailed gesture-to-sound processing mappings are exposed in the text. An electroacoustic musical piece was successfully rehearsed and recorded using the Gest-O system. </abstract>
  </document>
  <document>
    <name>nime2012_264.pdf</name>
    <abstract>The Fingerphone, a reworking of the Stylophone in conductive paper, is presented as an example of new design approaches for sustainability and playability of electronic musical instruments. </abstract>
  </document>
  <document>
    <name>nime2012_271.pdf</name>
    <abstract>This short paper follows an earlier NIME paper [1] describing the invention and construction of the Electrumpet. Revisions and playing experience are both part of the current paper. The Electrumpet can be heard in the performance given by Hans Leeuw and Diemo Schwarz at this NIME conference. </abstract>
    <keywords>NIME, Electrumpet, live-electronics, hybrid instruments.  </keywords>
  </document>
  <document>
    <name>nime2012_272.pdf</name>
    <abstract>This paper presents a toolbox of gestural control mechanisms which are available when the input sensing apparatusis a pair of data gloves fitted with orientation sensors. Thetoolbox was developed in advance of a live music performance in which the mapping from gestural input to audiooutput was to be developed rapidly in collaboration with theperformer. The paper begins with an introduction to the associated literature before introducing a range of continuous,discrete and combined control mechanisms, enabling a flexible range of mappings to be explored and modified easily.An application of the toolbox within a live music performance is then described with an evaluation of the systemwith ideas for future developments.</abstract>
  </document>
  <document>
    <name>nime2012_275.pdf</name>
    <abstract>I present a novel low-tech multidimensional gestural controller, based on the resistive properties of a 2D field ofpencil markings on paper. A set of movable electrodes (+,-, ground) made from soldered stacks of coins create a dynamic voltage potential field in the carbon layer, and another set of movable electrodes tap voltages from this field.These voltages are used to control complex sound enginesin an analogue modular synthesizer. Both the voltage fieldand the tap electrodes can be moved freely. The designwas inspired by previous research in complex mappings foradvanced digital instruments, and provides a similarly dynamic playing environment for analogue synthesis. The interface is cheap to build, and provides flexible control overa large set of parameters. It is musically satisfying to play,and allows for a wide range of playing techniques, from wildexploration to subtle expressions. I also present an inventory of the available playing techniques, motivated by theinterface design, musically, conceptually and theatrically.The performance aspects of the interface are also discussed.The interface has been used in a number of performancesin Sweden and Japan in 2011, and is also used by othermusicians.</abstract>
  </document>
  <document>
    <name>nime2012_279.pdf</name>
    <abstract>As a 2010 Artist in Residence in Musical Research at IRCAM,Mari Kimura used the Augmented Violin to develop newcompositional approaches, and new ways of creating interactiveperformances [1]. She contributed her empirical and historicalknowledge of violin bowing technique, working with the RealTime Musical Interactions Team at IRCAM. Thanks to thisresidency, her ongoing long-distance collaboration with theteam since 2007 dramatically accelerated, and led to solvingseveral compositional and calibration issues of the GestureFollower (GF) [2]. Kimura was also the first artist to developprojects between the two teams at IRCAM, using OMAX(Musical Representation Team) with GF. In the past year, theperformance with Augmented Violin has been expanded inlarger scale interactive audio/visual projects as well. In thispaper, we report on the various techniques developed for theAugmented Violin and compositions by Kimura using them,offering specific examples and scores.</abstract>
    <keywords>Augmented Violin, Gesture Follower, Interactive Performance </keywords>
  </document>
  <document>
    <name>nime2012_284.pdf</name>
    <abstract>The Electromagnetically Sustained Rhodes Piano is an original Rhodes Piano modified to provide control over theamplitude envelope of individual notes through aftertouchpressure. Although there are many opportunities to shapethe amplitude envelope before loudspeaker amplification,they are all governed by the ever-decaying physical vibrations of the tone generating mechanism. A single-note proofof concept for electromagnetic control over this vibratingmechanism was presented at NIME 2011.In the past year, virtually every aspect of the system hasbeen improved. We use a different vibration sensor thatis immune to electromagnetic interference, thus eliminating troublesome feedback. For control, we both reduce costand gain continuous position sensing throughout the entirerange of key motion in addition to aftertouch pressure. Finally, the entire system now fits within the space constraintspresented by the original piano, allowing it to be installedon adjacent notes.</abstract>
    <keywords>Rhodes, piano, mechanical synthesizer, electromagnetic, sus- tain, feedback INTRODUCTION The Rhodes Piano sound has been a staple of mainstream music since its </keywords>
  </document>
  <document>
    <name>nime2012_291.pdf</name>
    <abstract>We have added a dynamic bio-mechanical mapping layerthat contains a model of the human vocal tract with tonguemuscle activations as input and tract geometry as output toa real time gesture controlled voice synthesizer system usedfor musical performance and speech research. Using thismapping layer, we conducted user studies comparing controlling the model muscle activations using a 2D set of forcesensors with a position controlled kinematic input spacethat maps directly to the sound. Preliminary user evaluation suggests that it was more difficult to using force inputbut the resultant output sound was more intelligible andnatural compared to the kinematic controller. This resultshows that force input is a potentially feasible for browsingthrough a vowel space for an articulatory voice synthesissystem, although further evaluation is required.</abstract>
    <keywords>Gesture, Mapping, Articulatory, Speech, Singing, Synthesis </keywords>
  </document>
  <document>
    <name>nime2012_292.pdf</name>
    <abstract>This paper presents the author's _derivations system, an interactive performance system for solo improvising instrumentalist. The system makes use of a combination of realtime audio analysis, live sampling and spectral re-synthesis to build a vocabulary of possible performative responses to live instrumental input throughout an improvisatory performance. A form of timbral matching is employed to form a link between the live performer and an expanding database of musical materials. In addition, the system takes into account the unique nature of the rehearsal/practice space in musical performance through the implementation of performer-configurable cumulative rehearsal databases into the final design. This paper discusses the system in detail with reference to related work in the field, making specific reference to the system's interactive potential both inside and outside of a real-time performance context. </abstract>
    <keywords>Interactivity, performance systems, improvisation  </keywords>
  </document>
  <document>
    <name>nime2012_293.pdf</name>
    <abstract>We present Patchwerk, a networked synthesizer module withtightly coupled web browser and tangible interfaces. Patchwerk connects to a pre-existing modular synthesizer usingthe emerging cross-platform HTML5 WebSocket standardto enable low-latency, high-bandwidth, concurrent controlof analog signals by multiple users. Online users controlphysical outputs on a custom-designed cabinet that reflectstheir activity through a combination of motorized knobsand LEDs, and streams the resultant audio. In a typicalinstallation, a composer creates a complex physical patchon the modular synth that exposes a set of analog anddigital parameters (knobs, buttons, toggles, and triggers)to the web-enabled cabinet. Both physically present andonline audiences can control those parameters, simultaneously seeing and hearing the results of each other's actions.By enabling collaborative interaction with a massive analogsynthesizer, Patchwerk brings a broad audience closer to arare and historically important instrument. Patchwerk isavailable online at http://synth.media.mit.edu.</abstract>
    <keywords>Modular synthesizer, HTML5, tangible interface, collabora- tive musical instrument </keywords>
  </document>
  <document>
    <name>nime2012_299.pdf</name>
    <abstract>Message mapping between control interfaces and sound engines is an important task that could benefit from toolsthat streamline development. A new Open Sound Control (OSC) namespace called Nexus Data Exchange Format(NDEF) streamlines message mapping by offering developers the ability to manage sound engines as network nodesand to query those nodes for the messages in their OSC address spaces. By using NDEF, developers will have an easier time managing nodes and their messages, especially forscenarios in which a single application or interface controlsmultiple sound engines. NDEF is currently implementedin the JunctionBox interaction toolkit but could easily beimplemented in other toolkits.</abstract>
    <keywords>OSC, namespace, interaction, node </keywords>
  </document>
  <document>
    <name>nime2012_301.pdf</name>
    <abstract>Aural - of or relateing to the ear or hearingAura - an invisible breath, emanation, or radiationAR - Augmented RealityAuRal is an environmental audio system in which individual participants form ad hoc ensembles based on geolocationand affect the overall sound of the music associated with thelocation that they are in.The AuRal environment binds physical location and thechoices of multiple, simultaneous performers to act as thegenerative force of music tied to the region. Through a mobile device interface, musical participants, or agents, have adegree of input into the generated music essentially defining the sound of a given region. The audio landscape issuperimposed onto the physical one. The resultant musical experience is not tied simply to the passage of time,but through the incorporation of participants over time andspatial proximity, it becomes an aural location as much asa piece of music. As a result, walking through the samelocation at different times results in unique collaborativelistening experiences.</abstract>
  </document>
  <document>
    <name>nime2012_303.pdf</name>
    <abstract>ion for Distributed NIMEsCharles RobertsMedia Arts and TechnologyProgram, UCSBcharlie@charlie-roberts.comGraham WakefieldMedia Arts and TechnologyProgram, UCSBwakefield@mat.ucsb.eduMatthew WrightMedia Arts and TechnologyProgram, UCSBmatt@create.ucsb.eduABSTRACTDesigning mobile interfaces for computer-based musical performance is generally a time-consuming task that can be exasperating for performers. Instead of being able to experiment freely with physical interfaces' affordances, performersmust spend time and attention on non-musical tasks including network configuration, development environments forthe mobile devices, defining OSC address spaces, and handling the receipt of OSC in the environment that will controland produce sound. Our research seeks to overcome suchobstacles by minimizing the code needed to both generateand read the output of interfaces on mobile devices. For iOSand Android devices, our implementation extends the application Control to use a simple set of OSC messages to defineinterfaces and automatically route output. On the desktop,our implementations in Max/MSP/Jitter, LuaAV, and SuperCollider allow users to create mobile widgets mapped tosonic parameters with a single line of code. We believe thefluidity of our approach will encourage users to incorporatemobile devices into their everyday performance practice.</abstract>
    <keywords>NIME, OSC, Zeroconf, iOS, Android, Max/MSP/Jitter, Lu- aAV, SuperCollider, Mobile </keywords>
  </document>
  <document>
    <name>nime2012_308.pdf</name>
    <abstract>This paper provides an overview of a new method for approaching beat sequencing. As we have come to know them drum machines provide means to loop rhythmic patterns over a certain interval. Usually with the option to specify different beat divisions. What I developed and propose for consideration is a rethinking of the traditional drum machine confines. The Sinkapater is an untethered beat sequencer in that the beat division, and the loop length can be arbitrarily modified for each track. The result is the capability to create complex syncopated patterns which evolve over time as different tracks follow their own loop rate. To keep cohesion all channels can be locked to a master channel forcing a loop to be an integer number of "Master Beats". Further a visualization mode enables exploring the patterns in another new way. Using synchronized OpenGL a 3-Dimensional environment visualizes the beats as droplets falling from faucets of varying heights determined by the loop length. Waves form in the bottom as beats splash into the virtual "sink". By combining compelling visuals and a new approach to sequencing a new way of exploring beats and experiencing music has been created.</abstract>
    <keywords>NIME, proceedings, drum machine, sequencer, visualization </keywords>
  </document>
  <document>
    <name>nime2012_309.pdf</name>
    <abstract>This research aims to improve the correspondence between music and dance, and explores the use of human respiration pattern for musical applications with focus on the motional aspect of breathing. While respiration is frequently considered as an indicator of the metabolic state of human body that contains meaningful information for medicine or psychology, motional aspect of respiration has been relatively unnoticed in spite of its strong correlation with muscles and the brain. This paper introduces an interactive system to control music playback for dance performances based on the respiration pattern of the dancer. A wireless wearable sensor device detects the dancer's respiration, which is then utilized to modify the dynamic of music. Two different respirationdynamic mappings were designed and evaluated through public performances and private tests by professional choreographers. Results from this research suggest a new conceptual approach to musical applications of respiration based on the technical characteristics of music and dance. </abstract>
    <keywords>Music, dance, respiration, correspondence, wireless interface,  interactive performance  </keywords>
  </document>
  <document>
    <name>nime2012_310.pdf</name>
    <abstract>We present the integration of two musical interfaces into anew music-making system that seeks to capture the experience of a choir and bring it into the mobile space. Thissystem relies on three pervasive technologies that each support a different part of the musical experience. First, themobile device application for performing with an artificialvoice, called ChoirMob. Then, a central composing and conducting application running on a local interactive display,called Vuzik. Finally, a network protocol to synchronizethe two. ChoirMob musicians can perform music togetherat any location where they can connect to a Vuzik centralconducting device displaying a composed piece of music. Weexplored this system by creating a chamber choir of ChoirMob performers, consisting of both experienced musiciansand novices, that performed in rehearsals and live concertscenarios with music composed using the Vuzik interface.</abstract>
    <keywords>singing synthesis, mobile music, interactive display, inter- face design, OSC, ChoirMob, Vuzik, social music, choir </keywords>
  </document>
  <document>
    <name>nime2012_315.pdf</name>
    <abstract>In the following paper we propose a new tiered granularity approach to developing modules or abstractions in the PdL2Ork visual multimedia programming environment with the specific goal of devising creative environments that scale their educational scope and difficulty to encompass several stages within the context of primary and secondary (K-12) education. As part of a preliminary study, the team designed modules targeting 4th and 5th grade students, the primary focus being exploration of creativity and collaborative learning. The resulting environment infrastructure - coupled with the Boys &amp; Girls Club of Southwest Virginia Satellite Linux Laptop Orchestra - offers opportunities for students to design and build original instruments, master them through a series of rehearsals, and ultimately utilize them as part of an ensemble in a performance of a predetermined piece whose parameters are coordinated by instructor through an embedded networked module. The ensuing model will serve for the assessment and development of a stronger connection with content-area standards and the development of creative thinking and collaboration skills. </abstract>
    <keywords>Granular, Learning Objects, K-12, Education, L2Ork, Pd- L2Ork  </keywords>
  </document>
</documents>